<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"kyrie2to11.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="记录学习和生活，留下时光的痕迹">
<meta property="og:type" content="website">
<meta property="og:title" content="Jarvis&#39;s Blog">
<meta property="og:url" content="https://kyrie2to11.github.io/index.html">
<meta property="og:site_name" content="Jarvis&#39;s Blog">
<meta property="og:description" content="记录学习和生活，留下时光的痕迹">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="jarvis">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://kyrie2to11.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Jarvis's Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Jarvis's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记录生活，沉淀自己</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">jarvis</p>
  <div class="site-description" itemprop="description">记录学习和生活，留下时光的痕迹</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2024/08/17/aliyun-github-blog-config/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/08/17/aliyun-github-blog-config/" class="post-title-link" itemprop="url">Hexo Blog Updates to Aliyun and Github Page</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-08-17 11:59:05 / 修改时间：20:19:38" itemprop="dateCreated datePublished" datetime="2024-08-17T11:59:05+08:00">2024-08-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="aliyun-服务器配置"><a href="#aliyun-服务器配置" class="headerlink" title="aliyun 服务器配置"></a>aliyun 服务器配置</h2><h3 id="ubuntu-系统更新"><a href="#ubuntu-系统更新" class="headerlink" title="ubuntu 系统更新"></a>ubuntu 系统更新</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt upgrade</span><br></pre></td></tr></table></figure>

<h3 id="创建新用户"><a href="#创建新用户" class="headerlink" title="创建新用户"></a>创建新用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">adduser jarvis <span class="comment"># jarvis 为用户名</span></span><br><span class="line"><span class="built_in">chmod</span> 740 /etc/sudoers</span><br><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure>

<p>找到如下 <code>root ALL=(ALL:ALL) ALL</code> 后，在其下面添加一行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jarvis ALL=(ALL:ALL) ALL</span><br></pre></td></tr></table></figure>

<h3 id="切换到新用户目录下"><a href="#切换到新用户目录下" class="headerlink" title="切换到新用户目录下"></a>切换到新用户目录下</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">su jarvis</span><br><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install vim git htop screenfetch curl wget <span class="comment"># 安装常用软件</span></span><br></pre></td></tr></table></figure>

<h3 id="配置-ssh"><a href="#配置-ssh" class="headerlink" title="配置 ssh"></a>配置 ssh</h3><p>在服务器用户目录下创建 <code>~/.ssh</code> 和 <code>authorized_keys</code> 文件，赋予权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> ~/.ssh</span><br><span class="line">vim ~/.ssh/authorized_keys </span><br><span class="line"><span class="built_in">chmod</span> 600 ~/.ssh/authorized_keys </span><br><span class="line"><span class="built_in">chmod</span> 700 ~/.ssh/</span><br></pre></td></tr></table></figure>

<p>然后切回本机，将 <code>~/.ssh/id_rsa.pub</code> 公钥复制到远程服务器的 <code>~/.ssh/authorized_keys</code>里面；本地测试，验证 ssh 无密码登录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -v jarvis@SERVER_IP <span class="comment"># -v 参数显示详细信息 verbose</span></span><br></pre></td></tr></table></figure>

<h3 id="配置-git"><a href="#配置-git" class="headerlink" title="配置 git"></a>配置 git</h3><p>创建工作目录 blog,初始化 Git 裸库 blog.git,创建 hook 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line"><span class="built_in">mkdir</span> blog</span><br><span class="line"><span class="built_in">mkdir</span> repos</span><br><span class="line"><span class="built_in">cd</span> repos</span><br><span class="line">git init --bare blog.git</span><br><span class="line">vim blog.git/hooks/post-receive  <span class="comment"># 创建 hook 文件</span></span><br></pre></td></tr></table></figure>

<p>编辑 hook 内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#！/bin/sh</span></span><br><span class="line">git --work-tree=/home/jarvis/blog --git-dir=/home/jarvis/repos/blog.git checkout -f</span><br></pre></td></tr></table></figure>

<p>添加运行权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x blog.git/hooks/post-receive</span><br></pre></td></tr></table></figure>

<h3 id="配置-nginx"><a href="#配置-nginx" class="headerlink" title="配置 nginx"></a>配置 nginx</h3><p>安装 nginx 并修改对应配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install</span><br><span class="line">sudo vim /etc/nginx/sites-available/default</span><br></pre></td></tr></table></figure>

<p>找到</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># include snippets/snakeoil.conf;</span></span><br><span class="line"></span><br><span class="line">root /var/www/html; </span><br></pre></td></tr></table></figure>

<p>替换为</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># include snippets/snakeoil.conf;</span></span><br><span class="line"></span><br><span class="line">root /home/jarvis/blog;</span><br></pre></td></tr></table></figure>

<p>此刻直接访问云服务器的公网 IP 会显示 nginx 欢迎界面。</p>
<h2 id="本地-hexo-config-yml-文件配置"><a href="#本地-hexo-config-yml-文件配置" class="headerlink" title="本地 hexo _config.yml 文件配置"></a>本地 hexo _config.yml 文件配置</h2><p>配置如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line">deploy:</span><br><span class="line">    <span class="built_in">type</span>: git</span><br><span class="line">    repo: </span><br><span class="line">      github: git@github.com:kyrie2to11/kyrie2to11.github.io.git </span><br><span class="line">      aliyun: jarvis@aliyun_server_ip:/home/jarvis/blog/blog.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure>

<h2 id="用-aliyun-ip-访问博客报错-404-处理"><a href="#用-aliyun-ip-访问博客报错-404-处理" class="headerlink" title="用 aliyun ip 访问博客报错 404 处理"></a>用 aliyun ip 访问博客报错 <code>404</code> 处理</h2><p>部署完毕，访问 SERVER IP 出现 404 报错, 查看 log,显示没有访问 <code>/home/jarvis/blog</code> 的权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /var/log/nginx/error.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出如下</span></span><br><span class="line">2024/08/17 19:44:42 [crit] 9007<span class="comment">#9007: *14 stat() &quot;/home/jarvis/blog/&quot; failed (13: Permission denied), client: 114.97.236.162, server: _, request: &quot;GET / HTTP/1.1&quot;, host: &quot;47.100.101.82&quot;</span></span><br><span class="line">2024/08/17 19:44:42 [crit] 9007<span class="comment">#9007: *14 stat() &quot;/home/jarvis/blog/&quot; failed (13: Permission denied), client: 114.97.236.162, server: _, request: &quot;GET / HTTP/1.1&quot;, host: &quot;47.100.101.82&quot;</span></span><br><span class="line">2024/08/17 19:44:43 [crit] 9007<span class="comment">#9007: *14 stat() &quot;/home/jarvis/blog/&quot; failed (13: Permission denied), client: 114.97.236.162, server: _, request: &quot;GET / HTTP/1.1&quot;, host: &quot;47.100.101.82&quot;</span></span><br><span class="line">2024/08/17 19:44:43 [crit] 9007<span class="comment">#9007: *14 stat() &quot;/home/jarvis/blog/&quot; failed (13: Permission denied), client: 114.97.236.162, server: _, request: &quot;GET / HTTP/1.1&quot;, host: &quot;47.100.101.82&quot;</span></span><br></pre></td></tr></table></figure>

<p>查看 nginx 所有进程，找到 nginx worker process 为 <code>www-data</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出如下</span></span><br><span class="line">nginx: master process /usr/sbin/nginx -g daemon on; master_process on;</span><br><span class="line">www-data   12637  0.0  0.3  55992  6488 ?        S    19:51   0:00 nginx: worker process</span><br><span class="line">www-data   12638  0.0  0.3  55992  5656 ?        S    19:51   0:00 nginx: worker process</span><br><span class="line">root       13217  0.0  0.1   6612  2444 pts/5    S+   20:01   0:00 grep --color=auto nginx</span><br></pre></td></tr></table></figure>

<p>参照<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/25774999/nginx-stat-failed-13-permission-denied">如下链接</a>赋予 nginx worker process: <code>www-data</code> 访问博客路径 <code>root /home/jarvis/blog</code> 的权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpasswd -a www-data jarvis</span><br><span class="line"><span class="built_in">chmod</span> g+x /home  &amp;&amp; <span class="built_in">chmod</span> g+x /home/jarvis &amp;&amp; <span class="built_in">chmod</span> g+x /home/jarvis/blog</span><br><span class="line">nginx -s reload <span class="comment"># 重启 nginx</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/11/18/QuantLLM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/18/QuantLLM/" class="post-title-link" itemprop="url">QuantLLM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-11-18 15:25:11" itemprop="dateCreated datePublished" datetime="2023-11-18T15:25:11+08:00">2023-11-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:21:10" itemprop="dateModified" datetime="2024-12-02T15:21:10+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Quantization-of-LLM"><a href="#Quantization-of-LLM" class="headerlink" title="Quantization of LLM"></a>Quantization of LLM</h2><h3 id="LLM-Quantization-Survey"><a href="#LLM-Quantization-Survey" class="headerlink" title="LLM Quantization Survey"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.07633">LLM Quantization Survey</a></h3><h3 id="Awesome-LLM-Quantization-Repository"><a href="#Awesome-LLM-Quantization-Repository" class="headerlink" title="Awesome LLM Quantization Repository"></a><a target="_blank" rel="noopener" href="https://github.com/HuangOwen/Awesome-LLM-Compression#quantization">Awesome LLM Quantization Repository</a></h3><h3 id="LLM-Quantization-Papers"><a href="#LLM-Quantization-Papers" class="headerlink" title="LLM Quantization Papers"></a>LLM Quantization Papers</h3><h4 id="Quantization-Aware-Training-QAT"><a href="#Quantization-Aware-Training-QAT" class="headerlink" title="Quantization-Aware Training(QAT)"></a>Quantization-Aware Training(QAT)</h4><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.17888">LLM-QAT (from META)</a><ul>
<li>Motivation:<ol>
<li>Lacking training data</li>
<li>Training LLMs involves instruction tuning, reinforcement learning and etc, which are difficult to replicate during QAT</li>
</ol>
</li>
<li>Method:  <ol>
<li>Data-free quantization-aware training (QAT) which produces QAT data using next token data generation <code>-&gt;</code> Select appropriate fine-tuning dataset    <style>.qoxwrzqkbunj{}</style></li>
<li>Per-channel weight quantization and per-token activation quantization (symmetric MinMax quantization), per-token quantization for KV cache <code>-&gt;</code> Identify suitable quantizer    <style>.vtdysxjjxvwe{}</style></li>
<li>Cross-entropy based loss <code>-&gt;</code> Knowledge distillation from full precision model    <style>.rmfrpsyzygce{}</style></li>
</ol>
</li>
<li>Result:<ol>
<li>Empirical recommendations:<ul>
<li>8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case</li>
<li>4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size <code>-&gt;</code> 4-bit LLM-QAT models towards the best efficiency-accuracy tradeoff</li>
</ul>
</li>
<li>Partial results:    <style>.gjnqkwnyubmp{}</style></li>
</ol>
</li>
<li>Limitation:<ol>
<li>4-bit quantization does not have hardware support out-of-the-box <code>-&gt;</code> no hardware implementation</li>
<li>Method works well for 4-bit weights, 4-bit KV cache and 8-bit activations <code>-&gt;</code> Insufficient for 4-bit activation quantization</li>
</ol>
</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14152">PEQA (from NAVER)</a><ul>
<li>Motivation:<ol>
<li>Bridging the gap between parameter-efficient fine-tuning(PEFT e.g. LoRA, Prefix Tuning) and Quantization <code>-&gt;</code> combine PEFT with quantized LLMs</li>
</ol>
</li>
<li>Method:<ol>
<li>Overall pipeline    <style>.gfwmdclhaqks{}</style></li>
<li>Solely updating quantization scales while freezing the integer quantization values of pre-trained weights  <style>.driqeraxbhaf{}</style></li>
</ol>
</li>
<li>Result:<ol>
<li>Memory footprint, inference latency performance  <style>.vsatlelcxpoe{}</style>  
  <style>.uqoqelqwimxw{}</style></li>
<li>Common-sense reasoning and in-context learning performance    <style>.sessfnsjemyk{}</style></li>
<li>Massive Multitask Language Understanding (MMLU) benchmark performance    <style>.cpcsckwhqeuj{}</style></li>
</ol>
</li>
<li>Limitation:<ol>
<li>low-bit weight-only quantization in a linear asymmetric per-channel context <code>-&gt;</code> Lacking weight-activation quantization part</li>
</ol>
</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">QLoRA (from University of Washington’s UW NLP group)</a><ul>
<li>Motivation:<ol>
<li>Reduce memory footprint of parameter-efficient fine-tuning(PEFT) stage</li>
</ol>
</li>
<li>Method:<ol>
<li>Overall pipeline    <style>.tjgaijpuonqb{}</style></li>
<li>QLoRA    <style>.iyrcylivdnst{}</style>
<ul>
<li>4-bit NormalFloat Quantization <code>-&gt;</code> better quantization data type for normally distributed data compared with 4-bit Integers and 4-bit Floats (See the paper for details)</li>
<li>Double Quantization <code>-&gt;</code> combined with NF4 to reduce the memory footprint of quantization constants i.e. weights (See the paper for details)</li>
</ul>
</li>
<li>Paged Optimizers <code>-&gt;</code> manage memory spikes i.e. manage the memory swap between CPU and GPU</li>
</ol>
</li>
<li>Result:<ol>
<li>MMLU test accuracy    <style>.ciweypclhpnu{}</style></li>
<li>Memory footprint <code>-&gt;</code> enables the finetuning of 33B parameter models on a single consumer GPU and 65B parameter models on a single professional GPU, even 7B parameter models on mobile phones(e.g. iPhone 12 Plus)  <style>.fkgpnpglnbdb{}</style></li>
</ol>
</li>
<li>Limitation:<ol>
<li>Can’t establish that QLoRA can match full 16-bit finetuning performance at 33B and 65B scales…</li>
<li>Did not evaluate different bit-precisions e.g.3-bit base models, or different adapter methods</li>
</ol>
</li>
</ul>
</li>
</ol>
<h4 id="Post-Training-Quantization-PTQ"><a href="#Post-Training-Quantization-PTQ" class="headerlink" title="Post-Training Quantization(PTQ)"></a>Post-Training Quantization(PTQ)</h4><h5 id="Weight-Quantization"><a href="#Weight-Quantization" class="headerlink" title="Weight Quantization"></a>Weight Quantization</h5><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.09557">LUT-GEMM</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.07339">LLM.int8()</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.17323">GPTQ</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.00978">AWQ</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.02272">OWQ</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.03078">SpQR</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.07629">SqueezeLLM</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.13304">QuIP</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.05516">SignRound</a></li>
</ol>
<h5 id="Weight-and-Activation-Quantization"><a href="#Weight-and-Activation-Quantization" class="headerlink" title="Weight and Activation Quantization"></a>Weight and Activation Quantization</h5><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.01861">ZeroQuant</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.10438">SmoothQuant</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.01089">RPTQ</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.07493">OliVe</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.08302">ZeroQuant-V2</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.09145">OutlierSuppression+</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.12356">MoFQ</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.09782">ZeroQuant-FP</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.15987">FPTQ</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.01885">QuantEase</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.02784">NormTweaking</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.13137">OmniQuant</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/10/19/ZMO_interview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/19/ZMO_interview/" class="post-title-link" itemprop="url">Interview</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-19 14:39:35" itemprop="dateCreated datePublished" datetime="2023-10-19T14:39:35+08:00">2023-10-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 11:53:24" itemprop="dateModified" datetime="2024-12-02T11:53:24+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="2023-10-19-ZMO-Round-2"><a href="#2023-10-19-ZMO-Round-2" class="headerlink" title="2023.10.19 ZMO Round 2"></a>2023.10.19 ZMO Round 2</h2><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><ol>
<li>产品 background remover&#x2F; AI designer &#x2F; text or image to image &#x2F; magic remover 这几个都是用什么模型做的</li>
<li>想了解下用卡的规模，对比下学校的集群 A40 A800</li>
<li>有无stable diffusion做移动端部署的需求和规划（参考：<a target="_blank" rel="noopener" href="https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android?spm=a2c6h.12873639.article-detail.11.379f1ba9d19yg7%EF%BC%89">https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android?spm=a2c6h.12873639.article-detail.11.379f1ba9d19yg7）</a></li>
<li>日常工作中模型训练以及针对实际应用场景优化这部分，对于日常工作占大头吗 还是说集中在看论文找idea这种</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/10/15/Work_Log_1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/15/Work_Log_1/" class="post-title-link" itemprop="url">Quantization of SR Methods Work Log</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-15 16:16:29" itemprop="dateCreated datePublished" datetime="2023-10-15T16:16:29+08:00">2023-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:23:21" itemprop="dateModified" datetime="2024-12-02T15:23:21+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Time-2023-10-09-2023-11-10"><a href="#Time-2023-10-09-2023-11-10" class="headerlink" title="Time:2023.10.09-2023.11.10"></a>Time:2023.10.09-2023.11.10</h2><h2 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h2><ol>
<li>VRT: A Video Restoration Transformer (arXiv 2022)-&gt; <code>VRT</code><ul>
<li>feature: parallel computation + long-range dependency modelling + mutual attention for frame alignment</li>
</ul>
</li>
<li>BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment (CVPR 2022) -&gt; <code>BasicVSR++</code></li>
<li>Learning Trajectory-Aware Transformer for Video Super-Resolution (CVPR 2022) -&gt; <code>TTVSR</code></li>
<li>An Implicit Alignment for Video Super-Resolution (CVPR2023) -&gt; <code>IA-RT/IA-CNN</code></li>
<li>Rethinking Alignment in Video Super-Resolution Transformers (NIPS2022) -&gt; <code>PSRT</code></li>
<li>ResQ: Residual Quantization for Video Perception (ICCV 2023) -&gt; <code>ResQ</code><ul>
<li>motivation: residuals exhibit a significantly lower variance than the frame activations, and can be quantized with lower error.</li>
<li>verified tasks: Human Pose Estimation&#x2F;Semantic Segmentation</li>
<li>limitations:<ul>
<li>requires the propagation of representations to future timesteps, leading to a memory overhead potentially impacting latency -&gt; 对VSR任务影响小,例如BasicVSR++ 本身就是基于帧间传播的,且目前VSR对latency要求不高</li>
<li>implementing location-specific quantized operations is not trivial and requires specialized hardware or gather-scatter implementations of convolutions -&gt; 实际部署困难问题 特定区域的量化选择 涉及稀疏处理的调度问题</li>
<li>ResQ is able to reduce the amortized cost of video processing, yet the peak BOPs is not reduced</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><ol>
<li><p><code>BasicVSR++</code> + <code>Paddleslim</code> 量化看结果 -&gt; false</p>
</li>
<li><p>openmmlab 的 <code>mmagic</code> project 内置的 <code>BasicVSR++</code> + <code>mmrazor</code> project 进行量化 -&gt; false</p>
</li>
<li><p><code>BasicVSR++</code> + <code>Dipoorlet</code> PTQ -&gt; false: ValueError: cannot reshape array of size 3628800 into shape (0,0,3,180,320) dipoorlet可能不支持动态输入</p>
</li>
<li><p><code>BasicVSR++</code> + <code>MQBench</code> PTQ -&gt; pending</p>
<ol>
<li>使用<code>mmedit</code>构建的<code>BasicVSR++</code>在<code>symbolic traces</code>时会出现报错<code>TypeError: &#39;BasicVSR&#39; object is not subscriptable</code>, 故尝试直接通过模型的<code>archetecture</code>和<code>checkpoint</code>构建模型 -&gt; suspend (必要性不强，工作量不小~)</li>
<li>torch fx <code>if</code> symbolic trace fause: <code>torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow</code></li>
</ol>
</li>
<li><p><code>BasicVSR++</code> + <code>PPL Quantization Tool(PPQ)</code> PTQ -&gt; inprogress</p>
</li>
<li></li>
</ol>
<h2 id="Issue-Log"><a href="#Issue-Log" class="headerlink" title="Issue Log"></a>Issue Log</h2><ol>
<li><p>command: <code>whereis</code> vs <code>which</code></p>
<ol>
<li>用途：<code>whereis</code> 用于查找可执行文件、源代码文件和帮助文档等。<br>  输出：它返回指定命令的可执行文件路径、man页面（帮助文档）路径以及源代码路径（如果可用）。<br>  限制：通常，whereis 不搜索PATH环境变量中指定的所有目录，而是搜索标准的系统目录。因此，它可能无法找到用户自定义安装的命令。</li>
<li>用途：<code>which</code>用于查找可执行命令的位置，通常用于查找命令是否在系统PATH中，并返回找到的第一个匹配的命令。<br>  输出：它会搜索PATH环境变量中指定的目录以查找命令。<br>  注意：which 仅返回第一个匹配的命令的路径，因此如果有多个同名命令，它只会返回一个。</li>
</ol>
</li>
<li><p>environment setup</p>
<ol>
<li>NvidiaDriver&#x2F;CUDA&#x2F;CUDNN installation<ul>
<li>note: 传统上，安装 NVIDIA Driver 和 CUDA Toolkit 的步骤是分开的，但实际上现在可以直接安装 CUDA Toolkit，系统将自动安装与其版本匹配的 NVIDIA Driver。</li>
</ul>
</li>
<li>Pull docker image: torch 1.13.1<ul>
<li>command: <code>docker pull cnstark/pytorch:1.13.1-py3.9.16-cuda11.7.1-ubuntu20.04</code></li>
</ul>
</li>
<li>Dipoorlet dependency:<ul>
<li>CUDA Toolkit  &#x3D;&#x3D; 11.8</li>
<li>CUDNN &#x3D;&#x3D; 8.7.0</li>
<li>onnxruntime-gpu &#x3D;&#x3D; 1.16.0</li>
<li>python &#x3D;&#x3D; 3.8.10</li>
<li>torch &#x3D;&#x3D; 2.0.0</li>
</ul>
</li>
<li>Ubuntu kenerl unroll (回滚)<ul>
<li><code>sudo dpkg --get-selections | grep linux-image</code>  查看系统已经安装的kernel</li>
<li><code>sudo apt-get remove linux-image-x.x.x-xx-generic</code> 卸载目前的kernel</li>
<li><code>sudo update-grub</code> 更新开机引导程序 ps: GRUB（GRand Unified Bootloader）是一个用于管理计算机开机引导过程的引导加载程序,支持引导多操作系统 windows&#x2F;linux发行版&#x2F;BSD</li>
<li><code>sudo reboot</code> 重启系统</li>
<li><code>uname -r</code> 查看当前kernel是否已经完成回滚</li>
<li><code>sudo apt-mark hold/unhold linux-image-5.4.0-xx-generic</code> 设置 hold 参数保持当前kernel不更新, 设置 unhold 解除更新限制</li>
</ul>
</li>
</ol>
</li>
<li><p>command: <code>pip install -U package</code> 命令中的 <code>-U</code> 参数表示升级(update)已安装的 Python package到最新版本。</p>
</li>
<li><p>confusion: PTQ static vs dynamic</p>
<ol>
<li>PTQ static<ul>
<li>使用校准数据集离线计算缩放因子(Scale)和零点(Zero Point)</li>
<li>所有激活(Activation)都使用相同的缩放因子和零点</li>
</ul>
</li>
<li>PTQ dynamic<ul>
<li>缩放因子(Scale)和零点(Zero Point)是在推理时计算的，并且特定用于每次的激活(Activation)</li>
<li>因此它们更准确，但引入了额外的计算开销</li>
</ul>
</li>
</ol>
</li>
<li><p>confusion: BI degradation vs BD degradation</p>
<ol>
<li>BI -&gt; bicubic-down,降质过程仅包含双三次下采样</li>
<li>BD -&gt; blur-down,通过高斯模糊下采样</li>
</ol>
</li>
<li><p>confusion: argparse library</p>
<ol>
<li>basic: <code>parser = argparse.ArgumetParser(description, epilog)</code> -&gt; 创建 ArgumentParser 对象，其中 description 是一个简要的程序描述，epilog 是一个在帮助信息的结尾显示的额外文本</li>
<li>basic: <code>add_argument(name or flags, ...)</code> -&gt; 添加命令行参数(<code>xxx</code>: positional argument or <code>--xxx</code>: option that takes a value)。name or flags 参数可以是单个选项（例如 ‘-f’），也可以是多个选项（例如 ‘-f’, ‘–file’）。你可以使用许多其他关键字参数来配置参数的行为，如 type、default、help 等,示例如下<ul>
<li>整数类型参数: <code>parser.add_argument(&#39;--count&#39;, type=int, help=&#39;An integer value&#39;)</code></li>
<li>浮点类型参数: <code>parser.add_argument(&#39;--rate&#39;, type=float, help=&#39;A floating-point value&#39;)</code></li>
<li>字符串类型参数: <code>parser.add_argument(&#39;--name&#39;, type=str, help=&#39;A string value&#39;)</code></li>
<li>布尔类型参数: <code>parser.add_argument(&#39;--verbose&#39;, action=&#39;store_true&#39;, help=&#39;Enable verbose mode&#39;)</code></li>
<li>文件路径参数: <code>parser.add_argument(&#39;--file&#39;, type=argparse.FileType(&#39;r&#39;), help=&#39;A file path&#39;)</code></li>
<li>目录路径参数: <code>parser.add_argument(&#39;--directory&#39;, type=str, help=&#39;A directory path&#39;)</code></li>
<li><strong>note:</strong> 很奇怪的点在于关键字参数<code>--xx_x</code>中使用<code>_</code>传入参数时会出现<code>error: unrecognized arguments: --xx_x</code>,故用<code>-</code>代替</li>
</ul>
</li>
<li>basic: <code>parse_args()</code>: 解析命令行参数，并返回一个包含所有参数值的命名空间对象</li>
<li>extension: <code>add_subparsers()</code> -&gt; 添加子命令解析器，允许你为你的程序创建子命令（类似于 git 命令的子命令，如 git clone、git commit 等）</li>
<li>extension: <code>set_defaults()</code> -&gt; 为参数设置默认值</li>
<li>extension: <code>add_argument_group()</code> -&gt; 将参数分组到一个组中，用于更好地组织帮助信息</li>
<li>extension: <code>format_help()</code> -&gt; 生成帮助信息</li>
<li>extension: <code>error(msg)</code> -&gt; 在参数解析过程中发生错误时触发错误消息</li>
</ol>
</li>
<li><p>confusion: <code>&#123;:08d&#125;.png</code> 字符串格式化模板</p>
<ol>
<li><code>&#123;&#125;</code>: 这是一个占位符，用于表示将要插入的值。在这个模板中，{} 用来表示一个整数</li>
<li><code>:08d</code>: 这是格式说明符，指定了如何格式化这个整数。其中：<ul>
<li><code>0</code> 表示要用零来填充空白位置</li>
<li><code>8</code> 表示总共要占用 8 个字符的宽度，包括填充的零和数字本身</li>
<li><code>d</code> 表示要格式化的值是一个十进制整数</li>
</ul>
</li>
</ol>
</li>
<li><p>confusion: function <code>os.path.splitext()</code></p>
<ol>
<li>input: 文件路径（或文件名）</li>
<li>return: 一个包含两个部分的元组tuple i.e. (文件名, 文件扩展名)</li>
</ol>
</li>
<li><p>confusion: print(model) 实现</p>
<ol>
<li>print(model)  # 当你使用 print 函数来打印一个对象时，它会尝试调用该对象的 <code>__str__()</code> 或 <code>__repr__()</code> 方法来获取一个可打印的字符串表示<ul>
<li><code>model.__repr__</code> 输出model的属性<code>__repr__</code>  i.e. 方法的名称  -&gt; <code>&lt;bound method Module.__repr__ of xxx&gt;</code></li>
<li><code>model.__repr__()</code> 输出model的method<code>__repr__()</code> i.e. 方法的调用 的结果 -&gt; <code>xxx</code></li>
<li><strong>note:</strong> 属性是对象的数据成员，而方法是对象上的函数成员，用于定义对象的行为。实际上，方法是对象上的可调用属性</li>
</ul>
</li>
</ol>
</li>
<li><p>confusion: <code>glob.glob(os.path.join(img_dir, &#39;*&#39;))</code></p>
<ol>
<li><code>import os</code> 后 <code>os.path.join(img_dir, &#39;*&#39;)</code> 返回值 <code>&#39;img_dir/*&#39;</code>类型为 <code>str</code> -&gt; <code>type(os.path.join(img_dir, &#39;*&#39;)) == &lt;class &#39;str&#39;&gt;</code></li>
<li><code>import glob</code> 后 <code>glob.glob(os.path.join(img_dir, &#39;*&#39;))</code> 返回值为指定路径下的文件列表 -&gt; <code>type(glob.glob(os.path.join(img_dir, &#39;*&#39;))) == &lt;class &#39;list&#39;&gt;</code></li>
</ol>
</li>
<li><p>confusion: <code>img_dir_split = re.split(r&#39;[\\/]&#39;, img_dir)</code> 这行代码使用正则表达式来分割文件路径</p>
<ol>
<li>具体来讲首先<code>import re # re(regular expression) 是 python 中的正则表达式模块</code> , r’[\&#x2F;]’ 表示一个正则表达式的字符集, 它匹配一个正斜杠 &#x2F; 或反斜杠 \ 中的任何一个字符, ‘\‘ 用于转义字符。这在处理文件路径时很有用，因为不同的操作系统使用不同的路径分隔符，有些使用正斜杠 &#x2F;，而有些使用反斜杠 \。使用 [\&#x2F;] 可以在跨平台的情况下匹配路径中的分隔符，而不必担心操作系统差异</li>
<li><code>img_dir_split = re.split(r&#39;[\\/]&#39;, img_dir)</code>  它会将指定的文件路径 img_dir 按照正斜杠 &#x2F; 或反斜杠 \ 进行分割，并将分割的部分存储在一个列表中。返回值为由文件路径名各部分(str)组成的list -&gt; 例如  img_dir_split &#x3D;&#x3D; [‘data’, ‘demo_000’]</li>
</ol>
</li>
<li><p>confusion: <code>img_dir_split[:-1]</code></p>
<ol>
<li>img_dir_split[:-1] 是 Python 中的列表切片（slicing）操作，它用于获取列表 img_dir_split 中的一部分元素</li>
<li>具体来说，[:-1] 表示切片从列表的开头到倒数第二个元素（不包括倒数第一个元素）。这个操作会返回一个新的列表，包含了 img_dir_split 中从第一个元素到倒数第二个元素（不包括倒数第一个元素）的所有元素</li>
</ol>
</li>
<li><p>confusion: <code>lq_folder = reduce(os.path.join, img_dir_split[:-1])</code></p>
<ol>
<li><code>from functools import reduce</code> 导入包<code>reduce</code>，它用于将一个二元函数应用于可迭代的元素，从左到右依次累积结果</li>
<li>上述代码的目的是将 img_dir_split 列表切片后的元素 i.e. <code>img_dir_split[:-1]</code>通过操作系统路径连接起来，然后返回连接后的结果。</li>
</ol>
</li>
<li><p>confusion: <code>assert isinstance(transforms, Sequence)</code></p>
<ol>
<li><code>from collections.abc import Sequence</code> <code>Sequence</code>是在Python中的抽象基类，用于定义一组通用的接口和方法，而不是具体的实现。<code>Sequence</code>它表示序列类型，如列表、元组、字符串等。</li>
<li><code>assert isinstance(transforms, Sequence)</code>这种检查可以用于确保transforms具有序列类型的行为，以便在代码中安全地使用类似列表或元组的操作。</li>
</ol>
</li>
<li><p>confusion: <code>@PIPELINES.register_module()</code></p>
<ol>
<li><code>from ..registry import PIPELINES</code>: 从相对于当前模块的上一级目录中的 registry 模块中导入名为 PIPELINES 的对象</li>
<li><code>@PIPELINES.register_module()</code>的作用是将被装饰的函数或类注册到名为 PIPELINES 的模块或类的注册表中。这通常在Python中用于实现插件或扩展性架构，以便在运行时动态添加和管理功能。</li>
</ol>
</li>
<li><p>confusion: <code>dict</code>类实例化后的对象<code>object</code>如何访问</p>
<ol>
<li>在Python中，字典的键和值是通过方括号<code>[]</code>来访问的(如<code>data[&quot;meta&quot;]</code>, data为dict类的实例, <code>meta</code> 是其中一个key)，而不是通过点.来访问</li>
<li>在Python中，可以使用点号 <code>.</code>来访问以下类型的成员或属性<ul>
<li>类的成员(类变量和类方法): <code>MyClass.my_class_variable 或 MyClass.my_class_method()</code></li>
<li>实例对象的属性和方法: <code>my_object.my_instance_variable 或 my_object.my_instance_method()</code></li>
<li>模块的函数和变量：<code>math.sqrt()</code></li>
<li>实例化后的内置类：<code>my_string.upper()</code>，其中 <code>upper()</code> 是字符串(str)对象的一个方法</li>
</ul>
</li>
</ol>
</li>
<li><p>confusion: python func <code>range()</code> 生成有序的整数序列</p>
<ol>
<li><code>range(stop)</code>: start（可选）：序列的起始值，默认为 0</li>
<li><code>range(start, stop)</code>: stop（必需）：序列的结束值，但不包括该值。range() 会生成从 start 到 stop-1 的整数序列</li>
<li><code>range(start, stop, step)</code>: step（可选）：可选参数，控制序列中的值之间的步长，默认为 1, e.g. <code>list(range(0, 21, 5)) == [0, 5, 10, 15, 20]</code></li>
</ol>
</li>
<li><p>confusion: python slicing operation</p>
<ol>
<li>假设data为一维list或str, <code>data[start:end]</code> 选取范围 start -&gt; end-1, step &#x3D;&#x3D; 1; 若start未指定，则默认为0; 若end未指定，则默认选取到最后一个元素(包含最后一个元素)</li>
<li>假设data为多维NumPy array, <code>data[:, start:end:step]</code> 选取 dim&#x3D;&#x3D;0 所有元素, 选取 dim&#x3D;&#x3D;1 的 start -&gt; end-1, step&#x3D;&#x3D;step的元素</li>
<li>假设data为一维list或str, <code>data[-5:]</code> 选取从倒数第5个元素直至末尾最后一个元素,最后一个元素index为-1</li>
</ol>
</li>
<li><p>command: <code>find . -name &quot;*.tar&quot; | while read NAME ; do mkdir -p &quot;$&#123;NAME%.tar&#125;&quot;; tar -xvf &quot;$&#123;NAME&#125;&quot; -C &quot;$&#123;NAME%.tar&#125;&quot;; rm -f &quot;$&#123;NAME&#125;&quot;; done</code> recursively unzip files</p>
<ol>
<li><code>find . -name &quot;*.tar&quot;</code>: 这部分使用 <code>find</code> 命令来在当前目录及其子目录中查找文件名匹配 <code>*.tar</code> 的文件</li>
<li><code>|</code>: 这是管道符号，它将 <code>find</code> 命令的输出（即找到的所有 <code>.tar</code>文件的路径列表）传递给管道符号右侧的命令</li>
<li><code>while read NAME</code>: 这部分创建一个 <code>while</code> 循环，它将逐行读取管道传入的文件路径，并将每行内容赋值给 <code>NAME</code> 变量</li>
<li><code>do</code>: 这标志着 <code>while</code> 循环的开始</li>
<li><code>mkdir -p &quot;$&#123;NAME%.tar&#125;&quot;</code>: 这是在循环中的第一个命令。它使用 <code>mkdir</code> 命令创建目录，并且 <code>-p</code> no error if existing, make parent directories as needed. <code>$&#123;NAME%.tar&#125;</code> 是一种变量扩展，它会从 <code>NAME</code> 变量的值中删除 <code>.tar</code>扩展名，然后创建一个对应的目录</li>
<li><code>tar -xvf &quot;$&#123;NAME&#125;&quot; -C &quot;$&#123;NAME%.tar&#125;&quot;</code>: 这是在循环中的第二个命令。它使用 <code>tar</code> 命令来解压缩 <code>NAME</code>变量中指定的 <code>.tar</code>文件，并将解压后的文件放入对应的目录 <code>$&#123;NAME%.tar&#125;</code>, note: 参数说明 <code>-C, --directory=DIR   change to directory DIR</code> 用于指定解压缩操作的目标目录</li>
<li><code>rm -f &quot;$&#123;NAME&#125;&quot;</code>: 这是在循环中的第三个命令。它使用 <code>rm</code> 命令删除原始的 <code>.tar</code> 文件。<code>-f</code> 选项表示不会询问确认</li>
<li><code>done</code>: 这标志着 <code>while</code> 循环的结束</li>
</ol>
</li>
<li><p>command: <code>python -c &quot;Python code to execute&quot;</code></p>
<ol>
<li><code>-c</code> 参数是 <code>Python</code> 解释器的一个命令行选项，它允许你在命令行中执行一段 <code>Python</code> 代码，而不必编写一个独立的 <code>Python</code> 脚本文件。</li>
</ol>
</li>
<li><p>confusion: <code>dict(xx=yy) 和 dict = &#123;&#39;xx&#39;: yy&#125;</code>异同</p>
<ol>
<li><code>dict(xx=yy)</code> 和 <code>dict = &#123;&#39;xx&#39;: yy&#125;</code> 构造出来的字典在本质上是相同的</li>
<li><code>dict(xx=yy)</code> 的语法是一种关键字参数的方式，其中 <code>xx</code> 是键，<code>yy</code> 是值。这种方式适用于在函数调用中将多个键值对传递给函数，而不需要明确创建字典对象</li>
<li><code>dict = &#123;&#39;xx&#39;: yy&#125;</code> 是显式创建一个字典的方式，其中 <code>&#39;xx&#39;</code> 是键，<code>yy</code> 是值。这种方式更适用于创建独立的字典对象，以便在程序中进行操作和访问</li>
</ol>
</li>
<li><p>confusion: <code>from easydict import EasyDict</code> 作用: 可以像访问属性一样访问字典的值<code>value</code>, 而不必使用<code>my_dict[&#39;key&#39;]</code></p>
</li>
<li><p>confusion: <code>apt</code> 和 <code>dpkg</code></p>
<ol>
<li>dpkg 和 apt 是在 Debian 及其衍生发行版（如 Ubuntu）中用于软件包管理的两个重要工具，它们之间存在密切的关系，但有不同的职责。<ul>
<li>dpkg(Debian Package): dpkg 是底层的软件包管理工具，用于安装、卸载和管理 Debian 系统上的软件包。它直接处理软件包的安装和卸载，以及配置文件的处理。dpkg 可以从本地 .deb 文件安装软件包，也可以从软件源下载并安装软件包。</li>
<li>apt(Advanced Package Tool): apt 是一个高级的软件包管理工具，建立在 dpkg 之上，提供更高级的功能。apt 可以自动解决软件包之间的依赖关系，并处理升级、安装、卸载等操作。它使用软件源（repositories）来获取软件包信息，并允许用户方便地搜索、安装和更新软件。</li>
<li>简而言之，dpkg 是更基础的工具，直接负责软件包的安装和卸载，而 apt 提供了更高级、用户友好的接口，使软件包管理更加方便，它处理了更多的任务，包括依赖解决、更新软件源、搜索软件包等。</li>
</ul>
</li>
</ol>
</li>
<li><p>confusion: <code>SIMD</code> <code>MIMD</code> <code>SIMT</code></p>
<ol>
<li><code>SIMD</code>: SIMD 是指单指令流多数据流（Single Instruction, Multiple Data）的计算模式。在计算机体系结构中，SIMD 是一种并行计算的方式，它允许同时对多个数据执行相同的操作，以提高并行计算的效率。</li>
<li><code>SIMT</code>: 类似于 SIMD，SIMT 是 NVIDIA GPU 中一种并行计算模式，它允许执行单一指令在多个线程上。</li>
<li><code>MIMD</code>: MIMD（Multiple Instruction, Multiple Data）： 在 MIMD 模式中，多个处理单元同时执行不同的指令，处理不同的数据。每个处理单元都有自己的指令流和数据流，可以独立运行。MIMD 适用于处理多个独立的任务，每个任务可能需要不同的指令序列。</li>
</ol>
</li>
<li><p>confusion: <code>GCC编译器</code> <code>Make</code> <code>CMake</code> <code>Makefile</code> <code>CMakeLists.txt</code> 区别</p>
<ul>
<li>GCC 编译器：<ol>
<li>作用： 将高级语言源代码编译成机器码或可执行文件。</li>
<li>使用场景： 用于直接编译源代码，生成可执行文件。</li>
</ol>
</li>
<li>Make：<ol>
<li>作用： 构建工具，根据 Makefile 中定义的规则和依赖关系来管理和调度项目(调用GCC编译器)的构建过程。</li>
<li>使用场景： 用于自动化构建过程，确保只有发生更改的文件被重新编译。</li>
</ol>
</li>
<li>Makefile：<ol>
<li>作用： 文本文件，包含项目的构建规则、依赖关系和编译动作。</li>
<li>使用场景： Make 工具根据 Makefile 中的规则来判断哪些文件需要重新构建，然后调用适当的编译器。</li>
</ol>
</li>
<li>CMake：<ol>
<li>作用： 生成用于不同构建系统的构建配置文件，如 Makefile。</li>
<li>使用场景： 提供跨平台的构建配置，允许开发者在不同的构建系统上使用相同的配置。</li>
<li>makefile在一些简单的工程完全可以人工手下，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。这时候就出现了Cmake这个工具，cmake就可以更加简单的生成makefile文件给上面那个make用。当然cmake还有其他功能，就是可以跨平台生成对应平台能用的makefile，你不用再自己去修改了</li>
</ol>
</li>
<li>CMakeLists.txt：<ol>
<li>作用： 文本文件，包含 CMake 的配置和项目信息。</li>
<li>使用场景： CMake 使用 CMakeLists.txt 来生成项目的构建配置文件，其中定义了项目的结构、依赖关系和编译选项。</li>
<li>cmake根据什么生成makefile呢？它又要根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile。 CMakeLists.txt文件谁写啊？亲，是你自己手写的。</li>
</ol>
</li>
<li>总结：<br>GCC 编译器是用于将源代码编译为机器码的工具。<br>Make 是一个构建工具，使用 Makefile 来自动管理项目的构建过程。<br>CMake 是一个用于生成跨平台构建配置的工具，可以生成 Makefile 或其他构建系统的配置文件。<br>Makefile 包含构建项目所需的规则和命令，由 Make 工具读取执行。<br>CMakeLists.txt 包含项目的配置信息和结构，由 CMake 解析生成构建配置。</li>
</ul>
</li>
<li><p>confusion: ssh 添加 public_key 至目标远程主机实现无需密码登录</p>
<ul>
<li>本地host <code>ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com</code><ol>
<li>-t rsa: 指定密钥类型为 RSA。</li>
<li>-b 4096: 指定密钥的位数，4096 位是一种常见的安全选择。</li>
<li>-C “<a href="mailto:&#121;&#x6f;&#117;&#x72;&#x5f;&#x65;&#x6d;&#97;&#x69;&#x6c;&#x40;&#x65;&#120;&#97;&#x6d;&#112;&#108;&#101;&#46;&#99;&#x6f;&#109;">&#121;&#x6f;&#117;&#x72;&#x5f;&#x65;&#x6d;&#97;&#x69;&#x6c;&#x40;&#x65;&#120;&#97;&#x6d;&#112;&#108;&#101;&#46;&#99;&#x6f;&#109;</a>“: 在生成的密钥中添加注释，通常使用你的电子邮件地址。</li>
</ol>
</li>
<li>将生成的ssh public key (id_rsa.pub) 复制到远程target 的 <code>~/.ssh/authorized_keys</code> (如果没有则自建此文件)中即可</li>
</ul>
</li>
<li><p>confusion: <code>chmod mode file</code> 中的 mode 含义 -&gt; 用每个分组读写操作权限用3bit表示， 从左到右依次是 <code>rwx</code></p>
<ul>
<li>示例： <code>drwxr--r--</code> : d 代表 directory 目录， 所有者(user)拥有权限 read write 没有 execute,用数字表示为 <code>6 = 4 + 2 + 0</code> ,群组(group)和其他(others)只有权限 read, 数字表示为 <code>4 = 4 + 0 + 0</code>, 综上此文件的 mode 为 <code>644</code></li>
</ul>
</li>
<li><p>confusion: ubuntu 除用 <code>ifconfig</code> 查看本机ip之外，还可以用 <code>ip addr</code> (较新的ubuntu默认安装了 <code>ip</code> 命令), 一般前者失效时后者有效，不用再安装 <code>net-tools</code> 包</p>
</li>
<li><p>confusion: <code>register_buffer()</code></p>
<ul>
<li><code>register_buffer</code> 是 PyTorch 中 nn.Module 类提供的一个方法，用于注册一个缓冲张量。这个缓冲张量不会被视为模型的参数，但会被包含在模型的状态中，并在模型的 <code>state_dict</code> 中保存。这通常用于存储不需要优化的固定参数，比如在模型中使用的常数或预先计算好的张量。</li>
</ul>
</li>
<li><p>confusion: Slurm（Simple Linux Utility for Resource Management）作业调度系统</p>
</li>
<li><p>confusion: github 克隆别人的仓库，修改更新后如何推送到自己的 github 账户下</p>
<ul>
<li><p>新建 github 上我的空repo, 可以不需要 readme.md 和 license, 因为克隆别人仓库一般都有</p>
</li>
<li><p>clone github 上他人仓库，重命名仓库文件夹名称使之与github上我的repo同名</p>
</li>
<li><p>git remote 设定远端repo是我的github新建的repo <code>git remote set-url origin https://github.com/kyrie2to11/gptq_test.git</code></p>
</li>
<li><p>修改本地仓库文件后，推送到远端github repo  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git status <span class="comment"># 查看本地仓库哪些文件被修改了</span></span><br><span class="line">git add . <span class="comment"># 把修改的文件放入staging area,准备commit</span></span><br><span class="line">git commit -m <span class="string">&quot;commit remark message&quot;</span> <span class="comment"># 本地仓库正式commit更改</span></span><br><span class="line">git push origin main <span class="comment"># 将本地修改同步到github repo</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>confusion: 更改ubuntu root密码 <code>sudo passwd</code> 或者 <code>sudo passwd root</code></p>
</li>
<li><p>confusion: python 切片 slicing 语法 <code>[start:stop:step]</code>  </p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start：起始位置的索引。</span><br><span class="line">stop：终止位置的索引（不包含在切片内）。</span><br><span class="line">step：步长，表示每次移动的距离。</span><br></pre></td></tr></table></figure>

<p>如果不指定这些值，默认值为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start 默认为第一个元素（索引 0）。</span><br><span class="line">stop 默认为最后一个元素的下一个位置（即列表的长度）。</span><br><span class="line">step 默认为 1。</span><br></pre></td></tr></table></figure>

<p>当你使用 [::-1] 时，两个冒号 : 表示没有指定 start 和 stop，因此默认取整个序列。而 -1 作为步长表示从最后一个元素开始，以步长为 1 的方向逐步向前取值，实现反向取值。</p>
</li>
<li><p><code>results_list = self.get_bboxes(*outs, img_metas=img_metas, rescale=rescale)</code> 中 <code>*outs</code>的作用</p>
<ul>
<li>在这个上下文中，*outs 是一种使用在函数调用中的语法，它表示将一个可迭代对象（比如列表或元组）中的元素分别传递给函数作为独立的参数。这个语法称为“拆包”（unpacking）。</li>
<li>具体到你的代码中，outs 可能是一个包含多个元素的可迭代对象（比如元组），而 self.get_bboxes 函数的参数需要这些元素作为独立的参数传递进去。使用 *outs 就能够方便地将 outs 中的元素拆包传递给函数。</li>
<li>这种方式在函数参数数量不确定，或者希望通过一个可迭代对象传递参数时非常有用。在这里，*outs 将 outs 中的内容展开，作为 self.get_bboxes 函数的独立参数传递给函数。</li>
</ul>
</li>
</ol>
<h2 id="Metrics（Full-Reference）"><a href="#Metrics（Full-Reference）" class="headerlink" title="Metrics（Full-Reference）"></a>Metrics（Full-Reference）</h2><ol>
<li>Peak Signal to Noise Ratio (PSNR)</li>
<li>Structural SIMilarity (SSIM)</li>
</ol>
<h2 id="Milestone"><a href="#Milestone" class="headerlink" title="Milestone"></a>Milestone</h2><table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>VapSR_4_1</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: RELU, Attention using Partial conv</td>
<td>REDS</td>
<td>27.790268</td>
<td>0.77721727</td>
<td>59,468</td>
<td>654.0 (INT8_CPU)</td>
<td>7.462</td>
</tr>
<tr>
<td>SWAT_0</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;4)</td>
<td>REDS</td>
<td>27.842232</td>
<td>0.77754354</td>
<td>50,624</td>
<td>271.0   (FP16_CPU)</td>
<td>5.803</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em>  </p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Source</th>
<th>Dataset</th>
<th>Test PSNR</th>
<th>Test SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Diggers</td>
<td>Real-Time Video Super-Resolution based on Bidirectional RNNs(2021 SOTA)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.98</td>
<td>-</td>
<td>39,640</td>
<td>-</td>
</tr>
<tr>
<td>2</td>
<td>VSR_12</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.981062</td>
<td>0.7824855</td>
<td>57,696</td>
<td>62.8</td>
</tr>
</tbody></table>
<h2 id="PaperWriting"><a href="#PaperWriting" class="headerlink" title="PaperWriting"></a>PaperWriting</h2><h2 id="No-1"><a href="#No-1" class="headerlink" title="No.1"></a>No.1</h2><ol>
<li>BSConvU as shallow feature extraction</li>
</ol>
<h2 id="PaperReference"><a href="#PaperReference" class="headerlink" title="PaperReference"></a>PaperReference</h2><ol>
<li>Rethinking Alignment in Video Super-Resolution Transformers(NIPS 2022) -&gt; VIT 视频超分(VSR)中帧&#x2F;特征对齐不是必要操作</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/09/13/Competition_0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/13/Competition_0/" class="post-title-link" itemprop="url">Single Image Depth Estimation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-13 21:10:10" itemprop="dateCreated datePublished" datetime="2023-09-13T21:10:10+08:00">2023-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 11:46:21" itemprop="dateModified" datetime="2024-12-02T11:46:21+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Time-2023-09-13-2023-10-05"><a href="#Time-2023-09-13-2023-10-05" class="headerlink" title="Time: 2023.09.13-2023.10.05"></a>Time: 2023.09.13-2023.10.05</h2><h2 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><ol>
<li>HR-WSI: Structure-Guided Ranking Loss for Single Image Depth Prediction</li>
<li>Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset</li>
<li>DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data</li>
<li>ReDWeb V1: Monocular Relative Depth Perception with Web Stereo Data Supervision</li>
<li>The Replica Dataset: A Digital Replica of Indoor Spaces</li>
<li>Taskonomy: Disentangling Task Transfer Learning</li>
</ol>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><ul>
<li><p>authority recommend</p>
<ol>
<li>ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth (arXiv 2023.02)</li>
<li>Vision Transformers for Dense Prediction (ICCV 2021)</li>
<li>Learning to Recover 3D Scene Shape from a Single Image (CVPR 2021)</li>
</ol>
</li>
<li><p>lightweight SIDE research</p>
<ol>
<li>Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation (arXiv 2023.09)<ul>
<li><strong>fully convolutional depth estimation network</strong> using <strong>contextual feature fusion</strong></li>
<li>use <strong>high-resolution and low-resolution features</strong> to reserve information on small targets and fast-moving objects <strong>instead of long-range fusion</strong></li>
<li>employing lightweight <strong>channel attention</strong> based on convolution in the <strong>decoder stage</strong></li>
</ul>
</li>
<li>RT-MonoDepth: Real-time Monocular Depth Estimation on Embedded Systems (arXiv 2023.08)<ul>
<li>Fast inference based on convolution: RT-MonoDepth and RT-MonoDepthS, runs at 18.4&amp;30.5 FPS on NVIDIA Jetson Nano and 253.0&amp;364.1 FPS on NVIDIA Jetson AGX Orin on a single RGB image of resolution 640×192, and achieve relative stateof-the-art accuracy on the KITTI dataset.</li>
<li>Encoder (downsample inputs): 4-layer pyramid convolution encoder, removing the normalization layer, standard convolutions instead of depth-wise separable convolution.</li>
<li>Decoder (upsample and fuse): upsampling -&gt; 3 × 3 depth-wise separable convolution followed by nearest-neighbor interpolation with a scale factor of 2; fusion -&gt; mixed use of element-wise addition and concatenate; prediction -&gt; convs + activating functions: leakyReLU, sigmoid.</li>
</ul>
</li>
<li>Lightweight Monocular Depth Estimation via Token-Sharing Transformer (2023 IEEE International Conference on Robotics and Automation (ICRA), CCF-B)<ul>
<li>Token-Sharing Transformer (TST): On the NYU Depth v2 dataset, TST can deliver depth maps up to 63.4 FPS in NVIDIA Jetson nano and 142.6 FPS in NVIDIA Jetson TX2.</li>
<li>Design concept: hierarchy-focused architecture (gradually reduces the resolutions of tokens) + <strong>bottleneck-focused architecture</strong> (bottleneck-focused architecture reduces the resolution through CNN and applies self-attention only in low-resolution tokens)</li>
</ul>
</li>
<li>Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation (CVPR 2023)<ul>
<li>efficient combination of CNNs and Transformers: Consecutive Dilated Convolutions (CDC) module -&gt; shallow CNNs with dilated convolution to enhance local features; Local-Global Features Interaction (LGFI) module -&gt; cross-covariance attention to compute the attention along the feature channels.</li>
</ul>
</li>
<li>Boosting LightWeight Depth Estimation via Knowledge Distillation (International Conference on Knowledge Science, Engineering and Management, KSEM 2023, CCF-C)<ul>
<li>lightweight network (MobileNet-v2 Encoder, Channel-wise attention) + <strong>Promoting KD with Auxiliary Data</strong></li>
</ul>
</li>
<li>Lightweight Monocular Depth Estimation with an Edge Guided Network (2022 17th International Conference on Control, Automation, Robotics and Vision, ICARCV, CORE Computer Science Conference Rankings: A)<ul>
<li>Preliminary: edge information are important cues for convolutional neural networks (CNNs) to estimate depth.</li>
<li>Encoder-Decoder Architecture:<ol>
<li>Multi-scale Feature Extractor -&gt; MobileNetV2 as the backbone</li>
<li>Edge Guidance Branch -&gt; <strong>guiding depth estimation</strong></li>
<li>Transformer-Based Feature Aggregation Module</li>
</ol>
</li>
</ul>
</li>
<li>Lightweight Monocular Depth Estimation through Guided Decoding (2022 International Conference on Robotics and Automation (ICRA), CCF-B)<ul>
<li>lightweight encoder-decoder architecture for embedded platforms + <strong>Guided Upsampling Block</strong></li>
<li>inference:<ol>
<li>NYU Depth V2: 35.1 fps on the NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX</li>
<li>KITTI: 23.7 fps on the Jetson Nano and 102.9 fps on the Xavier NX</li>
</ol>
</li>
</ul>
</li>
<li>MobileXNet: An Efficient Convolutional Neural Network for Monocular Depth Estimation (IEEE Transactions on Intelligent Transportation Systems, 2022, CCF-B)<ul>
<li>Encoder-Decoder style CNN architecture: Conv, DWConv, DilatedConv, Bilinear Upsampling</li>
<li>To penalize the errors around edges -&gt; hybrid loss: the regular L1 loss + the image gradient-based L1 loss</li>
</ul>
</li>
</ol>
</li>
<li><p>others</p>
<ol>
<li>DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation (arXiv 2023.08)<ul>
<li>Paradigm innovation: regression or classification -&gt; denoising diffusion</li>
</ul>
</li>
<li>Edge-guided occlusion fading reduction for a light-weighted self-supervised monocular depth estimation (arXiv 2019.11)<ul>
<li>Atrous Spatial Pyramid Pooling (ASPP) -&gt; (Dilated&#x2F;Atrous Convolution) reduce the computational costs</li>
<li>Edge-Guided post-processing -&gt; reduce the occlusion fading</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><ol>
<li><p><strong>相对误差（Relative Error，REL）</strong>：</p>
<ul>
<li>相对误差用于度量模型估计的深度值与真实深度值之间的相对差异。</li>
<li>公式：$REL &#x3D; \frac{|D_{\text{est}} - D_{\text{gt}}|}{D_{\text{gt}}}$</li>
</ul>
</li>
<li><p><strong>均方根误差（Root Mean Square Error，RMSE）</strong>：</p>
<ul>
<li>均方根误差衡量模型估计值与真实值之间的绝对差异，通过平方差的平均值再开平方根得到。</li>
<li>公式：$RMSE &#x3D; \sqrt{\frac{1}{N} \sum (D_{\text{est}} - D_{\text{gt}})^2}$</li>
</ul>
</li>
<li><p><strong>平均绝对误差（Mean Absolute Error，MAE）</strong>：</p>
<ul>
<li>平均绝对误差度量估计深度值与真实深度值之间的平均绝对差异。</li>
<li>公式：$MAE &#x3D; \frac{1}{N} \sum |D_{\text{est}} - D_{\text{gt}}|$</li>
</ul>
</li>
<li><p><strong>对数均方根误差（Log Root Mean Square Error，Log-RMSE）</strong>：</p>
<ul>
<li>对数均方根误差在对数尺度上度量估计深度值与真实深度值之间的均方根差异。</li>
<li>公式：$Log-RMSE &#x3D; \sqrt{\frac{1}{N} \sum (\log(D_{\text{est}} + \epsilon) - \log(D_{\text{gt}} + \epsilon))^2}$</li>
<li>这里的$\epsilon$是一个小的常数，通常用于避免对数中的除零错误。</li>
</ul>
</li>
</ol>
<h2 id="Milestones"><a href="#Milestones" class="headerlink" title="Milestones"></a>Milestones</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/07/12/Git_Commands/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/12/Git_Commands/" class="post-title-link" itemprop="url">Git Commands</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-07-12 14:33:49" itemprop="dateCreated datePublished" datetime="2023-07-12T14:33:49+08:00">2023-07-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-01-03 13:48:29" itemprop="dateModified" datetime="2024-01-03T13:48:29+08:00">2024-01-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <table>
<thead>
<tr>
<th>Operation</th>
<th>Commander</th>
</tr>
</thead>
<tbody><tr>
<td>初始化本地仓库</td>
<td><code>git init</code></td>
</tr>
<tr>
<td>添加文件到Git暂存区</td>
<td><code>git add &lt;文件名&gt;</code> 或 <code>git add .</code></td>
</tr>
<tr>
<td>提交暂存区的文件到本地仓库</td>
<td><code>git commit -m &quot;提交消息&quot;</code></td>
</tr>
<tr>
<td>关联本地仓库与远程仓库</td>
<td><code>git remote add origin &lt;远程仓库URL&gt;</code></td>
</tr>
<tr>
<td>推送本地仓库的代码到远程仓库</td>
<td><code>git push origin &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>克隆远程仓库到本地</td>
<td><code>git clone &lt;远程仓库URL&gt;</code></td>
</tr>
<tr>
<td>拉取远程仓库的更新到本地</td>
<td><code>git pull origin &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>创建一个新的分支</td>
<td><code>git branch &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>切换到指定分支</td>
<td><code>git checkout &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>查看当前分支</td>
<td><code>git branch</code></td>
</tr>
<tr>
<td>查看仓库的状态</td>
<td><code>git status</code></td>
</tr>
<tr>
<td>创建并切换到一个新的分支</td>
<td><code>git checkout -b &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>合并指定分支到当前分支</td>
<td><code>git merge &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>查看提交历史记录</td>
<td><code>git log</code></td>
</tr>
<tr>
<td>撤销工作目录中的修改</td>
<td><code>git restore &lt;文件名&gt;</code></td>
</tr>
<tr>
<td>创建标签并附上注释</td>
<td><code>git tag -a &lt;标签名&gt; -m &quot;标签注释&quot;</code></td>
</tr>
<tr>
<td>查看标签列表</td>
<td><code>git tag</code></td>
</tr>
<tr>
<td>切换到指定标签</td>
<td><code>git checkout &lt;标签名&gt;</code></td>
</tr>
<tr>
<td>同步远程仓库的分支列表</td>
<td><code>git remote update origin --prune</code></td>
</tr>
<tr>
<td>查看远程仓库列表</td>
<td><code>git remote -v</code></td>
</tr>
<tr>
<td>从本地仓库中删除指定分支</td>
<td><code>git branch -d &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>从远程仓库中删除指定分支</td>
<td><code>git push origin --delete &lt;分支名&gt;</code></td>
</tr>
<tr>
<td>撤销上一次提交</td>
<td><code>git revert HEAD</code></td>
</tr>
<tr>
<td>撤销上一次提交并丢弃相关的修改</td>
<td><code>git reset HEAD~</code> 或 <code>git reset &lt;提交ID&gt;</code></td>
</tr>
<tr>
<td>撤销上一次提交并保留相关的修改</td>
<td><code>git reset HEAD~ --soft</code> 或 <code>git reset &lt;提交ID&gt; --soft</code></td>
</tr>
<tr>
<td>解决合并冲突后，继续合并操作</td>
<td><code>git merge --continue</code></td>
</tr>
<tr>
<td>取消合并操作</td>
<td><code>git merge --abort</code></td>
</tr>
<tr>
<td>生成 SSH 密钥</td>
<td><code>ssh-keygen -t rsa -b 4096 -C &quot;你的邮箱地址&quot;</code></td>
</tr>
<tr>
<td>查看公钥内容</td>
<td><code>cat ~/.ssh/id_rsa.pub</code></td>
</tr>
<tr>
<td>添加 SSH 密钥至 GitHub</td>
<td><code>github GUI operation</code></td>
</tr>
</tbody></table>
<p><em>More info:</em> <a target="_blank" rel="noopener" href="https://www.runoob.com/git/git-tutorial.html">https://www.runoob.com/git/git-tutorial.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/07/07/Work_Log_2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/07/Work_Log_2/" class="post-title-link" itemprop="url">Video Super-Resolution Quantization Work Log</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-07-07 14:31:56" itemprop="dateCreated datePublished" datetime="2023-07-07T14:31:56+08:00">2023-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 12:02:41" itemprop="dateModified" datetime="2024-12-02T12:02:41+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Video-Super-Resolution-Quantization-Time-2023-07-07-2023-08-07"><a href="#Video-Super-Resolution-Quantization-Time-2023-07-07-2023-08-07" class="headerlink" title="Video Super-Resolution Quantization (Time:2023.07.07-2023.08.07)"></a>Video Super-Resolution Quantization (Time:2023.07.07-2023.08.07)</h2><h2 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h2><ol>
<li>Dynamic Network Quantization for Efficient Video Inference <strong>(ICCV2021)</strong><ul>
<li><strong>Feat</strong>: selects optimal precision for each frame conditioned on the input for efficient video recognition</li>
</ul>
</li>
<li>ResQ: Residual Quantization for Video Perception <strong>(ICCV2023)</strong><ul>
<li><strong>Feat</strong>: difference in network activations between two neighboring frames, exhibit properties that make them highly quantizable</li>
</ul>
</li>
<li>QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution <strong>(NIPS2023)</strong><ul>
<li>To overcome the representation homogeneity caused by quantization in the network, we introduce the Redistribution-driven Learnable Quantizer (RLQ). This is accomplished through an inference-agnostic efficient redistribution design, which adds additional information in both forward and backward passes to improve the representation ability of quantized networks. (为了克服网络中量化造成的表示同质性，我们引入了重分布驱动的可学习量化器 (RLQ)。这是通过与推理无关的高效重分布设计实现的，它在前向和后向传递中添加了额外信息，以提高量化网络的表示能力。)</li>
<li>Furthermore, to achieve flexible inference and break the upper limit of accuracy, we propose the Depth-dynamic Quantized Architecture (DQA). Our DQA allows for the trade-off between efficiency and accuracy during inference through weight sharing.(此外，为了实现灵活的推理并突破准确率的上限，我们提出了深度动态量化架构（DQA）。我们的DQA通过权重共享，实现了推理过程中效率和准确率之间的平衡。)</li>
</ul>
</li>
<li>Knowledge Distillation for Optical Flow-Based Video Superresolution <strong>(JCSE2023)</strong><ul>
<li><strong>Feat</strong>: Video super-resolution; Optical flow; Knowledge distillation;</li>
</ul>
</li>
<li>EDVR: Video Restoration with Enhanced Deformable Convolutional Networks <strong>(NTIRE2019)</strong></li>
<li>leverage temporal redundancies to accelerate video processing<ol>
<li>Towards High Performance Video Object Detection for Mobiles <strong>(MSRA_arxiv2018)</strong></li>
<li>Temporally Distributed Networks for Fast Video Semantic Segmentation <strong>(CVPR2020)</strong><ul>
<li><strong>Feat</strong>: 在连续帧上用前层网络获取浅层特征，通过 attention 将当前帧前的浅层特征传播到当前帧来近似得到在当前帧上使用深层网络获取深层特征的效果。在分割任务上简单高效</li>
</ul>
</li>
<li>Mobile Video Object Detection with Temporally-Aware Feature Maps <strong>(CVPR2018)</strong><ul>
<li><strong>Feat</strong>: 来自之前帧的 hidden state 当作 temperal information 增强当前帧的目标检测效果</li>
</ul>
</li>
<li>Low-Latency Video Semantic Segmentation <strong>(CVPR2018)</strong><ul>
<li><strong>Feat</strong>: 视频语义分割 当前帧处理受之前帧中间特征影响，判断是否为关键帧，关键帧用高计算强度的模块处理</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><ol>
<li>需要搞清楚 basicvsr++ 模型接受的输入是怎样的，模型的大致处理过程是怎样的? input example: torch.Size([1, 141, 3, 240, 320]) -&gt; finish</li>
<li>需要搞清楚 test 加载数据计算指标的 pipeline? 成功, test 结果如下： -&gt; finish<ol>
<li>orig: <code>07/11 20:25:27 - mmengine - INFO - Iter(test) [4/4]    REDS4-BIx4-RGB/PSNR: 32.3965  REDS4-BIx4-RGB/SSIM: 0.9075  data_time: 13.1019  time: 57.3645</code></li>
<li>curret_best: <code>07/12 17:36:58 - mmengine - INFO - Iter(test) [4/4]    REDS4-BIx4-RGB/PSNR: 25.3909  REDS4-BIx4-RGB/SSIM: 0.6822  data_time: 12.9891  time: 64.2525</code></li>
<li>current_now: <code>07/12 22:35:48 - mmengine - INFO - Iter(test) [4/4]    REDS4-BIx4-RGB/PSNR: 25.3899  REDS4-BIx4-RGB/SSIM: 0.6821  data_time: 13.4293  time: 71.2697</code></li>
<li>current_all:<ol>
<li><code>REDS4-BIx4-RGB/PSNR: 25.3908  REDS4-BIx4-RGB/SSIM: 0.6821</code></li>
<li><code>Vimeo-90K-T-BDx4-Y/PSNR: 29.6901  Vimeo-90K-T-BDx4-Y/SSIM: 0.8333  Vimeo-90K-T-BIx4-Y/PSNR: 30.3137  Vimeo-90K-T-BIx4-Y/SSIM: 0.8437</code></li>
<li><code>UDM10-BDx4-Y/PSNR: 30.7291  UDM10-BDx4-Y/SSIM: 0.8677</code></li>
<li><code>VID4-BDx4-Y/PSNR: 22.9580  VID4-BDx4-Y/SSIM: 0.5820  VID4-BIx4-Y/PSNR: 23.2985  VID4-BIx4-Y/SSIM: 0.5998</code></li>
</ol>
</li>
</ol>
</li>
<li>如何降低量化时间，提升量化后效果？ -&gt; cease<ol>
<li>current: the calibration time is 16175.18998336792 s 约 4.5 h) 暂时无解</li>
</ol>
</li>
<li>EDVR 在 REDS 上测试? -&gt; cease<ol>
<li>orig_0: <code>07/16 16:04:32 - mmengine - INFO - Iter(test) [400/400]    REDS4-BIx4-RGB/PSNR: 24.7137  SSIM: 0.6305  data_time: 0.1508  time: 0.5635</code></li>
<li>orig_1: <code>07/16 16:15:56 - mmengine - INFO - Iter(test) [400/400]    REDS4-BIx4-RGB/PSNR: 23.5544  SSIM: 0.6249  data_time: 0.1505  time: 0.5589</code></li>
<li>orig_2: <code>07/16 18:42:34 - mmengine - INFO - Iter(test) [400/400]    REDS4-BIx4-RGB/PSNR: 23.8858  REDS4-BIx4-RGB/SSIM: 0.6057  data_time: 0.1552  time: 0.5929</code></li>
</ol>
</li>
<li>转向在 VSR 小模型上测试量化算法的效果 -&gt; cease<ol>
<li>小的视频超分模型几乎都有各自的特点：有用剪枝的 有突出功耗低的 有用重参数化技巧的 种种已有特点不适合再叠加量化算法</li>
</ol>
</li>
<li>转向在 SISR 模型上测试量化算法的效果 -&gt; cease</li>
<li>尝试其它轻量化技巧，聚焦移动设备应用<ol>
<li>结构重参数</li>
</ol>
</li>
</ol>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><ol>
<li>PSNR</li>
<li>SSIM</li>
<li>Memory</li>
<li>Latency</li>
</ol>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Milestone-0"><a href="#Milestone-0" class="headerlink" title="Milestone_0"></a>Milestone_0</h3><table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Source</th>
<th>Dataset</th>
<th>PSNR</th>
<th>SSIM</th>
<th>Memory</th>
<th>Latency</th>
</tr>
</thead>
</table>
<h3 id="Milestone-1"><a href="#Milestone-1" class="headerlink" title="Milestone_1"></a>Milestone_1</h3><h2 id="PaperWriting"><a href="#PaperWriting" class="headerlink" title="PaperWriting"></a>PaperWriting</h2><h3 id="No-1"><a href="#No-1" class="headerlink" title="No.1"></a>No.1</h3><h2 id="PaperReference"><a href="#PaperReference" class="headerlink" title="PaperReference"></a>PaperReference</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/04/10/Group_Week_Report/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/10/Group_Week_Report/" class="post-title-link" itemprop="url">Group Week Report</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-04-10 10:47:22" itemprop="dateCreated datePublished" datetime="2023-04-10T10:47:22+08:00">2023-04-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:37:47" itemprop="dateModified" datetime="2024-12-02T15:37:47+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="模型压缩与部署组工作进度-（2023-4-3-2023-4-9）"><a href="#模型压缩与部署组工作进度-（2023-4-3-2023-4-9）" class="headerlink" title="模型压缩与部署组工作进度 （2023.4.3-2023.4.9）"></a>模型压缩与部署组工作进度 （2023.4.3-2023.4.9）</h2><h3 id="高扬城"><a href="#高扬城" class="headerlink" title="高扬城"></a>高扬城</h3><ul>
<li>I-ViT模块复现：完成I-LayerNorm的复现；</li>
<li>基金申请审稿意见回复；</li>
<li>长江流域非法活动监测项目初版方案及可行性分析；</li>
<li>组会ppt编写；</li>
</ul>
<h3 id="李亚伟"><a href="#李亚伟" class="headerlink" title="李亚伟"></a>李亚伟</h3><ul>
<li>workshop<ul>
<li><p>在用L1 charbonnier损失进行预训练后，继续使用L2损失训练 -&gt; PSNR：27.76 -&gt; 27.81 上升</p>
</li>
<li><p>改进注意力模块：1.增大感受野 2.部分卷积用分组卷积替代 -&gt; Params: 25,664 -&gt; 24,160 下降 FLOPs: 2.949 -&gt; 2.776 下降，但是runtime反而上涨了 27.8 -&gt; 30.0，tflite对分组卷积算子的支持不好</p>
</li>
<li><p>目前最好结果</p>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SWAT_5</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 250,000</td>
<td>REDS</td>
<td>27.811176</td>
<td>0.7763541</td>
<td>25,664</td>
<td>27.6 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
</tbody></table>
<hr>
</li>
</ul>
</li>
<li>work<ul>
<li>搜集在REDS数据集上完全相同实验设置的paper，汇总相关指标情况</li>
<li>剪枝&#x2F;权重聚类的代码之前在基于单帧的单输入单输出模型上跑通，现模型多输入多输出，进行调整后现已跑通</li>
</ul>
</li>
</ul>
<h2 id="后期计划"><a href="#后期计划" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="高扬城-1"><a href="#高扬城-1" class="headerlink" title="高扬城"></a>高扬城</h3><ul>
<li>完成I-ViT模型复现；</li>
<li>完成组会ppt编写；</li>
<li>尝试在Jetson nano或raspberry pi上使用TVM进行模型部署；</li>
</ul>
<h3 id="李亚伟-1"><a href="#李亚伟-1" class="headerlink" title="李亚伟"></a>李亚伟</h3><ul>
<li>work<ul>
<li>完成当前多输入多输出模型的INT8&#x2F;FP16的量化部分</li>
</ul>
</li>
</ul>
<h2 id="模型压缩与部署组工作进度-（2023-7-03-2023-7-16）"><a href="#模型压缩与部署组工作进度-（2023-7-03-2023-7-16）" class="headerlink" title="模型压缩与部署组工作进度 （2023.7.03-2023.7.16）"></a>模型压缩与部署组工作进度 （2023.7.03-2023.7.16）</h2><h3 id="李亚伟-2"><a href="#李亚伟-2" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>video super-resolution on mobile device<ul>
<li>FANI 代码整理，上传github</li>
</ul>
</li>
<li>NeurIPS 审稿</li>
<li>补充PPT: 模型压缩部署部分</li>
<li>Jetson Nano 部署 ZeroDCE,远远达不到实时性要求,处理单张512×512图片暗光增强耗时 &gt; 2 min。具体结果如下：<ul>
<li><img src="/Week_Report/ZeroDCE_test_result.jpg" alt="Elapsed Time(ms)/photo"></li>
</ul>
</li>
</ol>
<h3 id="苗康"><a href="#苗康" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>调研了李亚伟推荐的几篇量化文献:ZeroQ,HAWQ-V3，调试了 micronet 项目上几个量化操作的 demo:QAT&#x2F;PTQ -&gt; QAFT</li>
<li>请假回家，处理家里一些杂事</li>
</ol>
<h2 id="后期计划-1"><a href="#后期计划-1" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="李亚伟-3"><a href="#李亚伟-3" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>调研了解最新压缩量化进展，寻找下个工作方向</li>
<li>8-bit 浮点数量化项目(FP8 quantization)高通已开源，测试了解下有无follow的空间</li>
<li>trt_pose 姿态估计项目摄像头随动功能实现</li>
<li>Bingda机器人小车文档学习<ul>
<li><img src="/Week_Report/Robot_Tutorial.png" alt="Robot Tutorial"></li>
</ul>
</li>
</ol>
<h3 id="苗康-1"><a href="#苗康-1" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>和模型压缩组内成员讨论下一步的选题方向</li>
<li>熟悉模型压缩方向的最新进展</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-8-07-2023-8-13）"><a href="#模型压缩与部署组工作进度-（2023-8-07-2023-8-13）" class="headerlink" title="模型压缩与部署组工作进度 （2023.8.07-2023.8.13）"></a>模型压缩与部署组工作进度 （2023.8.07-2023.8.13）</h2><h3 id="苗康-2"><a href="#苗康-2" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>撰写 icdm审稿意见</li>
<li>参与韦炎炎师兄项目书撰写，即围绕“复杂环境下对监控画面进行增强和实时分析”主题，调研了两个比较细分的小方向，一篇综述是Areview ofcomputer visionbased structuralhealth monitoring at localand globallevels，利用计算机视觉对建筑进行健康检测;另一篇综述是 Anomaly Detection in RoadTrafic Using Visual Surveillance:ASurvey，调查了基于计算机视觉和视觉监控的技术来理解交通违规或其他类型的道路异常的相关技术</li>
<li>撰写开题报告，以轻量化超分为主题</li>
</ol>
<h3 id="李亚伟-4"><a href="#李亚伟-4" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>PRCV审稿</li>
<li>FP8 Quantization 调研<ul>
<li>FP8 Quantization: The Power of the Exponent (Qualcomm_NeurIPS 2022)<ol>
<li>FP8更适应离群值多的场景</li>
<li>PTQ时精度优于INT8，QAT时精度比INT8略差</li>
</ol>
</li>
<li>FP8 FORMATS FOR DEEP LEARNING (NVIDIA&#x2F;Arm&#x2F;Intel_ArXiv 2022.09) -&gt; 训练推理统一数据格式FP8<ol>
<li>FP8 可以加速训练和减少训练所需的资源，同时方便部署且可以保证训练出的精度</li>
<li>INT8 量化模型通常需要进行校准或微调，训练与推理数据类型不一致不便于部署，且通常精度会下降</li>
</ol>
</li>
<li>FP8 versus INT8 for efficient deep learning inference (Qualcomm_ArXiv 2023.06) -&gt; FP8 目前在性能和精度上不能取代INT8推理，目前INT4-INT8-INT16是边缘端推理的最优解<ol>
<li>PTQ时在离群值显著的情况下，FP8相较INT8有精度优势; 通常这种情况可以通过W8A16混合精度以及QAT来解决</li>
<li>FP8推理硬件开销大, FP8 MAC 单元效率比 INT8 低50%至180%</li>
<li>为了更高效，已经有一些INT4量化的工具, 但到目前为止并没有FP4相关的工作</li>
</ol>
</li>
<li>Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models (MSRA_ArXiv 2023.05) -&gt; Layer wise混合精度LLM</li>
</ul>
</li>
<li>Jetson Nano 部署暗光增强 ZeroDCE++,处理单张512×512图片耗时约10ms,但有波动(最高4931.46 ms&#x2F;张),基本满足实时性要求<ul>
<li><img src="Week_Report/ZeroDCE++_test_result.png" alt="Elapsed Time(ms)/photo" width="500" height="300"></li>
</ul>
</li>
<li>Jetson Nano 部署 Face Tracking,结合之前的Pose Estimation 达不到实时30 frame&#x2F;s的要求<ul>
<li><img src="Week_Report/face_detection_screenshot.png" alt="Elapsed Time(ms)/photo" width="700" height="300"></li>
<li><img src="Week_Report/pose_estimation_screenshot.png" alt="Elapsed Time(ms)/photo" width="700" height="300"></li>
</ul>
</li>
</ol>
<h2 id="后期计划-2"><a href="#后期计划-2" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-3"><a href="#苗康-3" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>确定项目书的模板论文，补充项目书</li>
<li>寻找新工作的方向</li>
</ol>
<h3 id="李亚伟-5"><a href="#李亚伟-5" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>撰写开题报告</li>
<li>尝试集成Face Tracking 和 Pose Estimation实现相机角度跟随人体并进行姿态估计</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-8-14-2023-8-27）"><a href="#模型压缩与部署组工作进度-（2023-8-14-2023-8-27）" class="headerlink" title="模型压缩与部署组工作进度 （2023.8.14-2023.8.27）"></a>模型压缩与部署组工作进度 （2023.8.14-2023.8.27）</h2><h3 id="苗康-4"><a href="#苗康-4" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>完成了项目申请书的撰写工作，在师兄的指导下修改了项目书的背景，相关工作，技术路线等内容</li>
<li>开题报告</li>
<li>调研并学习了商汤的MOBench 量化工具，针对不同工作的训练pipeline存在差异导致复现结果不同，其提供了统一的理论算法和量化策略。打算作为接下来一段时间的研究方向</li>
</ol>
<h3 id="李亚伟-6"><a href="#李亚伟-6" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>机载广域持续监视方案调研,PPT制作</li>
<li>开题报告</li>
<li>jetson nano 项目：Face Tracking + Pose Estimation<ul>
<li>原有基于nvidia官方trt_pose项目的姿态估计推理速度慢,现基于Shanghai AI Lab 2023最新的轻量姿态估计项目RTMPose进行部署</li>
<li>调研了解商汤MMdeploy 和 MMPose项目，编译安装相关依赖并在jetson nano上搭建了部署环境</li>
<li>完成了驱动舵机调整摄像头位置的C++代码，后续通过ctypes库实现在py文件中调用此部分调整摄像头姿态的C++代码</li>
</ul>
</li>
</ol>
<h2 id="后期计划-3"><a href="#后期计划-3" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-5"><a href="#苗康-5" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>深入了解运用MQBenche</li>
</ol>
<h3 id="李亚伟-7"><a href="#李亚伟-7" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>完成jetson nano 项目：Face Tracking + Pose Estimation</li>
<li>调研了解TensorRT&#x2F;TNN&#x2F;MNN&#x2F;NCNN等推理框架，重点尝试运用TensorRT加速RTMPose的推理</li>
<li>参加大湾区算法比赛： 视频插帧 + 单目深度估计</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-9-25-2023-10-08）"><a href="#模型压缩与部署组工作进度-（2023-9-25-2023-10-08）" class="headerlink" title="模型压缩与部署组工作进度 （2023.9.25-2023.10.08）"></a>模型压缩与部署组工作进度 （2023.9.25-2023.10.08）</h2><h3 id="苗康-6"><a href="#苗康-6" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>参加视频插帧赛道的比赛，结果不太行。</li>
<li>和师兄交流，修改icdm论文里的语法、格式问题。</li>
<li>找到一篇23CVPR的论文“CABM: Content-Aware Bit Mapping for Single Image Super-Resolution Network with Large Input”，其动机与我icdm大致相同，区别在于这篇论文为每个patch选择量化比特，我的论文为每个patch选择block数。而且这篇论文很大程度上借鉴了21CVPR “ARM: Any-Time Super-Resolution Method”依然能中，说明这个方向仍然可以继续探索。</li>
</ol>
<h3 id="王明申"><a href="#王明申" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li><p>审硕士论文抽检</p>
</li>
<li><p>和师兄一起参加大湾区比赛，熟悉比赛流程，学习不同任务的模型调参工作等。</p>
</li>
<li><p>EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models（arXiv 23.10.05）PTQ的时间 QAT的精度</p>
<ul>
<li>动机：现有PTQ量化在DM中W4A4量化时无法产生较好效果，QLoRA缺点：无法将LoRA权重与量化模型权重相融合。</li>
<li>提出量化感知低秩适配器QALoRA，将LoRA权重与FP模型权重合并共同量化至目标位宽，权重量化：channel-wise，激活量化：layer-wise。</li>
<li>Activation量化：将LSQ量化方法运用在每一步去噪步骤中，单独优化激活量化尺度。</li>
</ul>
</li>
</ol>
<h3 id="李亚伟-8"><a href="#李亚伟-8" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>大湾区单目深度估计比赛：<ul>
<li>数据的理解存在偏差，涉及共计6个不同数据集的ground truth, label的标签意义未能理解清(如单位mm还是m, skymask, validmask等等)</li>
<li>选择部分结构清晰(仅包含imgs, gts)的数据集送入目前的SOTA模型 ZoeDepth 对其 metric bins module 进行微调，结果训练后的精度比原作只在 NYU Depth V2 数据集上进行微调的效果还差</li>
<li>目前的提交的结果：A榜 42&#x2F;60, B榜决定最终排名尚未出结果</li>
</ul>
</li>
<li>ICDM camera ready 版本修改&#x2F;国奖申请答辩</li>
</ol>
<h2 id="后期计划-4"><a href="#后期计划-4" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-7"><a href="#苗康-7" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>研究CABM的量化部分，然后迁移到我借鉴的baseline论文“Adaptive patch<br>   exiting for scalable single image super-resolution”里看看效果。</li>
</ol>
<h3 id="王明申-1"><a href="#王明申-1" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>从ECCV 2022 CADyQ和CVPR2023 CABM 两篇论文中，寻找优化量化SISR任务的角度。</li>
<li>从其他方向寻找量化工作角度，如大模型Diffusion Model。</li>
</ol>
<h3 id="李亚伟-9"><a href="#李亚伟-9" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>参照SISR量化思路(ECCV 2022 CADyQ, WACV 2022 DAQ)，搭建基于目前SOTA模型 (BasicVSR++&#x2F;VRT&#x2F;RVRT) 的量化baseline</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-10-09-2023-10-15）"><a href="#模型压缩与部署组工作进度-（2023-10-09-2023-10-15）" class="headerlink" title="模型压缩与部署组工作进度 （2023.10.09-2023.10.15）"></a>模型压缩与部署组工作进度 （2023.10.09-2023.10.15）</h2><h3 id="苗康-8"><a href="#苗康-8" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>注册提交icdm终稿</li>
<li>研究几篇超分量化相关工作ARM、CABM、CADYQ代码，ARM代码有一处存在疑问，作者暂未回复</li>
<li>确定工作思路，即基于patch确定对应的网络block数，再根据block数确定量化位数，不过原理方面解释性不强</li>
</ol>
<h3 id="王明申-2"><a href="#王明申-2" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>复现SR量化任务APE、CADyQ、CABM模型中baseline效果</li>
<li>寻找SR量化trick</li>
<li>EQ-Net：Elastic Quantization Neural Networks（ICCV 2023）<br>OFA 动机：不同硬件支持的量化形式多样，现有解决方案局限性需要迭代训练优化<ul>
<li>对于权重量化提出从偏度和峰度信息正则化</li>
<li>提出GPG类似知识蒸馏结构组渐进式指导，CQAP MLP结构选择粒度和对称性，最后用遗传算法加快搜索</li>
</ul>
</li>
</ol>
<h3 id="李亚伟-10"><a href="#李亚伟-10" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>Video Super-Resolution Quantization<ul>
<li>基于目前在Vid4、Vimeo90k、REDS数据集上SOTA模型 BasicVSR++ 进行channel-wise distribution-aware 量化pipeline的搭建（目前尚没有视频超分量化超分方向的baseline，代码难度较大）</li>
<li>尝试引入在其它视频感知任务（Human Pose Estimation，Semantic Segmentation，Video Object Segmentation）有效上的方法，参考 ICCV2023 ResQ 将网络中相邻帧的激活之间的残差用于量化，更小的方差有利于缩小量化误差<ul>
<li><img src="Week_Report/ResQ.png" alt="ResQ Motivation" width="500" height="300"></li>
</ul>
</li>
</ul>
</li>
<li>ICDM注册提交</li>
</ol>
<h2 id="后期计划-5"><a href="#后期计划-5" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-9"><a href="#苗康-9" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>根据上述工作思路，继续搭建模型架构，同时查找相关论文，思索更好的原理解释</li>
</ol>
<h3 id="王明申-3"><a href="#王明申-3" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>继续寻找SR量化trick</li>
<li>深度阅读现有SR量化工作代码</li>
</ol>
<h3 id="李亚伟-11"><a href="#李亚伟-11" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>结合 ResQ 完成 BasicVSR++ 量化pipeline的搭建</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-10-16-2023-10-22）"><a href="#模型压缩与部署组工作进度-（2023-10-16-2023-10-22）" class="headerlink" title="模型压缩与部署组工作进度 （2023.10.16-2023.10.22）"></a>模型压缩与部署组工作进度 （2023.10.16-2023.10.22）</h2><h3 id="苗康-10"><a href="#苗康-10" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>继续上周计划，基于patch做超分量化任务，调试代码，修改模型结构</li>
<li>修改专利</li>
<li>调研了几篇剪枝超分方面的文章，Aligned Structured Sparsity Learning for Efficient Image Super-Resolution (nips2021),  Learning efficient image super-resolution networks through structural regular pruning (ICLR2022), 结论是由于超分网络存在不少跳连和残差，剪枝在超分领域应用的泛化性不是很好</li>
</ol>
<h3 id="王明申-4"><a href="#王明申-4" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>SCIS审稿</li>
<li>通过pdb方式阅读SR量化代码</li>
</ol>
<h3 id="李亚伟-12"><a href="#李亚伟-12" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>Video Super-Resolution Quantization<ul>
<li>参考 GPTQ 完成了 BasicVSR++ (未涉及ViT)量化的基础部分</li>
<li>阅读论文,了解其它几个SOTA模型(ViTs)是否有需要单独改进的模块：<ul>
<li>CVPR2022: TTVSR</li>
<li>NIPS2022: PSRT, RVRT</li>
<li>CVPR2023: IART</li>
</ul>
</li>
</ul>
</li>
<li>SelecQ latex 排版调整,期刊注册提交</li>
<li>学校HPC实例到期, 实验室浪潮集群上 Docker 镜像搭建</li>
<li>专利修改</li>
</ol>
<h2 id="后期计划-6"><a href="#后期计划-6" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-11"><a href="#苗康-11" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>继续搭建模型结构</li>
</ol>
<h3 id="王明申-5"><a href="#王明申-5" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>阅读ICCV2023中Workshop关于Low-Bit Quantized Neural Networks的汇报</li>
<li>阅读Transformer或LLM有关量化的文章</li>
</ol>
<h3 id="李亚伟-13"><a href="#李亚伟-13" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>结合 ResQ 改进视频超分ViTs量化模块</li>
<li>深度神经网络课程PPT制作</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-10-23-2023-10-29）"><a href="#模型压缩与部署组工作进度-（2023-10-23-2023-10-29）" class="headerlink" title="模型压缩与部署组工作进度 （2023.10.23-2023.10.29）"></a>模型压缩与部署组工作进度 （2023.10.23-2023.10.29）</h2><h3 id="苗康-12"><a href="#苗康-12" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>初步搭建出基于patch的超分量化框架，效果很差，判定是代码问题，又因为存在创新型的问题，暂时放一放</li>
<li>与组内协作，参与视频超分工作，研究BasicVsr++视频超分模型。</li>
</ol>
<h3 id="王明申-6"><a href="#王明申-6" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li><p>阅读论文</p>
<ul>
<li>VSR：BasicVSR（CVPR 2021）、BasicVSR++（CVPR2022）</li>
<li>Quantization：<ul>
<li>EfficientViT（ICCV 2023 MIT Transformer轻量化 对分类、分割、复原三个领域中general的模型轻量化且效果极好）</li>
<li>Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective（CVPR 2023 从量化误差导致震荡角度出发，优化量化结果）</li>
</ul>
</li>
</ul>
</li>
<li><p>跑通BasicVSR++代码</p>
</li>
<li><p>通过TensorRT量化框架对Yolov7模型实现自动插入量化节点量化，mAP掉了0.03%</p>
</li>
</ol>
<h3 id="李亚伟-14"><a href="#李亚伟-14" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>Video Super-Resolution Quantization<ul>
<li>试用百度 paddleslim 分别用静态动态量化（PTQ）对 BasicVSR++ 进行量化</li>
</ul>
</li>
<li>深度神经网络课程PPT制作</li>
</ol>
<h2 id="后期计划-7"><a href="#后期计划-7" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-13"><a href="#苗康-13" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>参与视频超分工作</li>
</ol>
<h3 id="王明申-7"><a href="#王明申-7" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>阅读量化文章</li>
<li>学习网络模型量化，并完成VSR任务量化</li>
</ol>
<h3 id="李亚伟-15"><a href="#李亚伟-15" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>配合完成 paddleslim 量化 BasicVSR++</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-10-30-2023-11-05）"><a href="#模型压缩与部署组工作进度-（2023-10-30-2023-11-05）" class="headerlink" title="模型压缩与部署组工作进度 （2023.10.30-2023.11.05）"></a>模型压缩与部署组工作进度 （2023.10.30-2023.11.05）</h2><h3 id="李亚伟-16"><a href="#李亚伟-16" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>Video Super-Resolution Quantization<ul>
<li>BasicVSR++ PTQ： 量化过程有bug正在解决<ol>
<li>BasicVSR++ torch模型转onnx模型并检查</li>
<li>激活校准，产出量化参数: scale zero_point</li>
<li>权重调整，提升量化精度</li>
<li>量化误差分析，定位量化问题</li>
</ol>
</li>
<li><strong>note:</strong> 目前 BasicVSR++ 的 PTQ 基于开源工具 Dipoorlet 进行，优点代码简洁明了易修改，相较百度框架 paddleslim 便于快捷验证idea; VSR 量化方法成熟后可进一步迁移至 paddleslim</li>
</ul>
</li>
<li>读文献找idea提升PTQ精度</li>
</ol>
<h3 id="苗康-14"><a href="#苗康-14" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li><p>一篇 Neural Network 审稿工作</p>
</li>
<li><p>参考 paddleslim 官方文档示例完成yolov5 PTQ:</p>
<ul>
<li>将pytorch的pt文件转化为onnx格式</li>
<li>将onnx文件输入paddleslim执行脚本输出模型及权重文件</li>
<li>迁移部署到tensorRT平台部分暂不清楚</li>
<li>结果： <img src="Week_Report/paddleslim_demo.png" alt="Paddleslim demo" width="800" height="200"></li>
</ul>
</li>
</ol>
<h3 id="王明申-8"><a href="#王明申-8" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>阅读论文<ul>
<li>VSR：ESPCN(CVPR 2017)</li>
<li>Quantization：<br>1. Efficient LLM Inference on CPUs (arxiv 2311.00502, Intel 精度几乎无损)<br>2. DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks (WACV 2022 ISRQ; 在Ablation Study中分析了 Gaussian, Uniform, Laplacian, Gamma分布对channel-wise量化的影响)</li>
</ul>
</li>
<li>ICASSP 2024审稿</li>
<li>完成对YOLOv7网络模型的手动量化节点插入，并通过敏感层分析，逐层网络分析及打印量化对精度影响最大的Top10层。</li>
</ol>
<h2 id="后期计划-8"><a href="#后期计划-8" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-15"><a href="#苗康-15" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>paddleslim yolov5 PTQ 过程迁移到 BasicVSR++ 上</li>
<li>尝试将 ResQ 思路用 paddleslim 实现</li>
</ol>
<h3 id="王明申-9"><a href="#王明申-9" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>阅读量化文章，寻找新的视频超分量化idea</li>
<li>学习网络模型量化</li>
</ol>
<h3 id="李亚伟-17"><a href="#李亚伟-17" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>解决 dipoorlet 量化 BasicVSR++ 遇到的bug</li>
</ol>
<h2 id="模型压缩与部署组工作进度-（2023-11-06-2023-11-12）"><a href="#模型压缩与部署组工作进度-（2023-11-06-2023-11-12）" class="headerlink" title="模型压缩与部署组工作进度 （2023.11.06-2023.11.12）"></a>模型压缩与部署组工作进度 （2023.11.06-2023.11.12）</h2><h3 id="苗康-16"><a href="#苗康-16" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>深度神经网络原理课程ppt制作。</li>
<li>利用paddleslim其中的AutoCompression接口对yolov5进行自动压缩（包括量化和蒸馏两部分），代码中numpy库的api有冲突，在调试。</li>
<li>利用3090比较了各类框架在yolov5上的部署测试。<img src="Week_Report/3090_qyolov5.png" alt="qyolov5" width="800" height="200"></li>
</ol>
<h3 id="王明申-10"><a href="#王明申-10" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>阅读论文<ul>
<li>VSR：DRDVSR(CVPR 2018)</li>
<li>Quantization：Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks(解决正则化损失与复原损失冲突问题)</li>
</ul>
</li>
<li>Neurocomputing审稿</li>
<li>完成YOLOv7网络PTQ、QAT量化学习，从手动加入QDQ节点，到逐层分析量化的敏感度，对于敏感度高的层进行处理，对输入Concat节点前的多个输出节点做统一scale处理，最后通过训练迭代优化量化损失，导出量化模型的ONNX模型。</li>
<li>分析VSR量化任务的难点，早期VSR任务模型具有更简单的网路结构，近几年的VSR任务模型结构中可能含有不利于通用量化的网络模块，这样就需要手动去加入适配的QDQ节点，难度大且无法做公平的对比实验。</li>
</ol>
<h3 id="李亚伟-18"><a href="#李亚伟-18" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li><p>Video Super-Resolution Quantization</p>
<ul>
<li>BasicVSR++ 采用 <code>Dipoorlet</code> PTQ： 量化过程有不支持动态输入的问题, 即不支持视频随机长度(time_step)的问题, github提了issue 暂未有回复</li>
<li>BasicVSR++ 采用 <code>MQBench</code> PTQ: BasicVSR++ 模型 forward 过程存在动态控制流, 即控制流的判断条件含有运算变量(Input&#x2F;Activation)参与, 而<code>MQBench</code>调用 <code>torch.fx</code> 的 <code>symbolic_trace</code> 完成 forward 过程计算图捕捉, 其本身的限制不支持动态控制流。正尝试：<ol>
<li>把模型的动态控制流用静态的代替</li>
<li>torch 2.0 新发布的 <code>torch.compile</code> 也即 (TorchDynamo), 了解后尝试来解决模型 forward 中广泛存在的动态控制流</li>
</ol>
</li>
</ul>
</li>
<li><p>RustDesk 中继服务搭建, 降低远程桌面的延迟</p>
</li>
</ol>
<h2 id="后期计划-9"><a href="#后期计划-9" class="headerlink" title="后期计划"></a>后期计划</h2><h3 id="苗康-17"><a href="#苗康-17" class="headerlink" title="苗康"></a>苗康</h3><ol>
<li>解决自动压缩的bug, 采用paddleslim对yolov5 PTQ, 分析其量化分析工具及精度重构工具。</li>
</ol>
<h3 id="王明申-11"><a href="#王明申-11" class="headerlink" title="王明申"></a>王明申</h3><ol>
<li>从较早的VSR任务模型开始着手, 类比用TensorRT框架对YOLOv7的量化尝试对VSR模型进行量化。</li>
<li>学习对于不同结构的量化op方法。</li>
</ol>
<h3 id="李亚伟-19"><a href="#李亚伟-19" class="headerlink" title="李亚伟"></a>李亚伟</h3><ol>
<li>推进 VSR 模型的常规量化(Naive PTQ)的工作</li>
<li>实习相关工作</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/02/17/Workshop_Log_0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/17/Workshop_Log_0/" class="post-title-link" itemprop="url">MAI 2023 Mobile VSR Workshop Log</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-17 10:29:03" itemprop="dateCreated datePublished" datetime="2023-02-17T10:29:03+08:00">2023-02-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:23:03" itemprop="dateModified" datetime="2024-12-02T15:23:03+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Workshop-and-Challenges-CVPR-2023"><a href="#Workshop-and-Challenges-CVPR-2023" class="headerlink" title="Workshop and Challenges @ CVPR 2023"></a>Workshop and Challenges @ CVPR 2023</h2><ol>
<li>Efficient Super-Resolution Challenge(ESR)<ul>
<li>经典baseline:<ul>
<li>information multi-distillation block,IMDN (2019)</li>
<li>Residual feature distillation block,RFDN (2020)</li>
<li>Residual Local Feature Network,RLFN (ByteESR2022)</li>
</ul>
</li>
<li>初期调试跑起来时，目录名称有一点变化就会在别处导致意想不到的错误:(</li>
<li>很多队伍都用到了Quantization Aware Training (QAT)</li>
<li>2022参赛上榜的网络结构和权重都有提供</li>
<li>results</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val Time [ms]</th>
<th>Params [M]</th>
<th>FLOPs [G]</th>
<th>Acts [M]</th>
<th>Mem [M]</th>
<th>Conv</th>
</tr>
</thead>
<tbody><tr>
<td>trained_rfdn_best</td>
<td>DIV2K_val(801-900)</td>
<td>28.73</td>
<td>37.62</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_1</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>41.38</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_2</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>43.86</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_3</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>37.59</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_4</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>34.20</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>IMDN_baseline_1</td>
<td>DIV2K_val(801-900)</td>
<td>29.13</td>
<td>45.11</td>
<td>0.894</td>
<td>58.53</td>
<td>154.14</td>
<td>471.78</td>
<td>43</td>
</tr>
<tr>
<td>IMDN_baseline_2</td>
<td>DIV2K_val(801-900)</td>
<td>29.13</td>
<td>45.03</td>
<td>0.894</td>
<td>58.53</td>
<td>154.14</td>
<td>471.78</td>
<td>43</td>
</tr>
<tr>
<td>IMDN_baseline_3</td>
<td>DIV2K_val(801-900)</td>
<td>29.13</td>
<td>44.44</td>
<td>0.894</td>
<td>58.53</td>
<td>154.14</td>
<td>471.78</td>
<td>43</td>
</tr>
</tbody></table>
<hr>
</li>
<li>Mobile AI workshop 2023<ul>
<li>测试可以用自己手机，也可使用提供的远程设备(速度慢有延迟)</li>
<li>2022 tracks</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Track</th>
<th>Sponsor</th>
<th>Evaluate_Platform</th>
<th>Final_Phase_Team&#x2F;Participants</th>
</tr>
</thead>
<tbody><tr>
<td>Bokeh Effect Rendering 背景虚化</td>
<td>Huawei</td>
<td>Kirin 9000’s Mali GPU</td>
<td>6&#x2F;90</td>
</tr>
<tr>
<td>Depth Estimation</td>
<td>Raspberry Pi</td>
<td>Raspberry Pi 4</td>
<td>7&#x2F;70</td>
</tr>
<tr>
<td>Learned Smartphone ISP</td>
<td>OPPO</td>
<td>Snapdragon’s 8 Gen 1</td>
<td>11&#x2F;140</td>
</tr>
<tr>
<td>Image Super-Resolution</td>
<td>Synaptics</td>
<td>Synaptics VS680</td>
<td>28&#x2F;250</td>
</tr>
<tr>
<td>Video Super-Resolution</td>
<td>MediaTek 联发科</td>
<td>MediaTek Dimensity 9000</td>
<td>11&#x2F;160</td>
</tr>
</tbody></table>
<hr>
<ul>
<li>2021 tracks</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Track</th>
<th>Sponsor</th>
</tr>
</thead>
<tbody><tr>
<td>Learned Smartphone ISP</td>
<td>MediaTek 联发科</td>
</tr>
<tr>
<td>Image Denoising</td>
<td>Samsung</td>
</tr>
<tr>
<td>Image Super-Resolution</td>
<td>Synaptics</td>
</tr>
<tr>
<td>Video Super-Resolution</td>
<td>OPPO</td>
</tr>
<tr>
<td>Depth Estimation</td>
<td>Raspberry Pi</td>
</tr>
<tr>
<td>Camera Scene Detection</td>
<td>Computer Vision Lab, ETH Zurich, Switzerland</td>
</tr>
</tbody></table>
<hr>
<ol>
<li>计划参加track:Image Super-Resolution 3月份开始 -&gt; 调整为track:Video Super-Resolution</li>
<li>Train 2021 anchor-based plain net (ABPN) 两次<ul>
<li>200 epoch 时报错停掉一次</li>
<li>600 epoch 完整跑完，但loss上下波动不收敛</li>
</ul>
</li>
<li>andriod对原作提供的TF-lite模型进行了测试,测试流程掌握了</li>
</ol>
</li>
</ol>
<h3 id="MobileAI-worshop-Video-Super-Resolution"><a href="#MobileAI-worshop-Video-Super-Resolution" class="headerlink" title="MobileAI worshop: Video Super-Resolution"></a>MobileAI worshop: Video Super-Resolution</h3><ol>
<li><p>papers</p>
<ol>
<li>Ntire 2019 challenge on video super-resolution: Methods and results</li>
<li>Ntire 2020 challenge on image and video deblurring</li>
<li>Pynet-v2 mobile: Efficient on-device photo processing with neural networks<ul>
<li>Image Signal Process(ISP): 手机成像流程 光-&gt;CMOS传感器-&gt;成像引擎ISP-&gt;AI(GPU)-&gt;图片；镜头和CMOS在将光学信号转化为由0、1、0、1组成的数字信号时可能存在细节上的遗漏和错误，而ISP单元的主要任务就是进行“纠错”、“校验”和“补偿”。</li>
<li>pynet模型便于移动端部署的mobile版本目的是end-to-end learned ISP,时间很近:2022 26th International Conference on Pattern Recognition (ICPR). IEEE, 2022.</li>
<li>CNN based</li>
</ul>
</li>
<li>Microisp: Processing 32mp photos on mobile devices with deep learning. In: European Conference on Computer Visio(2022)</li>
<li>Real-Time Video Super-Resolution on Smartphones with Deep Learning,Mobile AI 2021 Challenge: Report<ul>
<li>Results and Discussion<ul>
<li>Team Diggers 冠军方案基于Keras&#x2F;Tensorflow 电子科技大学 唯一一个使用循环连接（recurrent connections）来利用帧间依赖性获取更好重建结果，其他方案都是基于单帧超分的。</li>
</ul>
</li>
</ul>
</li>
<li>Power Efficient Video Super-Resolution on Mobile NPUs with Deep Learning, Mobile AI &amp; AIM 2022 challenge: Report<ul>
<li>tutorial: <a target="_blank" rel="noopener" href="https://github.com/MediaTek-NeuroPilot/mai22-real-time-video-sr">https://github.com/MediaTek-NeuroPilot/mai22-real-time-video-sr</a>. baseline:MobileRNN</li>
<li>scoring: Final Score &#x3D; α · PSNR + β · (1 - power consumption) α &#x3D; 1.66 and β &#x3D; 50，注重PSNR和power consumption两个指标</li>
<li>Discussion:<ul>
<li>The majority of models followed a simple single-frame restoration approach to improve the runtime and power efficiency. 大部分模型技术路线是降低单帧超分的运行时间和能量消耗，网络模型都比较浅</li>
<li>GenMedia Group(一家韩国公司) 基于上年度单帧超分冠军方案ABPN小改进而来，排名第6但psnr:28.40最好,是唯二psnr超过28的方案之一，另一个是221B团队基于RNN的方法</li>
<li>基于RNN的方案推理速度较慢且能耗高</li>
<li>总结：2022年来看设备上的视频超分CNN是适合的，因为CNN取得了runtime energy_consumption restoration_quality 的平衡</li>
</ul>
</li>
</ul>
</li>
<li>Sliding Window Recurrent Network for Efficient Video Super-Resolution<ul>
<li>SWRN makes use of the information from neighboring frames to reconstruct the HR frame. 从相邻帧提取信息来重建高清帧,相比单帧超分的方法有丰富的细节。</li>
<li>An bidirectional hidden state is used to recurrently collect temporal spatial relations over all frames.使用双向隐藏状态来循环收集所有帧的时间空间关系。</li>
<li>Pioneer network: SRCNN</li>
<li>Video super-resolution: the most important parts are frame alignment<ul>
<li>VESPCN and TOFlow: optical flow to align frames</li>
<li>TDAN and EDVR: deformable convolution. Especially, EDVR enjoys the merits of implicit alignment and its PCD module.</li>
<li>Incorporates recurrent networks, use the hidden state to record the important temporal information.</li>
</ul>
</li>
<li>在测试平台Runtime 10.1 ms、 0.80 W@30FPS,最后分数低问题就在这里，PSNR SSIM 比第一名MVideoSR（小米）都要好 -&gt; 寻找加速计算和减小耗能的方法</li>
</ul>
</li>
<li>Lightweight Video Super-Resolution for Compressed Video -&gt; Compression-informed Lightweight VSR (CILVSR)<ul>
<li>Recurrent Frame-based VSR Network (FRVSR, RBPN, RRN)</li>
<li>Spatio-Temporal VSR Network (SOF-VSR, STVSR, TDAN, TOFlow, TDVSR-L)</li>
<li>Generative Adversarial Network (GAN)-based SR Network</li>
<li>Video Compression-informed VSR Network (FAST, COMISR, CDVSR, CIAF)</li>
</ul>
</li>
<li>RCBSR: Re-parameterization Convolution Block for Super-Resolution<ul>
<li>ECBSR baseline</li>
<li>Multiple paths <strong>ECB re-parametrization</strong></li>
<li>FGNAS</li>
</ul>
</li>
<li>Deformable 3D Convolution for Video Super-Resolution<ul>
<li>deformable 3D convolution</li>
</ul>
</li>
<li>Efficient Image Super-Resolution Using Vast-Receptive-Field Attention(VapSR 有torch代码)<ul>
<li>improving the attention mechanism<ul>
<li>large kernel convolutions</li>
<li>depth-wise separable convolutions</li>
<li>pixel normalization -&gt; train steadily</li>
</ul>
</li>
<li>相比bytedance的RLFN -&gt; 性能sota,参数更少</li>
</ul>
</li>
<li>LiDeR: Lightweight Dense Residual Network for Video Super-Resolution on Mobile Devices(无代码)<ul>
<li>针对手机端，结构简单，REDS 320x180 X4 upscaling -&gt; psnr:27.51 ssim:0.769(有疑问这个结果到底是在手机上测出来的还是在手机上?)</li>
<li><strong>REDS 320x180 X4 upscaling</strong> 执行速度快 139FPS -&gt; FSRCNN: 45FPS ESPCN: 52FPS</li>
<li>测试平台：<strong>Tensorflow-lite</strong> <strong>fp16</strong> <strong>TF-Lite GPU delegate</strong> <strong>Xiaomi Mi 11</strong> <strong>Qualcomm Snapdragon 888 SoC, Qualcomm Adreno 660 GPU, and 8 GB RAM</strong></li>
</ul>
</li>
<li>Fast Online Video Super-Resolution with Deformable Attention Pyramid<ul>
<li><p>recurrent VSR architecture based on a deformable attention pyramid (DAP)</p>
</li>
<li><p>对比RRN(mobile_rrn MAI VSR官方用例很慢) -&gt;不适合用到MAI VSR中</p>
<ul>
<li><table>
<thead>
<tr>
<th>Run[ms]</th>
<th>fps[1&#x2F;s]</th>
<th>FLOPs[G]</th>
<th>MACs[G]</th>
</tr>
</thead>
<tbody><tr>
<td><strong>28</strong></td>
<td>35.7</td>
<td>387.5</td>
<td>193.6</td>
</tr>
<tr>
<td><strong>38</strong></td>
<td>26.3</td>
<td>330.0</td>
<td>164.8</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><p>2022 challenge methods (ranked)</p>
<ol>
<li><p>MVideoSR(无代码)</p>
<ul>
<li>paper title: ELSR: Extreme Low-Power Super Resolution Network For Mobile Devices</li>
<li>affiliation: Video Algorithm Group, Camera Department, Xiaomi Inc., China</li>
<li>methods:<ol>
<li><strong>core idea</strong>: mobile friendly network which consumes as little energy as possible, discard some complex operations such as optical flow, multi-frame feature alignment, and <strong>start from single frame baselines</strong>.</li>
<li><strong>multi-branch distillation structure</strong> show significant increase in energy consumption while  a slight increase in PSNR compared with the plain convolutional network of similar parameters. abandon multi-branch network architectures, and focus on plain convolutional SR networks.</li>
<li>though <strong>attention modules(ESA, CCA and PA)</strong> bring performance improvement, the extra energy consumption introduced is still unacceptable</li>
<li>architeture<ul>
<li>discription: single frame input  which only have 6 layers, of which only 5 have learnable parameters, including 4 Conv layers and a PReLU activation layer. Pixel-Shuffle operation (also known as depth2space) is used at last to upscale the size of output without introducing more calculation. The intermediate feature channels are all set to 6.</li>
<li><img src="/2023/02/17/Workshop_Log_0/MVideoSR_architecture.png" class="" title="fig_1"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>ZX VIP(无代码)</p>
<ul>
<li>paper title: RCBSR: Re-parameterization Convolution Block for Super-Resolution</li>
<li>affiliation: Audio &amp; Video Technology Platform Department, ZTE Corp., China</li>
<li>methods:<ol>
<li><strong>core idea</strong>: trade-off between SR quality and the energy consumption, <strong>ECBSR as baseline</strong>. In consideration of the low power consumption optimize the baseline from three aspects,<strong>network architecture, NAS and training strategy</strong>.</li>
<li><strong>network architecture</strong>:<strong>re-parameterization</strong> technique in the deploy stage, <strong>replace the activate function PReLU with ReLU</strong>.the power consumption of tflite model with ReLU is less than PReLU. Meanwhile there is no apparent discrepancy in PSNR.Finally, in order to further reduce power consumption, the <strong>output of first CNN layer</strong> is added into the backbone output instead of original input because original input needs to be copied the number of channels. We use <strong>sub-pixel convolution</strong> to upsample image in the network.</li>
<li><strong>NAS</strong>: The objective function of FGNAS is <strong>task-specific loss</strong> and <strong>regularizer penalty FLOPs</strong>. FGNAS -&gt; Kim, H., Hong, S., Han, B., Myeong, H., Lee, K.M.: Fine-grained neural architecture search. arXiv preprint arXiv:1911.07478 (2019)</li>
<li><strong>training strategy</strong>:<strong>replace L1 loss function with Charbonnier loss function</strong> because it causes the problem that the restored image is too smooth and lack of sense of reality.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop_Log_0/ZX_VIP_architecture.png" class="" title="fig2"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>Fighter(无代码)</p>
<ul>
<li>title: Fast Real-Time Video Super-Resolution</li>
<li>affiliation: None, China</li>
<li>methods:  <ol>
<li>shallow CNN model with <strong>depthwise separable convolutions</strong> and <strong>one residual connection</strong>. The number of convolution channels in the model was set to 8, the <strong>depth-to-space op</strong> was used at the end of the model to produce the final output.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop_Log_0/Fighter_architecture.png" class="" title="fig3"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>XJTU-MIGU SUPER(无代码)</p>
<ul>
<li>title: Light and Fast On-Mobile VSR</li>
<li>affiliation: School of Computer Science and Technology, Xi’an Jiaotong University, China MIGU Video Co. Ltd, China</li>
<li>methods:<ol>
<li>small CNN-based model. 示意图如下，总共训练了2600 epochs :(</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop_Log_0/XJTU-MIGU_SUPER_architecture.png" class="" title="fig4"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>BOE-IOT-AIBD(无代码)</p>
<ul>
<li>title: Lightweight Quantization CNN-Net for Mobile Video Super-Resolution</li>
<li>affiliation: BOE Technology Group Co., Ltd., China</li>
<li>methods:<ol>
<li>based on the <strong>CNN-Net architecture</strong>, its structure is illustrated in Fig 6. The authors applied <strong>model distillation</strong>, and used the <strong>RFDN CNN</strong> as a teacher model.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop_Log_0/BOE-IOT-AIBD_architecture.png" class="" title="fig5"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>GenMedia Group(无代码)</p>
<ul>
<li>title: SkipSkip Video Super-Resolution</li>
<li>affiliation: GenGenAI, South Korea</li>
<li>methods:<ol>
<li>inspired by the last year’s top solution from the MAI image super-resolution challenge.  <strong>added one extra skip connection to the mentioned anchor-based plain net (ABPN) model</strong>.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop_Log_0/GenMedia-Group_architecture.png" class="" title="fig6"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>NCUT VGroup(无代码)</p>
<ul>
<li>title: EESRNet: A Network for Energy Efficient Super Resolution</li>
<li>affiliation: North China University of Technology, China Institute of Automation, Chinese Academy of Sciences, China</li>
<li>methods:<ol>
<li>also based their solution on the <strong>ABPN model</strong>.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop_Log_0/NCUT-VGroup_architecture.png" class="" title="fig7"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
<li><p>ideas</p>
<ol>
<li><p>尝试BasicVSR++的轻量化</p>
</li>
<li><p>在ABPN的基础上加入BasicVSR++的主要idea进行改进</p>
</li>
<li><p>尝试将Pynet_v2应用于video super_resolution -&gt; relative complicated and tailored for ISP, so halt  </p>
</li>
<li><p>先train MRNN baseline</p>
<ul>
<li><p>环境</p>
<ol>
<li><p>Python&#x3D;&#x3D;3.8.10</p>
</li>
<li><p>Tensorflow-gpu&#x3D;&#x3D;2.9.0</p>
<ul>
<li>查看tensorflow cuda cudnn python 版本对照表： <a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source_windows">https://www.tensorflow.org/install/source_windows</a></li>
</ul>
</li>
<li><p>Cuda&#x3D;&#x3D;11.2</p>
<ul>
<li>CUDA: CUDA是一个<strong>计算平台和编程模型</strong>，用于在GPU上加速应用程序。CUDA版本指的是CUDA软件的版本</li>
<li>CUDA Toolkit: CUDA Toolkit是包含<strong>CUDA库</strong>和<strong>CUDA工具链</strong>的软件包，用于开发和编译CUDA应用程序。<ul>
<li>CUDA库: CUDA 库包含了 CUDA 编程所需的核心库文件，例如 <strong>CUDA Runtime 库、CUDA Driver 库、cuBLAS 库、cuDNN 库</strong>等。这些库文件提供了 GPU 加速的基本功能和算法，是 CUDA 编程的基础。</li>
<li>CUDA工具链：CUDA 工具链则包含了一系列辅助开发和调试 CUDA 程序的工具，例如 <strong>nvcc 编译器、CUDA-GDB 调试器、Visual Profiler 性能分析工具</strong>等。这些工具能够帮助开发者更方便地编写、调试和优化 CUDA 程序。</li>
</ul>
</li>
<li>note: 查看当前安装的显卡驱动最高支持的CUDA版本 nvidia-smi</li>
<li>note: 查看CUDA工具链版本 nvcc –version</li>
<li>CUDA Toolkit 与 Driver Version 对照：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></li>
<li></li>
</ul>
</li>
<li><p>Cudnn&#x3D;&#x3D;v8.7.0</p>
<ul>
<li>官网：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></li>
<li>cat &#x2F;etc&#x2F;os-release 查看linux版本</li>
<li>uname -m 查看cpu架构，cudnn有不同架构的版本 x86_64 PPC SBSA</li>
<li>tar -xvf解压缩后用以下命令安装并赋予所有用户读取权限</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">sudo cp path_to_cudnn/include/cudnn*    /usr/local/cuda-11.2/include</span><br><span class="line">sudo cp path_to_cudnn/lib/libcudnn*    /usr/local/cuda-11.2/lib64</span><br><span class="line">sudo chmod a+r /usr/local/cuda-11.2/include/cudnn*   /usr/local/cuda-11.2/lib64/libcudnn*</span><br></pre></td></tr></table></figure>

<ul>
<li>Cudnn和Cuda 安装完需在&#x2F;etc&#x2F;profile配置环境变量PATH和LD_LIBRARY_PATH</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64</span><br><span class="line">export PATH=$PATH:/usr/local/cuda/bin</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure>

<ul>
<li>可将文件夹 &#x2F;usr&#x2F;local&#x2F;cuda-11.2 与 &#x2F;usr&#x2F;local&#x2F;cuda 软连接起来</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/cuda-11.2 /usr/local/cuda</span><br></pre></td></tr></table></figure>

<ul>
<li><p>也可以通过linux下的<code>update-alternatives</code>命令行工具来进行cuda版本的管理,先用<code>sudo update-alternatives --install /usr/local/cuda(替代项名称) cuda(替代项链表名称) /usr/local/cuda-xx(实际路径) x(优先级)</code>来安装配置cuda的多个替代项,<code>sudo update-alternatives --config cuda</code>切换CUDA默认版本,其本质是更改了以下软连接: <code>/usr/local/cuda -&gt; /etc/alternatives/cuda -&gt; /usr/local/cuda-xx.x</code></p>
</li>
<li><p>用下面的命令查看cudnn版本,新版本查看cuDNN版本的命令为</p>
</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2  # -A 选项用来指定匹配成功的行之后显示2行内容</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>结果</p>
<ol>
<li>用默认config.yml训练太慢了大约需要1周时间，中途停掉了</li>
<li>用改进后config.yml训练。8小时左右训练完成，但是loss很大</li>
<li>结合往年此赛道总结文章放弃训练提供的mobilernn baseline 思考其它基于cnn的模型</li>
</ol>
</li>
</ul>
</li>
<li><p>可以从NTIRE 2022 efficient super-resolution challenge选取baseline运用剪枝蒸馏等改进到移动端</p>
<ul>
<li>course project for NCSU’s Computer Science 791-025: Real-Time AI &amp; High-Performance Machine Learning. 三板斧<ol>
<li>Pruning via NNI</li>
<li>Quantization via NNI</li>
<li>Hyper Parameter Optimization via NNI</li>
<li>Color Optimization: RGB -&gt; YCbCr</li>
</ol>
</li>
<li>选取2022 NTIRE ESR冠军方案RLFN(Byte Dance)作为baseline,先将其模型转换为 tensorflow 版本在 REDS 数据集上直接进行VSR的测试 -&gt; 中间软件依赖兼容性问题放弃RLFN torch-&gt;onnx-&gt;tensorflow路线</li>
<li>直接用tensorflow 重构 RLFN -&gt; train成功但是精度不达标’psnr’: 25.574987, ‘ssim’: 0.69084775，需要调试改进</li>
<li>现在的首要问题是确定自己的tensorflow 版本RLFN 与原作的 torch 版本RLFN 是否一致 -&gt; cease</li>
<li>可以先将其他模型利用torch_to_tensorflow 转化为tensorflow版本模型，并可视化查看效果 -&gt; 可行而且看源代码不复杂，难点在torch onnx onnx-tf tensorflow-gpu 版本对照，静等比赛开始官方scripts</li>
<li>现在当务之急不是版本对照问题需要尽快找到往年的baseline跑起来，改起来 -&gt; 跑此项目了解剪枝 量化 超参调整三板斧实际运用 ：<a target="_blank" rel="noopener" href="https://github.com/briancpark/video-super-resolution.git">https://github.com/briancpark/video-super-resolution.git</a> -&gt; 都是在调库 NNI</li>
<li>Train baseline SWRN：<a target="_blank" rel="noopener" href="https://github.com/shermanlian/swrn">https://github.com/shermanlian/swrn</a><ul>
<li>结构重参数化（structural re-parameterization）:用一个结构的一组参数转换为另一组参数，并用转换得到的参数来参数化（parameterize）另一个结构。只要参数的转换是等价的，这两个结构的替换就是等价的。</li>
<li>先测试提供的ckpt-98 -&gt; 测试结果’psnr’: 27.931335, ‘ssim’: 0.7803563</li>
<li>缩减recon_trunk_forward &#x2F; recon_trunk_backward &#x2F; recon_trunk 的 block_num到2, train from scratch 看结果</li>
</ul>
</li>
<li>按照去年赛道冠军方案MVedioSR的ELSR搭建pipeline<ul>
<li>L1 loss(Mean Absolute Error, MAE) -&gt; 样本预测值与标签之间差的绝对值取平均, 对异常值不敏感,鲁棒性更强; 对于接近零的数, 梯度为常数, 没有逐渐变小的趋势, 容易出现震荡现象</li>
<li>L2 loss(Mean Squared Error, MSE) -&gt; 样本预测值与标签之间平方差取平均, 对异常值敏感,鲁棒性不强; 对于接近零的数, 梯度随着误差的减小而逐渐减小, 避免了震荡现象。</li>
<li>TensorFlow中的内置损失函数非常丰富，包括L1、L2、L1_Charbonnier和MSE等常见的损失函数。这些损失函数都在tf.keras.losses模块中实现。具体来说，可以使用以下函数调用这些损失函数：<ul>
<li>L1损失函数：tf.keras.losses.mean_absolute_error(y_true, y_pred)</li>
<li>L2损失函数：tf.keras.losses.mean_squared_error(y_true, y_pred)</li>
<li>L1_Charbonnier损失函数：可以自定义实现，也可以使用以下库中的实现：TensorFlow Addons（需要单独安装）。</li>
<li>M2损失函数：tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)</li>
<li>note: 这些函数的参数都是y_true和y_pred，分别表示真实值和预测值。</li>
</ul>
</li>
<li>L1 Loss: L1 Loss: $L_1 &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \left| y_i - \hat{y_i} \right|$</li>
<li>L2 Loss (MSE): $L_2 &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \left( y_i - \hat{y_i} \right)^2$</li>
<li>L1 Charbonnier Loss: $L_{Charbonnier} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \sqrt{ \left( y_i - \hat{y_i} \right)^2 + \epsilon^2 }$</li>
<li>M2 Loss: $L_{M2} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \left( \frac{y_i - \hat{y_i}}{y_i + \epsilon} \right)^2$，其中 $\epsilon$ 为一个较小的数，如 $10^{-6}$，用于防止分母为零。</li>
</ul>
</li>
</ul>
</li>
<li><p>可以看看tflite加速的那些operations去更改模型</p>
</li>
<li><p>尝试将torch VapSR 从单图像超分向视频超分迁移</p>
<ul>
<li>构建模型的tensorflow 代码遇到个小坑: tf.keras.Sequential([upconv1,pixel_shuffle,lrelu,upconv2,pixel_shuffle]) 如果用两个一样的pixel_shuffle模块，用tf.keras.Sequential实现的时候必须用两个不一样的名称，否则无论如何Sequential内都只有一个pixel_shuffle模块</li>
<li>‘from .XXX import YYY’ 相对导入，Python 解释器会先从当前目录开始查找指定的模块或包,需要当前current.py文件在一个Python包内（创建一个空的 <strong>init</strong>.py 文件，即可将文件夹视为一个Python包）</li>
<li>[B,H,W,48] - conv1X1升维 -&gt; [B,H,W,64], conv1X1为[64,48,1,1]大小的Tensor</li>
<li>blueprint conv(M: input_channels N: out_channels); BSconvU: 先用M X 1权重向量对输入作通道聚合, 变为只有1个feature map,然后再用N个K X K的卷积输出N个feature map.<ul>
<li><img src="/2023/02/17/Workshop_Log_0/BSconv.png" class="" title="fig8"></li>
</ul>
</li>
</ul>
</li>
<li><p>对VapSR_2 剪枝量化</p>
<ul>
<li><p>列表推导式：list &#x3D; [expression for item in iterable]，其中 expression 是要添加到列表中的表达式，item 是可迭代对象中的每一项，iterable 是要迭代的对象。例如：metric_list &#x3D; [func for name, func in self.metric_functions.items()]</p>
</li>
<li><p>tfmot.sparsity.keras.prune_low_magnitude() 封装vapsr_3中的每一个tf.keras.layers.Conv2D进行剪枝</p>
</li>
<li><p>同一class下def的method默认第一个参数需要为self;一个method调用另一个method需要用 self.def()不能直接用 def()</p>
</li>
<li><p>keras建立网络的方法可以分为keras.models.Sequential() 和keras.models.Model()、继承类三种方式。注意：tensorflow2.* 以后的版本可以直接使用tf.keras.Sequential()和tf.keras.Model()两个类。不用再使用keras.models的API</p>
<ul>
<li>Keras提供两种API：Sequential API和Functional API。Sequential API是一种简单的线性堆叠模型，适用于许多简单的模型。但是，如果我们需要构建更加复杂的模型，比如有多个输入或输出的模型，那么就需要使用Functional API。</li>
<li>Functional API通过tf.keras.Model()实现，它提供了更加灵活的方式来定义模型的结构和层之间的连接。使用Functional API，我们可以创建具有多个输入和输出的模型，可以共享层，可以定义任意的计算图结构等等。相比之下，Sequential API则不能支持这些更高级的模型定义方式。</li>
<li>因此，使用Functional API来构建复杂的模型是更加灵活和强大的选择，而通过tf.keras.Model()实现这个API是为了提供一种方便和一致的方式来定义和构建深度学习模型。</li>
</ul>
</li>
<li><p>&#x2F; 表示普通的除法运算，例如 5 &#x2F; 2 的结果为 2.5。它返回的是一个浮点数，即使两个操作数都是整数。  &#x2F;&#x2F;表示整除运算，例如5 &#x2F;&#x2F; 2 的结果为 2。</p>
</li>
<li><p>&#x3D;和+直接赋值给变量是不好的，因为它们只是简单地创建一个新的变量，而不是对现有变量进行原位操作。assign()和assign_add()是TensorFlow中的<strong>原地操作</strong>，它们直接将结果分配给现有变量，而不是创建一个新的变量。</p>
</li>
<li><p>shell scripts(.sh)添加多行注释：<code>&lt;&lt; COMMENT ... COMMENT</code>, 在 Shell 中，&lt;&lt; 是 Here Document（文档嵌入）的语法，它可以用来将一段文本或代码块嵌入到 Shell 脚本中。</p>
</li>
<li><p>pruning 过程model type 变化</p>
<ol>
<li>initial: type(self.model) &#x3D;&#x3D; &lt;class ‘VapSR_3.vapsr_3’&gt; (i.e. Keras Subclass Model)<ul>
<li>Keras Subclass Model是一种创建自定义模型的方式，相较于Sequential和Functional API而言，其提供更大的灵活性。使用Subclass Model，用户可以通过定义一个继承自tf.keras.Model的Python类来构建模型。使用Subclass Model的优点在于，它可以自由灵活地创建非线性、复杂的模型结构，也可以方便地重复利用模型代码。</li>
</ul>
</li>
<li>apply tensorflow.keras.Model() method -&gt; type(functional_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;</li>
<li>add tfmot.sparsity.keras.prune_low_magnitude() wrapper -&gt; type(pruned_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;; 如果直接调用tfmot.sparsity.keras.prune_low_magnitude(functional_model, **pruning_params)还是会报错：ValueError: Subclassed models are not supported currently. :(</li>
<li>add tfmot.sparsity.keras.prune_low_magnitude() wrapper with another method -&gt; type(pruned_model_1) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;</li>
<li>pruning -&gt; type(pruned_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;</li>
<li>虽然type(pruned_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;，但是传入stripped_pruned_model &#x3D; tfmot.sparsity.keras.strip_pruning(pruned_model)就会报错：ValueError: Expected <code>model</code> argument to be a functional <code>Model</code> instance, but got a subclassed model instead: &lt;keras.saving.saved_model.load.BSConvU object at 0x7f66f06fa520&gt;</li>
<li>pruned_model.layers &#x3D;&#x3D; [&lt;keras.engine.input_layer.InputLayer object at 0x7f66f06fa580&gt;, &lt;keras.saving.saved_model.load.BSConvU object at 0x7f66f06fa520&gt;, &lt;keras.engine.sequential.Sequential object at 0x7f66f06fa4f0&gt;, &lt;keras.layers.convolutional.conv2d.Conv2D object at 0x7f66f0717c40&gt;, &lt;keras.layers.core.tf_op_layer.TFOpLambda object at 0x7f66f80a7fd0&gt;, &lt;keras.engine.sequential.Sequential object at 0x7f66f80a7cd0&gt;]</li>
</ol>
</li>
<li><p>pruning 去除tfmot.sparsity.keras.prune_low_magnitude() wrapper的报错就没停过 -&gt; 直接构建Functional Model VapSR</p>
</li>
<li><p>详细解释 Layer Norm &#x2F; Batch Norm &#x2F; Instance Norm &#x2F; Pixel Norm</p>
<ul>
<li><p>Batch Norm：对每个特征通道（C）进行归一化，使用整个批次（N）中的样本的均值和方差。在每个 batch 的 channel 维度上计算均值和方差。</p>
</li>
<li><p>Layer Norm：对每个样本（N）进行归一化，使用所有特征通道（C）和空间维度（H，W）的均值和方差。在每个 layer 的所有 feature maps 上计算均值和方差。</p>
</li>
<li><p>Instance Norm：对每个样本（N）和每个特征通道（C）进行归一化，使用空间维度（H，W）的均值和方差。在每个 instance 的 channel 维度上计算均值和方差。</p>
</li>
<li><p>Pixel Norm：对每个样本（N）和每个像素位置（H，W）进行归一化，使用所有特征通道（C）的均值和方差。</p>
</li>
<li><img src="/2023/02/17/Workshop_Log_0/BN_LN_IN_PN.png" class="" title="Visualize different normalization methods"></li>
<li><p>组会可以讨论下具体实现（作为最后一小部分）</p>
</li>
<li><p>VapSR 原作者 pixel norm torch 实现  </p>
</li>
<li><pre><code class="python"> class VAB(nn.Module):
     def __init__(self, d_model,d_atten):
         super().__init__()
         self.proj_1 = nn.Conv2d(d_model, d_atten, 1)
         self.activation = nn.GELU()
         self.atten_branch = Attention(d_atten)
         self.proj_2 = nn.Conv2d(d_atten, d_model, 1)
         self.pixel_norm = nn.LayerNorm(d_model)
         default_init_weights([self.pixel_norm], 0.1)
     
     def forward(self, x):
         shorcut = x.clone()
         x = self.proj_1(x)
         x = self.activation(x)
         x = self.atten_branch(x)
         x = self.proj_2(x)
         x = x + shorcut
         x = x.permute(0, 2, 3, 1) #(B, H, W, C)
         x = self.pixel_norm(x)
         x = x.permute(0, 3, 1, 2).contiguous() #(B, C, H, W)

         return x
     参考：https://blog.csdn.net/weixin_39228381/article/details/107939602    
</code></pre>
</li>
<li><p>x &#x3D; tf.constant([[1.,2.,4.,5.,7.,8.],[6.,7.,9.,10.,11.,12.],[2.,3.,5.,6.,8.,9.],[4.,5.,7.,8.,10.,11]])</p>
</li>
<li><p>mean, variance &#x3D; tf.nn.moments(x, axes, shift&#x3D;None, keepdims&#x3D;False, name&#x3D;None) The mean and variance are calculated by <strong>aggregating the contents of x across axes</strong>.  例： tf.nn.moments(x,1)  x.shape &#x3D;&#x3D; [4,2,3] -&gt; mean.shape &#x3D;&#x3D; [4,1,3]</p>
</li>
<li><p>后续还需要花时间搞清楚 tf LayerNormalization GroupNormalization 在axis&#x3D;list&#x2F;tuple多轴的情况下，到底计算了多少mean和variance，换言之如何用这两个built-in layer做到随心所欲的控制normalization的粒度,妥协方法我觉得是利用transpose,转换轴(相当于torch permute)间接实现相关功能。</p>
</li>
</ul>
</li>
<li><p>*args 和 **kwargs 都是 Python 中用于传递可变数量参数的特殊语法。它们的主要区别在于：</p>
<ul>
<li>*args 用于传递可变数量的位置参数，以元组(tuple)的形式传递给函数；</li>
<li>**kwargs 用于传递可变数量的关键字参数，以字典(dictionary)的形式传递给函数。</li>
</ul>
</li>
<li><p>接下来需要尽快完成 pruning clustering quantization pipeline, 将runtime降到10ms左右</p>
</li>
<li><p>递归函数的return不是返回一个值然后程序结束，而是返回一个值到上一层的递归函数，直到return到最外层</p>
</li>
<li><p>add_pruning_wrapper():</p>
<ul>
<li>通过Sequential.add()重建模型,在原模型就是Sequential的时候可行,但是原模型call() method加不进去</li>
<li>原地替换setattr(object, name, new_model)难点:<ol>
<li>递归当前tf.keras.layers.Conv2D不知道所属模块object 和 name</li>
<li>pruned_model &#x3D; copy.deepcopy(model)在复制的pruned_model上应用剪枝封装, subclassed tf.keras.Model() class -&gt; custom object 需要全部重写method: get_config() from_config()</li>
</ol>
</li>
<li>model.__dict__ 与dir(model) 区别<ol>
<li>model.__dict__ 返回一个字典对象，其中键是模型实例的属性名称(可用model.__dict__.keys()访问)，值是对应的属性值(可用model.__dict__.values()访问)。而 dir(model) 返回一个列表对象，其中包含模型实例的所有属性名称。</li>
<li>具体来说，model.__dict__ 只返回<strong>实例自身定义的属性，不包括其继承而来的属性</strong>。而 dir(model) 返回实例的所有属性名称，包括其自身定义的属性和继承而来的属性。</li>
<li>model.__dict__ 返回的字典对象只包含可写的属性。而 dir(model) 返回的属性列表可能包含不可写的属性，例如只读属性或方法等。</li>
</ol>
</li>
<li>pruned_model.layers[3] &#x3D;&#x3D; &lt;keras.layers.convolutional.conv2d.Conv2D object at 0x7ff557c57a60&gt; 这一层是 Keras 自带的 Conv2D 层，而不是通过继承 tf.keras.layers.Layer 类来自定义的。因此，它不会在 __dict__ 属性中出现。</li>
</ul>
</li>
<li><p>strip_pruning_wrapper():</p>
<ul>
<li>tfmot.sparsity.keras.strip_pruning(): Only sequential and functional models are supported for now.  </li>
<li>recursively strip pruning wrapper -&gt; success</li>
</ul>
</li>
<li><p>lr_scheduler: ConsineDecayRestarts</p>
</li>
<li><p>pruning_train, clustering_train loss 与 pretraining train loss 相差很大, 50+ vs 10+ 有点问题</p>
</li>
<li><p>quantization</p>
<ol>
<li><p>tensorflow quantize:</p>
<ul>
<li>def quantize_scope(*args)</li>
<li>def quantize_model(to_quantize, quantized_layer_name_prefix&#x3D;’quant_’)</li>
<li>def quantize_annotate_model(to_annotate)</li>
<li>def _add_quant_wrapper(layer)</li>
<li>def quantize_annotate_layer(to_annotate, quantize_config&#x3D;None)</li>
<li>def quantize_apply(model, scheme&#x3D;default_8bit_quantize_scheme.Default8BitQuantizeScheme(),           quantized_layer_name_prefix&#x3D;’quant_’)</li>
<li>def _extract_original_model(model_to_unwrap)</li>
<li>def _quantize(layer)</li>
<li>def _unwrap_first_input_name(inbound_nodes)</li>
<li>def _wrap_fixed_range(quantize_config, num_bits, init_min, init_max, narrow_range)</li>
<li>def _is_serialized_node_data(nested)</li>
<li>def _nested_to_flatten_node_data_list(nested)</li>
<li>def fix_input_output_range(model, num_bits&#x3D;8, input_min&#x3D;0.0, input_max&#x3D;1.0, output_min&#x3D;0.0, output_max&#x3D;1.0, narrow_range&#x3D;False)</li>
<li>def _is_functional_model(model)</li>
<li>def remove_input_range(model)</li>
</ul>
</li>
<li><p>*与**二者区别,及与C++ 中指针的区别:</p>
<ul>
<li>* 和 ** 都是Python中的特殊符号，用于参数传递和元组、字典的解包操作。它们与C++中的指针有些类似，但也有不同之处。</li>
<li>* 用于元组的解包操作，可以将一个元组中的元素解包成一个一个的单独元素</li>
<li>** 用于字典的解包操作，可以将一个字典中的键值对解包成一个一个的单独键和值</li>
<li>在函数调用时，* 可以用于传递可变数量的位置参数，而 ** 可以用于传递可变数量的关键字参数，如: def foo(*args, **kwargs): …</li>
<li>与C++中的指针类似，* 可以用于声明指针类型的变量，而 ** 则可以用于声明指向指针的指针类型的变量。但与C++不同的是，Python中的指针实际上是<strong>对象的引用</strong>，而不是内存地址，因此没有C++中的指针算术运算和指针类型转换等操作。<ul>
<li>与 C++ 不同的是，Python 中的对象引用是一个高级抽象，它们隐藏了对象的实际内存地址，因此 Python 中的引用和指针不是同一概念。在 Python 中，我们不需要显式地管理内存，而是由 Python 解释器自动处理内存管理的细节。因此，Python 中的引用更像是一个符号，它与实际的内存地址之间存在一个间接的映射关系。</li>
</ul>
</li>
</ul>
</li>
<li><p>self 与 cls:</p>
<ul>
<li>cls 是 Python 中类方法的第一个参数的常规名称。 它指的是类本身而不是类的实例。 它类似于在实例方法中使用 self。</li>
<li>在类方法中，cls 用于访问类级别的属性和方法，以及创建类的新实例。</li>
</ul>
</li>
<li><p>修好bug,在手机上测好 runtime; 目标: PSNR -&gt; 28, SSIM -&gt; 0.8, runtime -&gt; 30ms</p>
<ul>
<li>从VapSR_3_2开始在手机上都跑不通runtime测试了</li>
<li>通过tf.lite.TFLiteConverter.from_saved_model(‘path_to_model’)创建converter,转换为tflite模型后可以通过netron查看模型结构并分析可能的错误</li>
<li>使用tf.lite.TFLiteConverter.from_keras_model()或者tf.lite.TFLiteConverter.from_saved_model()使用创建converter的话总会遭遇两个问题<ol>
<li>model  input_size: [1,1,1,3] output_size[1,1,1,3] 异常</li>
<li>Make sure you apply&#x2F;link the Flex delegate before inference.</li>
<li>综上推荐配合model.save(‘path_to_model’)存为SavedModel格式，然后定义好concrete_func &#x3D; model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY ],使用tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])规避掉这两个问题</li>
</ol>
</li>
</ul>
</li>
<li><p>通过QuantizeConfig和Quantizer配合实现layer activations weights的自定义量化策略</p>
</li>
<li><p>上下文管理器用于管理某个代码块的上下文环境</p>
<ul>
<li>Python 中常见的上下文管理器包括 with open() as f 中的 open() 函数和 with tf.Session() as sess 中的 tf.Session() 函数等。</li>
<li>在 with 代码块结束后，Python 会自动调用上下文管理器的 __exit__ 方法，以确保资源的释放和清理等工作的完成。同时，上下文管理器可以在 __enter__ 方法中完成一些初始化工作。在 with 代码块内部，可以使用上下文管理器返回的对象，来操作上下文环境中的资源</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>tensorflow复现高通torch QuickSRNet 8-bit 量化</p>
<ul>
<li>android_aarch64代表的是基于64位ARM架构的Android设备，也被称为ARMv8-A架构.通常用于高端设备，如智能手机和平板电脑。</li>
<li>android_arm代表的是基于32位ARM架构的Android设备。通常用于低端设备，如廉价智能手机、平板电脑和物联网设备</li>
</ul>
</li>
<li><p>困扰了至少3周的bug： TFlite GPU Delegate init Batch size mismatch -&gt; solved</p>
<ul>
<li>根据<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/issues/34525#issuecomment-564755977">this link</a>提前规避了tflite gpu delegate不支持全连接层，即利用1*1全连接层替代</li>
<li>从0到1一点点逐个测试可能出问题的模块，最终定位在pixel norm模块(由tf.reshape和tf.keras.layers.LayerNormalization构成)，换为LayerNormalization得到解决，PSNR甚至有一点点提升:)</li>
</ul>
</li>
<li><p>奇怪的问题，在转换Functional Model为tflite模型时，import tensorflow.keras.backend as K 在模型中使用k.clip()时总是提示K未定义 -&gt; 直接更换为tf.keras.backend.clip()解决</p>
</li>
<li><p>在训练Mobile VSR小模型时，GPU利用率低的问题</p>
<ol>
<li>不是由于CPU读取处理数据慢造成的，增加线程无效</li>
<li>也不是batch size大造成的，减小batch size无效</li>
<li>想要提高GPU利用率估计有两个途径,一是增大模型而是使用nvidia DALI数据读取加速库</li>
</ol>
</li>
<li><p>感受野(receptive field) 计算</p>
<ul>
<li><p>假设输入图像大小为$W_{in}\times H_{in}$，卷积核大小为$k\times k$，步长为$s$，当前卷积层的感受野大小为$F_{in}$，则下一层的感受野大小$F_{out}$为：</p>
<p> $F_{out} &#x3D; F_{in} + (k - 1) \times \text{dilation rate}$</p>
<p>其中，$\text{dilation rate}$表示卷积核的膨胀率，如果不使用膨胀卷积，则$\text{dilation rate} &#x3D; 1$。如果下一层是池化层，则$s &#x3D; k$，并且不考虑膨胀率。</p>
<p>设输入图像大小为$224\times 224$，第一个卷积层使用$3\times 3$大小的卷积核，步长为1，不使用膨胀卷积。则第一个卷积层的感受野大小为$3$。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>results</p>
</li>
</ol>
<h4 id="milestone-0"><a href="#milestone-0" class="headerlink" title="milestone_0:"></a><strong>milestone_0:</strong></h4><hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SWRN_0</td>
<td>Origin</td>
<td>REDS</td>
<td>27.931335</td>
<td>0.7803562</td>
<td>43,472</td>
<td>25.6</td>
<td></td>
</tr>
<tr>
<td>SWRN_1</td>
<td>recon_trunk block num&#x3D;2</td>
<td>REDS</td>
<td>27.820051</td>
<td>0.77666414</td>
<td>36,512</td>
<td>26.9</td>
<td></td>
</tr>
<tr>
<td>ELSR_0(vsr 22 winner)</td>
<td>Origin</td>
<td>REDS</td>
<td>26.716854</td>
<td>0.73988235</td>
<td>3,468</td>
<td>19.3</td>
<td></td>
</tr>
<tr>
<td>RLFN_0(esr 22 winner)</td>
<td>Origin</td>
<td>REDS</td>
<td>26.78721</td>
<td>0.7389487</td>
<td>306,992</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>VapSR_0</td>
<td>Origin</td>
<td>REDS</td>
<td>28.103758</td>
<td>0.7864979</td>
<td>154,252</td>
<td>5191.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_1</td>
<td>Replace feature extraction conv and VAB’s 2 con1X1 with blueprint conv</td>
<td>REDS</td>
<td>28.02941</td>
<td>0.7845887</td>
<td>155,916</td>
<td>5798.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_2</td>
<td>Replace feature extraction conv with blueprint conv and  reduce Attention’s kernel size&#x3D;3X3</td>
<td>REDS</td>
<td>28.021387</td>
<td>0.7831156</td>
<td>131,276</td>
<td>2694.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_3</td>
<td>Correct custom realization of pixel normalization</td>
<td>REDS</td>
<td>28.018507</td>
<td>0.7836466</td>
<td>131,276</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>VapSR_3_1</td>
<td>Reduce VAB blocks from 11 to 5</td>
<td>REDS</td>
<td>27.826998</td>
<td>0.7771207</td>
<td>73,484</td>
<td>1222.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_3_2</td>
<td>Realize Pixel Normalization with tf.reshape() and tf.keras.layers.LayerNormalization(); Reduce VAB blocks from 5 to 4</td>
<td>REDS</td>
<td>27.550034</td>
<td>0.7687168</td>
<td>64,108</td>
<td>error</td>
<td></td>
</tr>
<tr>
<td>VapSR_4</td>
<td>apply pruning, weights clustering to conv kernels</td>
<td>REDS</td>
<td>27.833515(suspect)</td>
<td>0.7771123(suspect)</td>
<td>32,054(64,108)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VapSR_4_2_0</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: GELU</td>
<td>REDS</td>
<td>27.666351</td>
<td>0.77187574</td>
<td>64,108</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VapSR_4_2_1</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: RELU</td>
<td>REDS</td>
<td>27.539206</td>
<td>0.7669671</td>
<td>64,108</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VapSR_4_3</td>
<td>Functional VapSR_4 with self customed pixel normalization get rid of layer normalization</td>
<td>REDS</td>
<td>27.651005</td>
<td>0.7715401</td>
<td>63,852</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em></p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: FP16  </li>
<li>Acceleration: TFLite GPU Delegate</li>
</ul>
<hr>
<h4 id="milestone-1"><a href="#milestone-1" class="headerlink" title="milestone_1:"></a><strong>milestone_1:</strong></h4><hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>VapSR_4_1</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: RELU, Attention using Partial conv</td>
<td>REDS</td>
<td>27.790268</td>
<td>0.77721727</td>
<td>59,468</td>
<td>654.0 (INT8_CPU)</td>
<td>7.462</td>
</tr>
<tr>
<td>SWAT_0</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;4)</td>
<td>REDS</td>
<td>27.842232</td>
<td>0.77754354</td>
<td>50,624</td>
<td>271.0   (FP16_CPU)</td>
<td>5.803</td>
</tr>
<tr>
<td>SWAT_1</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;2), replace fc with 1*1 conv</td>
<td>REDS</td>
<td>27.759375</td>
<td>0.77492595</td>
<td>33,984</td>
<td>252.0 (FP16_CPU)</td>
<td>3.900</td>
</tr>
<tr>
<td>SWAT_2</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv</td>
<td>REDS</td>
<td>27.760305</td>
<td>0.77487457</td>
<td>25,664</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SWAT_3</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.761642</td>
<td>0.7748446</td>
<td>25,664</td>
<td>27.8 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_3_1</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17), Partial Conv(point_wise: standard, depth_wise: groups&#x3D;out_dim), Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.754656</td>
<td>0.77461684</td>
<td>24,160</td>
<td>30.0 (FP16_TFLite GPU Delegate)</td>
<td>2.776</td>
</tr>
<tr>
<td>SWAT_3_2</td>
<td>Sliding Window, VAB Attention(receptive field&#x3D;17), Partial Conv(feature fusion maintains standard conv1*1), Channel Shuffle(mix_ratio&#x3D;2), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.74189</td>
<td>0.7742521</td>
<td>26,016</td>
<td>32.4 (FP16_TFLite GPU Delegate)</td>
<td>2.996</td>
</tr>
<tr>
<td>SWAT_4</td>
<td>Sliding Window, VAB Attention, Replace partial conv with standard convlution, Remove Channel Shuffle, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.785185</td>
<td>0.77523285</td>
<td>53,696</td>
<td>38.5 (FP16_TFLite GPU Delegate)</td>
<td>6.202</td>
</tr>
<tr>
<td>SWAT_5</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 250,000</td>
<td>REDS</td>
<td>27.811176</td>
<td>0.7763541</td>
<td>25,664</td>
<td>27.6 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_6</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 150,000, Remove convs of hidden forward&#x2F;backward</td>
<td>REDS</td>
<td>27.738842</td>
<td>0.7743317</td>
<td>21,056</td>
<td>23.6 (FP16_TFLite GPU Delegate)</td>
<td>2.417</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em></p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/02/13/OnePlus_Recovery/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/13/OnePlus_Recovery/" class="post-title-link" itemprop="url">OnePlus Recovery</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-13 12:01:08" itemprop="dateCreated datePublished" datetime="2023-02-13T12:01:08+08:00">2023-02-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 12:28:18" itemprop="dateModified" datetime="2024-12-02T12:28:18+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h2><ol>
<li>备份数据：相册 安装应用名单</li>
<li>root 解锁</li>
<li>ADB 刷入TWRP Recovery镜像</li>
<li>刷入ROM（3种方法）<ol>
<li>电脑的磁盘列表中找到手机，复制ROM至手机的内部存储，复制完成后在recovery主菜单中，点击Install，点击ROM包，滑动后进行刷入。</li>
<li>用命令将ROM推送至手机（filename.zip为ROM名称，可拖动ROM文件至命令窗口自动填入完整文件地址，或输入文件名前几个字母后按Tab键来自动补全文件名）<ul>
<li>检测设备是否连接: adb devices</li>
<li>推送ROM: adb push ROM_name.zip &#x2F;sdcard&#x2F;</li>
<li>推送完成后在recovery主菜单中点击Install，点击ROM包，滑动进行刷入。</li>
</ul>
</li>
<li>在recovery主菜单中，点击“高级”，点击ADB sideload，滑动底部按钮，在PowerShell窗口用命令：.\adb sideload ROM_name.zip</li>
</ol>
</li>
<li>刷入Magisk<ol>
<li>目的:<ul>
<li>未获得 Google「认证」的设备无法从 Play 应用商店下载安装 Netflix，Google Pay、Pokémon Go 等应用不能在已 root 的设备上正常运行，改动过系统文件的 ROM 无法通过 OEM 渠道进行正常的 OTA 更新升级……</li>
<li>Magisk 的实现方式就像是一种魔法，当被挂载的 Magisk 分区被隐藏甚至被取消挂载时，原有系统分区的完整性丝毫未损，玩需要 root 验证的游戏、运行对设备认证状态有要求的应用甚至进行需要验证系统完整性的 OTA 更新都没有任何问题。</li>
</ul>
</li>
<li>方法:<ol>
<li>在刷入前，我们先安装 Magisk App 来检查设备的信息，来确定进一步的操作。我们先到官方项目地址：<a target="_blank" rel="noopener" href="https://github.com/topjohnwu/Magisk/releases">https://github.com/topjohnwu/Magisk/releases</a> 下载 apk 文件安装。<ul>
<li>Tip:从 Magisk 22 开始，不再区分刷写用的 .zip 包与安装管理器用到的 .apk 应用安装包，二者合一且只有后缀的区别，默认提供 .apk 包，更改后缀为 .zip 后即可被刷写。</li>
</ul>
</li>
<li>打开安装后的 Magisk App，像上面的最后一张截图一样，你能看到一项名为 Ramdisk 的值。请确保此项的值为「是」「True」，我们再进行下一步</li>
</ol>
</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jarvis</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
