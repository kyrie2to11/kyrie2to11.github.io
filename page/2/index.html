<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"kyrie2to11.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="记录学习和生活，留下时光的痕迹">
<meta property="og:type" content="website">
<meta property="og:title" content="Jarvis&#39;s Blog">
<meta property="og:url" content="https://kyrie2to11.github.io/page/2/index.html">
<meta property="og:site_name" content="Jarvis&#39;s Blog">
<meta property="og:description" content="记录学习和生活，留下时光的痕迹">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="jarvis">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://kyrie2to11.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Jarvis's Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Jarvis's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记录生活，沉淀自己</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">jarvis</p>
  <div class="site-description" itemprop="description">记录学习和生活，留下时光的痕迹</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/02/17/Workshop-Log-0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/17/Workshop-Log-0/" class="post-title-link" itemprop="url">MAI 2023 Mobile VSR Workshop Log</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-17 10:29:03" itemprop="dateCreated datePublished" datetime="2023-02-17T10:29:03+08:00">2023-02-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-16 11:20:24" itemprop="dateModified" datetime="2024-12-16T11:20:24+08:00">2024-12-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Workshop-and-Challenges-CVPR-2023"><a href="#Workshop-and-Challenges-CVPR-2023" class="headerlink" title="Workshop and Challenges @ CVPR 2023"></a>Workshop and Challenges @ CVPR 2023</h2><ol>
<li>Efficient Super-Resolution Challenge(ESR)<ul>
<li>经典baseline:<ul>
<li>information multi-distillation block,IMDN (2019)</li>
<li>Residual feature distillation block,RFDN (2020)</li>
<li>Residual Local Feature Network,RLFN (ByteESR2022)</li>
</ul>
</li>
<li>初期调试跑起来时，目录名称有一点变化就会在别处导致意想不到的错误:(</li>
<li>很多队伍都用到了Quantization Aware Training (QAT)</li>
<li>2022参赛上榜的网络结构和权重都有提供</li>
<li>results</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val Time [ms]</th>
<th>Params [M]</th>
<th>FLOPs [G]</th>
<th>Acts [M]</th>
<th>Mem [M]</th>
<th>Conv</th>
</tr>
</thead>
<tbody><tr>
<td>trained_rfdn_best</td>
<td>DIV2K_val(801-900)</td>
<td>28.73</td>
<td>37.62</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_1</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>41.38</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_2</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>43.86</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_3</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>37.59</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_4</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>34.20</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>IMDN_baseline_1</td>
<td>DIV2K_val(801-900)</td>
<td>29.13</td>
<td>45.11</td>
<td>0.894</td>
<td>58.53</td>
<td>154.14</td>
<td>471.78</td>
<td>43</td>
</tr>
<tr>
<td>IMDN_baseline_2</td>
<td>DIV2K_val(801-900)</td>
<td>29.13</td>
<td>45.03</td>
<td>0.894</td>
<td>58.53</td>
<td>154.14</td>
<td>471.78</td>
<td>43</td>
</tr>
<tr>
<td>IMDN_baseline_3</td>
<td>DIV2K_val(801-900)</td>
<td>29.13</td>
<td>44.44</td>
<td>0.894</td>
<td>58.53</td>
<td>154.14</td>
<td>471.78</td>
<td>43</td>
</tr>
</tbody></table>
<hr>
</li>
<li>Mobile AI workshop 2023<ul>
<li>测试可以用自己手机，也可使用提供的远程设备(速度慢有延迟)</li>
<li>2022 tracks</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Track</th>
<th>Sponsor</th>
<th>Evaluate_Platform</th>
<th>Final_Phase_Team&#x2F;Participants</th>
</tr>
</thead>
<tbody><tr>
<td>Bokeh Effect Rendering 背景虚化</td>
<td>Huawei</td>
<td>Kirin 9000’s Mali GPU</td>
<td>6&#x2F;90</td>
</tr>
<tr>
<td>Depth Estimation</td>
<td>Raspberry Pi</td>
<td>Raspberry Pi 4</td>
<td>7&#x2F;70</td>
</tr>
<tr>
<td>Learned Smartphone ISP</td>
<td>OPPO</td>
<td>Snapdragon’s 8 Gen 1</td>
<td>11&#x2F;140</td>
</tr>
<tr>
<td>Image Super-Resolution</td>
<td>Synaptics</td>
<td>Synaptics VS680</td>
<td>28&#x2F;250</td>
</tr>
<tr>
<td>Video Super-Resolution</td>
<td>MediaTek 联发科</td>
<td>MediaTek Dimensity 9000</td>
<td>11&#x2F;160</td>
</tr>
</tbody></table>
<hr>
<ul>
<li>2021 tracks</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Track</th>
<th>Sponsor</th>
</tr>
</thead>
<tbody><tr>
<td>Learned Smartphone ISP</td>
<td>MediaTek 联发科</td>
</tr>
<tr>
<td>Image Denoising</td>
<td>Samsung</td>
</tr>
<tr>
<td>Image Super-Resolution</td>
<td>Synaptics</td>
</tr>
<tr>
<td>Video Super-Resolution</td>
<td>OPPO</td>
</tr>
<tr>
<td>Depth Estimation</td>
<td>Raspberry Pi</td>
</tr>
<tr>
<td>Camera Scene Detection</td>
<td>Computer Vision Lab, ETH Zurich, Switzerland</td>
</tr>
</tbody></table>
<hr>
<ol>
<li>计划参加track:Image Super-Resolution 3月份开始 -&gt; 调整为track:Video Super-Resolution</li>
<li>Train 2021 anchor-based plain net (ABPN) 两次<ul>
<li>200 epoch 时报错停掉一次</li>
<li>600 epoch 完整跑完，但loss上下波动不收敛</li>
</ul>
</li>
<li>andriod对原作提供的TF-lite模型进行了测试,测试流程掌握了</li>
</ol>
</li>
</ol>
<h3 id="MobileAI-worshop-Video-Super-Resolution"><a href="#MobileAI-worshop-Video-Super-Resolution" class="headerlink" title="MobileAI worshop: Video Super-Resolution"></a>MobileAI worshop: Video Super-Resolution</h3><ol>
<li><p>papers</p>
<ol>
<li>Ntire 2019 challenge on video super-resolution: Methods and results</li>
<li>Ntire 2020 challenge on image and video deblurring</li>
<li>Pynet-v2 mobile: Efficient on-device photo processing with neural networks<ul>
<li>Image Signal Process(ISP): 手机成像流程 光-&gt;CMOS传感器-&gt;成像引擎ISP-&gt;AI(GPU)-&gt;图片；镜头和CMOS在将光学信号转化为由0、1、0、1组成的数字信号时可能存在细节上的遗漏和错误，而ISP单元的主要任务就是进行“纠错”、“校验”和“补偿”。</li>
<li>pynet模型便于移动端部署的mobile版本目的是end-to-end learned ISP,时间很近:2022 26th International Conference on Pattern Recognition (ICPR). IEEE, 2022.</li>
<li>CNN based</li>
</ul>
</li>
<li>Microisp: Processing 32mp photos on mobile devices with deep learning. In: European Conference on Computer Visio(2022)</li>
<li>Real-Time Video Super-Resolution on Smartphones with Deep Learning,Mobile AI 2021 Challenge: Report<ul>
<li>Results and Discussion<ul>
<li>Team Diggers 冠军方案基于Keras&#x2F;Tensorflow 电子科技大学 唯一一个使用循环连接（recurrent connections）来利用帧间依赖性获取更好重建结果，其他方案都是基于单帧超分的。</li>
</ul>
</li>
</ul>
</li>
<li>Power Efficient Video Super-Resolution on Mobile NPUs with Deep Learning, Mobile AI &amp; AIM 2022 challenge: Report<ul>
<li>tutorial: <a target="_blank" rel="noopener" href="https://github.com/MediaTek-NeuroPilot/mai22-real-time-video-sr">https://github.com/MediaTek-NeuroPilot/mai22-real-time-video-sr</a>. baseline:MobileRNN</li>
<li>scoring: Final Score &#x3D; α · PSNR + β · (1 - power consumption) α &#x3D; 1.66 and β &#x3D; 50，注重PSNR和power consumption两个指标</li>
<li>Discussion:<ul>
<li>The majority of models followed a simple single-frame restoration approach to improve the runtime and power efficiency. 大部分模型技术路线是降低单帧超分的运行时间和能量消耗，网络模型都比较浅</li>
<li>GenMedia Group(一家韩国公司) 基于上年度单帧超分冠军方案ABPN小改进而来，排名第6但psnr:28.40最好,是唯二psnr超过28的方案之一，另一个是221B团队基于RNN的方法</li>
<li>基于RNN的方案推理速度较慢且能耗高</li>
<li>总结：2022年来看设备上的视频超分CNN是适合的，因为CNN取得了runtime energy_consumption restoration_quality 的平衡</li>
</ul>
</li>
</ul>
</li>
<li>Sliding Window Recurrent Network for Efficient Video Super-Resolution<ul>
<li>SWRN makes use of the information from neighboring frames to reconstruct the HR frame. 从相邻帧提取信息来重建高清帧,相比单帧超分的方法有丰富的细节。</li>
<li>An bidirectional hidden state is used to recurrently collect temporal spatial relations over all frames.使用双向隐藏状态来循环收集所有帧的时间空间关系。</li>
<li>Pioneer network: SRCNN</li>
<li>Video super-resolution: the most important parts are frame alignment<ul>
<li>VESPCN and TOFlow: optical flow to align frames</li>
<li>TDAN and EDVR: deformable convolution. Especially, EDVR enjoys the merits of implicit alignment and its PCD module.</li>
<li>Incorporates recurrent networks, use the hidden state to record the important temporal information.</li>
</ul>
</li>
<li>在测试平台Runtime 10.1 ms、 0.80 W@30FPS,最后分数低问题就在这里，PSNR SSIM 比第一名MVideoSR（小米）都要好 -&gt; 寻找加速计算和减小耗能的方法</li>
</ul>
</li>
<li>Lightweight Video Super-Resolution for Compressed Video -&gt; Compression-informed Lightweight VSR (CILVSR)<ul>
<li>Recurrent Frame-based VSR Network (FRVSR, RBPN, RRN)</li>
<li>Spatio-Temporal VSR Network (SOF-VSR, STVSR, TDAN, TOFlow, TDVSR-L)</li>
<li>Generative Adversarial Network (GAN)-based SR Network</li>
<li>Video Compression-informed VSR Network (FAST, COMISR, CDVSR, CIAF)</li>
</ul>
</li>
<li>RCBSR: Re-parameterization Convolution Block for Super-Resolution<ul>
<li>ECBSR baseline</li>
<li>Multiple paths <strong>ECB re-parametrization</strong></li>
<li>FGNAS</li>
</ul>
</li>
<li>Deformable 3D Convolution for Video Super-Resolution<ul>
<li>deformable 3D convolution</li>
</ul>
</li>
<li>Efficient Image Super-Resolution Using Vast-Receptive-Field Attention(VapSR 有torch代码)<ul>
<li>improving the attention mechanism<ul>
<li>large kernel convolutions</li>
<li>depth-wise separable convolutions</li>
<li>pixel normalization -&gt; train steadily</li>
</ul>
</li>
<li>相比bytedance的RLFN -&gt; 性能sota,参数更少</li>
</ul>
</li>
<li>LiDeR: Lightweight Dense Residual Network for Video Super-Resolution on Mobile Devices(无代码)<ul>
<li>针对手机端，结构简单，REDS 320x180 X4 upscaling -&gt; psnr:27.51 ssim:0.769(有疑问这个结果到底是在手机上测出来的还是在手机上?)</li>
<li><strong>REDS 320x180 X4 upscaling</strong> 执行速度快 139FPS -&gt; FSRCNN: 45FPS ESPCN: 52FPS</li>
<li>测试平台：<strong>Tensorflow-lite</strong> <strong>fp16</strong> <strong>TF-Lite GPU delegate</strong> <strong>Xiaomi Mi 11</strong> <strong>Qualcomm Snapdragon 888 SoC, Qualcomm Adreno 660 GPU, and 8 GB RAM</strong></li>
</ul>
</li>
<li>Fast Online Video Super-Resolution with Deformable Attention Pyramid<ul>
<li><p>recurrent VSR architecture based on a deformable attention pyramid (DAP)</p>
</li>
<li><p>对比RRN(mobile_rrn MAI VSR官方用例很慢) -&gt;不适合用到MAI VSR中</p>
<ul>
<li><table>
<thead>
<tr>
<th>Run[ms]</th>
<th>fps[1&#x2F;s]</th>
<th>FLOPs[G]</th>
<th>MACs[G]</th>
</tr>
</thead>
<tbody><tr>
<td><strong>28</strong></td>
<td>35.7</td>
<td>387.5</td>
<td>193.6</td>
</tr>
<tr>
<td><strong>38</strong></td>
<td>26.3</td>
<td>330.0</td>
<td>164.8</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><p>2022 challenge methods (ranked)</p>
<ol>
<li><p>MVideoSR(无代码)</p>
<ul>
<li>paper title: ELSR: Extreme Low-Power Super Resolution Network For Mobile Devices</li>
<li>affiliation: Video Algorithm Group, Camera Department, Xiaomi Inc., China</li>
<li>methods:<ol>
<li><strong>core idea</strong>: mobile friendly network which consumes as little energy as possible, discard some complex operations such as optical flow, multi-frame feature alignment, and <strong>start from single frame baselines</strong>.</li>
<li><strong>multi-branch distillation structure</strong> show significant increase in energy consumption while  a slight increase in PSNR compared with the plain convolutional network of similar parameters. abandon multi-branch network architectures, and focus on plain convolutional SR networks.</li>
<li>though <strong>attention modules(ESA, CCA and PA)</strong> bring performance improvement, the extra energy consumption introduced is still unacceptable</li>
<li>architeture<ul>
<li>discription: single frame input  which only have 6 layers, of which only 5 have learnable parameters, including 4 Conv layers and a PReLU activation layer. Pixel-Shuffle operation (also known as depth2space) is used at last to upscale the size of output without introducing more calculation. The intermediate feature channels are all set to 6.</li>
<li><img src="/Workshop-Log_-0/MVideoSR_architecture.png" alt="fig_1"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>ZX VIP(无代码)</p>
<ul>
<li>paper title: RCBSR: Re-parameterization Convolution Block for Super-Resolution</li>
<li>affiliation: Audio &amp; Video Technology Platform Department, ZTE Corp., China</li>
<li>methods:<ol>
<li><strong>core idea</strong>: trade-off between SR quality and the energy consumption, <strong>ECBSR as baseline</strong>. In consideration of the low power consumption optimize the baseline from three aspects,<strong>network architecture, NAS and training strategy</strong>.</li>
<li><strong>network architecture</strong>:<strong>re-parameterization</strong> technique in the deploy stage, <strong>replace the activate function PReLU with ReLU</strong>.the power consumption of tflite model with ReLU is less than PReLU. Meanwhile there is no apparent discrepancy in PSNR.Finally, in order to further reduce power consumption, the <strong>output of first CNN layer</strong> is added into the backbone output instead of original input because original input needs to be copied the number of channels. We use <strong>sub-pixel convolution</strong> to upsample image in the network.</li>
<li><strong>NAS</strong>: The objective function of FGNAS is <strong>task-specific loss</strong> and <strong>regularizer penalty FLOPs</strong>. FGNAS -&gt; Kim, H., Hong, S., Han, B., Myeong, H., Lee, K.M.: Fine-grained neural architecture search. arXiv preprint arXiv:1911.07478 (2019)</li>
<li><strong>training strategy</strong>:<strong>replace L1 loss function with Charbonnier loss function</strong> because it causes the problem that the restored image is too smooth and lack of sense of reality.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop-Log-0/ZX_VIP_architecture.png" class="" title="fig2"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>Fighter(无代码)</p>
<ul>
<li>title: Fast Real-Time Video Super-Resolution</li>
<li>affiliation: None, China</li>
<li>methods:  <ol>
<li>shallow CNN model with <strong>depthwise separable convolutions</strong> and <strong>one residual connection</strong>. The number of convolution channels in the model was set to 8, the <strong>depth-to-space op</strong> was used at the end of the model to produce the final output.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop-Log-0/Fighter_architecture.png" class="" title="fig3"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>XJTU-MIGU SUPER(无代码)</p>
<ul>
<li>title: Light and Fast On-Mobile VSR</li>
<li>affiliation: School of Computer Science and Technology, Xi’an Jiaotong University, China MIGU Video Co. Ltd, China</li>
<li>methods:<ol>
<li>small CNN-based model. 示意图如下，总共训练了2600 epochs :(</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop-Log-0/XJTU-MIGU_SUPER_architecture.png" class="" title="fig4"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>BOE-IOT-AIBD(无代码)</p>
<ul>
<li>title: Lightweight Quantization CNN-Net for Mobile Video Super-Resolution</li>
<li>affiliation: BOE Technology Group Co., Ltd., China</li>
<li>methods:<ol>
<li>based on the <strong>CNN-Net architecture</strong>, its structure is illustrated in Fig 6. The authors applied <strong>model distillation</strong>, and used the <strong>RFDN CNN</strong> as a teacher model.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop-Log-0/BOE-IOT-AIBD_architecture.png" class="" title="fig5"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>GenMedia Group(无代码)</p>
<ul>
<li>title: SkipSkip Video Super-Resolution</li>
<li>affiliation: GenGenAI, South Korea</li>
<li>methods:<ol>
<li>inspired by the last year’s top solution from the MAI image super-resolution challenge.  <strong>added one extra skip connection to the mentioned anchor-based plain net (ABPN) model</strong>.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop-Log-0/GenMedia-Group_architecture.png" class="" title="fig6"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>NCUT VGroup(无代码)</p>
<ul>
<li>title: EESRNet: A Network for Energy Efficient Super Resolution</li>
<li>affiliation: North China University of Technology, China Institute of Automation, Chinese Academy of Sciences, China</li>
<li>methods:<ol>
<li>also based their solution on the <strong>ABPN model</strong>.</li>
<li>architeture<ul>
<li><img src="/2023/02/17/Workshop-Log-0/NCUT-VGroup_architecture.png" class="" title="fig7"></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
<li><p>ideas</p>
<ol>
<li><p>尝试BasicVSR++的轻量化</p>
</li>
<li><p>在ABPN的基础上加入BasicVSR++的主要idea进行改进</p>
</li>
<li><p>尝试将Pynet_v2应用于video super_resolution -&gt; relative complicated and tailored for ISP, so halt  </p>
</li>
<li><p>先train MRNN baseline</p>
<ul>
<li><p>环境</p>
<ol>
<li><p>Python&#x3D;&#x3D;3.8.10</p>
</li>
<li><p>Tensorflow-gpu&#x3D;&#x3D;2.9.0</p>
<ul>
<li>查看tensorflow cuda cudnn python 版本对照表： <a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source_windows">https://www.tensorflow.org/install/source_windows</a></li>
</ul>
</li>
<li><p>Cuda&#x3D;&#x3D;11.2</p>
<ul>
<li>CUDA: CUDA是一个<strong>计算平台和编程模型</strong>，用于在GPU上加速应用程序。CUDA版本指的是CUDA软件的版本</li>
<li>CUDA Toolkit: CUDA Toolkit是包含<strong>CUDA库</strong>和<strong>CUDA工具链</strong>的软件包，用于开发和编译CUDA应用程序。<ul>
<li>CUDA库: CUDA 库包含了 CUDA 编程所需的核心库文件，例如 <strong>CUDA Runtime 库、CUDA Driver 库、cuBLAS 库、cuDNN 库</strong>等。这些库文件提供了 GPU 加速的基本功能和算法，是 CUDA 编程的基础。</li>
<li>CUDA工具链：CUDA 工具链则包含了一系列辅助开发和调试 CUDA 程序的工具，例如 <strong>nvcc 编译器、CUDA-GDB 调试器、Visual Profiler 性能分析工具</strong>等。这些工具能够帮助开发者更方便地编写、调试和优化 CUDA 程序。</li>
</ul>
</li>
<li>note: 查看当前安装的显卡驱动最高支持的CUDA版本 nvidia-smi</li>
<li>note: 查看CUDA工具链版本 nvcc –version</li>
<li>CUDA Toolkit 与 Driver Version 对照：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></li>
<li></li>
</ul>
</li>
<li><p>Cudnn&#x3D;&#x3D;v8.7.0</p>
<ul>
<li>官网：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></li>
<li>cat &#x2F;etc&#x2F;os-release 查看linux版本</li>
<li>uname -m 查看cpu架构，cudnn有不同架构的版本 x86_64 PPC SBSA</li>
<li>tar -xvf解压缩后用以下命令安装并赋予所有用户读取权限</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">sudo cp path_to_cudnn/include/cudnn*    /usr/local/cuda-11.2/include</span><br><span class="line">sudo cp path_to_cudnn/lib/libcudnn*    /usr/local/cuda-11.2/lib64</span><br><span class="line">sudo chmod a+r /usr/local/cuda-11.2/include/cudnn*   /usr/local/cuda-11.2/lib64/libcudnn*</span><br></pre></td></tr></table></figure>

<ul>
<li>Cudnn和Cuda 安装完需在&#x2F;etc&#x2F;profile配置环境变量PATH和LD_LIBRARY_PATH</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64</span><br><span class="line">export PATH=$PATH:/usr/local/cuda/bin</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure>

<ul>
<li>可将文件夹 &#x2F;usr&#x2F;local&#x2F;cuda-11.2 与 &#x2F;usr&#x2F;local&#x2F;cuda 软连接起来</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/cuda-11.2 /usr/local/cuda</span><br></pre></td></tr></table></figure>

<ul>
<li><p>也可以通过linux下的<code>update-alternatives</code>命令行工具来进行cuda版本的管理,先用<code>sudo update-alternatives --install /usr/local/cuda(替代项名称) cuda(替代项链表名称) /usr/local/cuda-xx(实际路径) x(优先级)</code>来安装配置cuda的多个替代项,<code>sudo update-alternatives --config cuda</code>切换CUDA默认版本,其本质是更改了以下软连接: <code>/usr/local/cuda -&gt; /etc/alternatives/cuda -&gt; /usr/local/cuda-xx.x</code></p>
</li>
<li><p>用下面的命令查看cudnn版本,新版本查看cuDNN版本的命令为</p>
</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2  # -A 选项用来指定匹配成功的行之后显示2行内容</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>结果</p>
<ol>
<li>用默认config.yml训练太慢了大约需要1周时间，中途停掉了</li>
<li>用改进后config.yml训练。8小时左右训练完成，但是loss很大</li>
<li>结合往年此赛道总结文章放弃训练提供的mobilernn baseline 思考其它基于cnn的模型</li>
</ol>
</li>
</ul>
</li>
<li><p>可以从NTIRE 2022 efficient super-resolution challenge选取baseline运用剪枝蒸馏等改进到移动端</p>
<ul>
<li>course project for NCSU’s Computer Science 791-025: Real-Time AI &amp; High-Performance Machine Learning. 三板斧<ol>
<li>Pruning via NNI</li>
<li>Quantization via NNI</li>
<li>Hyper Parameter Optimization via NNI</li>
<li>Color Optimization: RGB -&gt; YCbCr</li>
</ol>
</li>
<li>选取2022 NTIRE ESR冠军方案RLFN(Byte Dance)作为baseline,先将其模型转换为 tensorflow 版本在 REDS 数据集上直接进行VSR的测试 -&gt; 中间软件依赖兼容性问题放弃RLFN torch-&gt;onnx-&gt;tensorflow路线</li>
<li>直接用tensorflow 重构 RLFN -&gt; train成功但是精度不达标’psnr’: 25.574987, ‘ssim’: 0.69084775，需要调试改进</li>
<li>现在的首要问题是确定自己的tensorflow 版本RLFN 与原作的 torch 版本RLFN 是否一致 -&gt; cease</li>
<li>可以先将其他模型利用torch_to_tensorflow 转化为tensorflow版本模型，并可视化查看效果 -&gt; 可行而且看源代码不复杂，难点在torch onnx onnx-tf tensorflow-gpu 版本对照，静等比赛开始官方scripts</li>
<li>现在当务之急不是版本对照问题需要尽快找到往年的baseline跑起来，改起来 -&gt; 跑此项目了解剪枝 量化 超参调整三板斧实际运用 ：<a target="_blank" rel="noopener" href="https://github.com/briancpark/video-super-resolution.git">https://github.com/briancpark/video-super-resolution.git</a> -&gt; 都是在调库 NNI</li>
<li>Train baseline SWRN：<a target="_blank" rel="noopener" href="https://github.com/shermanlian/swrn">https://github.com/shermanlian/swrn</a><ul>
<li>结构重参数化（structural re-parameterization）:用一个结构的一组参数转换为另一组参数，并用转换得到的参数来参数化（parameterize）另一个结构。只要参数的转换是等价的，这两个结构的替换就是等价的。</li>
<li>先测试提供的ckpt-98 -&gt; 测试结果’psnr’: 27.931335, ‘ssim’: 0.7803563</li>
<li>缩减recon_trunk_forward &#x2F; recon_trunk_backward &#x2F; recon_trunk 的 block_num到2, train from scratch 看结果</li>
</ul>
</li>
<li>按照去年赛道冠军方案MVedioSR的ELSR搭建pipeline<ul>
<li>L1 loss(Mean Absolute Error, MAE) -&gt; 样本预测值与标签之间差的绝对值取平均, 对异常值不敏感,鲁棒性更强; 对于接近零的数, 梯度为常数, 没有逐渐变小的趋势, 容易出现震荡现象</li>
<li>L2 loss(Mean Squared Error, MSE) -&gt; 样本预测值与标签之间平方差取平均, 对异常值敏感,鲁棒性不强; 对于接近零的数, 梯度随着误差的减小而逐渐减小, 避免了震荡现象。</li>
<li>TensorFlow中的内置损失函数非常丰富，包括L1、L2、L1_Charbonnier和MSE等常见的损失函数。这些损失函数都在tf.keras.losses模块中实现。具体来说，可以使用以下函数调用这些损失函数：<ul>
<li>L1损失函数：tf.keras.losses.mean_absolute_error(y_true, y_pred)</li>
<li>L2损失函数：tf.keras.losses.mean_squared_error(y_true, y_pred)</li>
<li>L1_Charbonnier损失函数：可以自定义实现，也可以使用以下库中的实现：TensorFlow Addons（需要单独安装）。</li>
<li>M2损失函数：tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)</li>
<li>note: 这些函数的参数都是y_true和y_pred，分别表示真实值和预测值。</li>
</ul>
</li>
<li>L1 Loss: L1 Loss: $L_1 &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \left| y_i - \hat{y_i} \right|$</li>
<li>L2 Loss (MSE): $L_2 &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \left( y_i - \hat{y_i} \right)^2$</li>
<li>L1 Charbonnier Loss: $L_{Charbonnier} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \sqrt{ \left( y_i - \hat{y_i} \right)^2 + \epsilon^2 }$</li>
<li>M2 Loss: $L_{M2} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} \left( \frac{y_i - \hat{y_i}}{y_i + \epsilon} \right)^2$，其中 $\epsilon$ 为一个较小的数，如 $10^{-6}$，用于防止分母为零。</li>
</ul>
</li>
</ul>
</li>
<li><p>可以看看tflite加速的那些operations去更改模型</p>
</li>
<li><p>尝试将torch VapSR 从单图像超分向视频超分迁移</p>
<ul>
<li>构建模型的tensorflow 代码遇到个小坑: tf.keras.Sequential([upconv1,pixel_shuffle,lrelu,upconv2,pixel_shuffle]) 如果用两个一样的pixel_shuffle模块，用tf.keras.Sequential实现的时候必须用两个不一样的名称，否则无论如何Sequential内都只有一个pixel_shuffle模块</li>
<li>‘from .XXX import YYY’ 相对导入，Python 解释器会先从当前目录开始查找指定的模块或包,需要当前current.py文件在一个Python包内（创建一个空的 <strong>init</strong>.py 文件，即可将文件夹视为一个Python包）</li>
<li>[B,H,W,48] - conv1X1升维 -&gt; [B,H,W,64], conv1X1为[64,48,1,1]大小的Tensor</li>
<li>blueprint conv(M: input_channels N: out_channels); BSconvU: 先用M X 1权重向量对输入作通道聚合, 变为只有1个feature map,然后再用N个K X K的卷积输出N个feature map.<ul>
<li><img src="/2023/02/17/Workshop-Log-0/BSconv.png" class="" title="fig8"></li>
</ul>
</li>
</ul>
</li>
<li><p>对VapSR_2 剪枝量化</p>
<ul>
<li><p>列表推导式：list &#x3D; [expression for item in iterable]，其中 expression 是要添加到列表中的表达式，item 是可迭代对象中的每一项，iterable 是要迭代的对象。例如：metric_list &#x3D; [func for name, func in self.metric_functions.items()]</p>
</li>
<li><p>tfmot.sparsity.keras.prune_low_magnitude() 封装vapsr_3中的每一个tf.keras.layers.Conv2D进行剪枝</p>
</li>
<li><p>同一class下def的method默认第一个参数需要为self;一个method调用另一个method需要用 self.def()不能直接用 def()</p>
</li>
<li><p>keras建立网络的方法可以分为keras.models.Sequential() 和keras.models.Model()、继承类三种方式。注意：tensorflow2.* 以后的版本可以直接使用tf.keras.Sequential()和tf.keras.Model()两个类。不用再使用keras.models的API</p>
<ul>
<li>Keras提供两种API：Sequential API和Functional API。Sequential API是一种简单的线性堆叠模型，适用于许多简单的模型。但是，如果我们需要构建更加复杂的模型，比如有多个输入或输出的模型，那么就需要使用Functional API。</li>
<li>Functional API通过tf.keras.Model()实现，它提供了更加灵活的方式来定义模型的结构和层之间的连接。使用Functional API，我们可以创建具有多个输入和输出的模型，可以共享层，可以定义任意的计算图结构等等。相比之下，Sequential API则不能支持这些更高级的模型定义方式。</li>
<li>因此，使用Functional API来构建复杂的模型是更加灵活和强大的选择，而通过tf.keras.Model()实现这个API是为了提供一种方便和一致的方式来定义和构建深度学习模型。</li>
</ul>
</li>
<li><p>&#x2F; 表示普通的除法运算，例如 5 &#x2F; 2 的结果为 2.5。它返回的是一个浮点数，即使两个操作数都是整数。  &#x2F;&#x2F;表示整除运算，例如5 &#x2F;&#x2F; 2 的结果为 2。</p>
</li>
<li><p>&#x3D;和+直接赋值给变量是不好的，因为它们只是简单地创建一个新的变量，而不是对现有变量进行原位操作。assign()和assign_add()是TensorFlow中的<strong>原地操作</strong>，它们直接将结果分配给现有变量，而不是创建一个新的变量。</p>
</li>
<li><p>shell scripts(.sh)添加多行注释：<code>&lt;&lt; COMMENT ... COMMENT</code>, 在 Shell 中，&lt;&lt; 是 Here Document（文档嵌入）的语法，它可以用来将一段文本或代码块嵌入到 Shell 脚本中。</p>
</li>
<li><p>pruning 过程model type 变化</p>
<ol>
<li>initial: type(self.model) &#x3D;&#x3D; &lt;class ‘VapSR_3.vapsr_3’&gt; (i.e. Keras Subclass Model)<ul>
<li>Keras Subclass Model是一种创建自定义模型的方式，相较于Sequential和Functional API而言，其提供更大的灵活性。使用Subclass Model，用户可以通过定义一个继承自tf.keras.Model的Python类来构建模型。使用Subclass Model的优点在于，它可以自由灵活地创建非线性、复杂的模型结构，也可以方便地重复利用模型代码。</li>
</ul>
</li>
<li>apply tensorflow.keras.Model() method -&gt; type(functional_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;</li>
<li>add tfmot.sparsity.keras.prune_low_magnitude() wrapper -&gt; type(pruned_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;; 如果直接调用tfmot.sparsity.keras.prune_low_magnitude(functional_model, **pruning_params)还是会报错：ValueError: Subclassed models are not supported currently. :(</li>
<li>add tfmot.sparsity.keras.prune_low_magnitude() wrapper with another method -&gt; type(pruned_model_1) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;</li>
<li>pruning -&gt; type(pruned_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;</li>
<li>虽然type(pruned_model) &#x3D;&#x3D; &lt;class ‘keras.engine.functional.Functional’&gt;，但是传入stripped_pruned_model &#x3D; tfmot.sparsity.keras.strip_pruning(pruned_model)就会报错：ValueError: Expected <code>model</code> argument to be a functional <code>Model</code> instance, but got a subclassed model instead: &lt;keras.saving.saved_model.load.BSConvU object at 0x7f66f06fa520&gt;</li>
<li>pruned_model.layers &#x3D;&#x3D; [&lt;keras.engine.input_layer.InputLayer object at 0x7f66f06fa580&gt;, &lt;keras.saving.saved_model.load.BSConvU object at 0x7f66f06fa520&gt;, &lt;keras.engine.sequential.Sequential object at 0x7f66f06fa4f0&gt;, &lt;keras.layers.convolutional.conv2d.Conv2D object at 0x7f66f0717c40&gt;, &lt;keras.layers.core.tf_op_layer.TFOpLambda object at 0x7f66f80a7fd0&gt;, &lt;keras.engine.sequential.Sequential object at 0x7f66f80a7cd0&gt;]</li>
</ol>
</li>
<li><p>pruning 去除tfmot.sparsity.keras.prune_low_magnitude() wrapper的报错就没停过 -&gt; 直接构建Functional Model VapSR</p>
</li>
<li><p>详细解释 Layer Norm &#x2F; Batch Norm &#x2F; Instance Norm &#x2F; Pixel Norm</p>
<ul>
<li><p>Batch Norm：对每个特征通道（C）进行归一化，使用整个批次（N）中的样本的均值和方差。在每个 batch 的 channel 维度上计算均值和方差。</p>
</li>
<li><p>Layer Norm：对每个样本（N）进行归一化，使用所有特征通道（C）和空间维度（H，W）的均值和方差。在每个 layer 的所有 feature maps 上计算均值和方差。</p>
</li>
<li><p>Instance Norm：对每个样本（N）和每个特征通道（C）进行归一化，使用空间维度（H，W）的均值和方差。在每个 instance 的 channel 维度上计算均值和方差。</p>
</li>
<li><p>Pixel Norm：对每个样本（N）和每个像素位置（H，W）进行归一化，使用所有特征通道（C）的均值和方差。</p>
</li>
<li><img src="/2023/02/17/Workshop-Log-0/BN_LN_IN_PN.png" class="" title="Visualize different normalization methods"></li>
<li><p>组会可以讨论下具体实现（作为最后一小部分）</p>
</li>
<li><p>VapSR 原作者 pixel norm torch 实现  </p>
</li>
<li><pre><code class="python"> class VAB(nn.Module):
     def __init__(self, d_model,d_atten):
         super().__init__()
         self.proj_1 = nn.Conv2d(d_model, d_atten, 1)
         self.activation = nn.GELU()
         self.atten_branch = Attention(d_atten)
         self.proj_2 = nn.Conv2d(d_atten, d_model, 1)
         self.pixel_norm = nn.LayerNorm(d_model)
         default_init_weights([self.pixel_norm], 0.1)
     
     def forward(self, x):
         shorcut = x.clone()
         x = self.proj_1(x)
         x = self.activation(x)
         x = self.atten_branch(x)
         x = self.proj_2(x)
         x = x + shorcut
         x = x.permute(0, 2, 3, 1) #(B, H, W, C)
         x = self.pixel_norm(x)
         x = x.permute(0, 3, 1, 2).contiguous() #(B, C, H, W)

         return x
     参考：https://blog.csdn.net/weixin_39228381/article/details/107939602    
</code></pre>
</li>
<li><p>x &#x3D; tf.constant([[1.,2.,4.,5.,7.,8.],[6.,7.,9.,10.,11.,12.],[2.,3.,5.,6.,8.,9.],[4.,5.,7.,8.,10.,11]])</p>
</li>
<li><p>mean, variance &#x3D; tf.nn.moments(x, axes, shift&#x3D;None, keepdims&#x3D;False, name&#x3D;None) The mean and variance are calculated by <strong>aggregating the contents of x across axes</strong>.  例： tf.nn.moments(x,1)  x.shape &#x3D;&#x3D; [4,2,3] -&gt; mean.shape &#x3D;&#x3D; [4,1,3]</p>
</li>
<li><p>后续还需要花时间搞清楚 tf LayerNormalization GroupNormalization 在axis&#x3D;list&#x2F;tuple多轴的情况下，到底计算了多少mean和variance，换言之如何用这两个built-in layer做到随心所欲的控制normalization的粒度,妥协方法我觉得是利用transpose,转换轴(相当于torch permute)间接实现相关功能。</p>
</li>
</ul>
</li>
<li><p>*args 和 **kwargs 都是 Python 中用于传递可变数量参数的特殊语法。它们的主要区别在于：</p>
<ul>
<li>*args 用于传递可变数量的位置参数，以元组(tuple)的形式传递给函数；</li>
<li>**kwargs 用于传递可变数量的关键字参数，以字典(dictionary)的形式传递给函数。</li>
</ul>
</li>
<li><p>接下来需要尽快完成 pruning clustering quantization pipeline, 将runtime降到10ms左右</p>
</li>
<li><p>递归函数的return不是返回一个值然后程序结束，而是返回一个值到上一层的递归函数，直到return到最外层</p>
</li>
<li><p>add_pruning_wrapper():</p>
<ul>
<li>通过Sequential.add()重建模型,在原模型就是Sequential的时候可行,但是原模型call() method加不进去</li>
<li>原地替换setattr(object, name, new_model)难点:<ol>
<li>递归当前tf.keras.layers.Conv2D不知道所属模块object 和 name</li>
<li>pruned_model &#x3D; copy.deepcopy(model)在复制的pruned_model上应用剪枝封装, subclassed tf.keras.Model() class -&gt; custom object 需要全部重写method: get_config() from_config()</li>
</ol>
</li>
<li>model.__dict__ 与dir(model) 区别<ol>
<li>model.__dict__ 返回一个字典对象，其中键是模型实例的属性名称(可用model.__dict__.keys()访问)，值是对应的属性值(可用model.__dict__.values()访问)。而 dir(model) 返回一个列表对象，其中包含模型实例的所有属性名称。</li>
<li>具体来说，model.__dict__ 只返回<strong>实例自身定义的属性，不包括其继承而来的属性</strong>。而 dir(model) 返回实例的所有属性名称，包括其自身定义的属性和继承而来的属性。</li>
<li>model.__dict__ 返回的字典对象只包含可写的属性。而 dir(model) 返回的属性列表可能包含不可写的属性，例如只读属性或方法等。</li>
</ol>
</li>
<li>pruned_model.layers[3] &#x3D;&#x3D; &lt;keras.layers.convolutional.conv2d.Conv2D object at 0x7ff557c57a60&gt; 这一层是 Keras 自带的 Conv2D 层，而不是通过继承 tf.keras.layers.Layer 类来自定义的。因此，它不会在 __dict__ 属性中出现。</li>
</ul>
</li>
<li><p>strip_pruning_wrapper():</p>
<ul>
<li>tfmot.sparsity.keras.strip_pruning(): Only sequential and functional models are supported for now.  </li>
<li>recursively strip pruning wrapper -&gt; success</li>
</ul>
</li>
<li><p>lr_scheduler: ConsineDecayRestarts</p>
</li>
<li><p>pruning_train, clustering_train loss 与 pretraining train loss 相差很大, 50+ vs 10+ 有点问题</p>
</li>
<li><p>quantization</p>
<ol>
<li><p>tensorflow quantize:</p>
<ul>
<li>def quantize_scope(*args)</li>
<li>def quantize_model(to_quantize, quantized_layer_name_prefix&#x3D;’quant_’)</li>
<li>def quantize_annotate_model(to_annotate)</li>
<li>def _add_quant_wrapper(layer)</li>
<li>def quantize_annotate_layer(to_annotate, quantize_config&#x3D;None)</li>
<li>def quantize_apply(model, scheme&#x3D;default_8bit_quantize_scheme.Default8BitQuantizeScheme(),           quantized_layer_name_prefix&#x3D;’quant_’)</li>
<li>def _extract_original_model(model_to_unwrap)</li>
<li>def _quantize(layer)</li>
<li>def _unwrap_first_input_name(inbound_nodes)</li>
<li>def _wrap_fixed_range(quantize_config, num_bits, init_min, init_max, narrow_range)</li>
<li>def _is_serialized_node_data(nested)</li>
<li>def _nested_to_flatten_node_data_list(nested)</li>
<li>def fix_input_output_range(model, num_bits&#x3D;8, input_min&#x3D;0.0, input_max&#x3D;1.0, output_min&#x3D;0.0, output_max&#x3D;1.0, narrow_range&#x3D;False)</li>
<li>def _is_functional_model(model)</li>
<li>def remove_input_range(model)</li>
</ul>
</li>
<li><p>*与**二者区别,及与C++ 中指针的区别:</p>
<ul>
<li>* 和 ** 都是Python中的特殊符号，用于参数传递和元组、字典的解包操作。它们与C++中的指针有些类似，但也有不同之处。</li>
<li>* 用于元组的解包操作，可以将一个元组中的元素解包成一个一个的单独元素</li>
<li>** 用于字典的解包操作，可以将一个字典中的键值对解包成一个一个的单独键和值</li>
<li>在函数调用时，* 可以用于传递可变数量的位置参数，而 ** 可以用于传递可变数量的关键字参数，如: def foo(*args, **kwargs): …</li>
<li>与C++中的指针类似，* 可以用于声明指针类型的变量，而 ** 则可以用于声明指向指针的指针类型的变量。但与C++不同的是，Python中的指针实际上是<strong>对象的引用</strong>，而不是内存地址，因此没有C++中的指针算术运算和指针类型转换等操作。<ul>
<li>与 C++ 不同的是，Python 中的对象引用是一个高级抽象，它们隐藏了对象的实际内存地址，因此 Python 中的引用和指针不是同一概念。在 Python 中，我们不需要显式地管理内存，而是由 Python 解释器自动处理内存管理的细节。因此，Python 中的引用更像是一个符号，它与实际的内存地址之间存在一个间接的映射关系。</li>
</ul>
</li>
</ul>
</li>
<li><p>self 与 cls:</p>
<ul>
<li>cls 是 Python 中类方法的第一个参数的常规名称。 它指的是类本身而不是类的实例。 它类似于在实例方法中使用 self。</li>
<li>在类方法中，cls 用于访问类级别的属性和方法，以及创建类的新实例。</li>
</ul>
</li>
<li><p>修好bug,在手机上测好 runtime; 目标: PSNR -&gt; 28, SSIM -&gt; 0.8, runtime -&gt; 30ms</p>
<ul>
<li>从VapSR_3_2开始在手机上都跑不通runtime测试了</li>
<li>通过tf.lite.TFLiteConverter.from_saved_model(‘path_to_model’)创建converter,转换为tflite模型后可以通过netron查看模型结构并分析可能的错误</li>
<li>使用tf.lite.TFLiteConverter.from_keras_model()或者tf.lite.TFLiteConverter.from_saved_model()使用创建converter的话总会遭遇两个问题<ol>
<li>model  input_size: [1,1,1,3] output_size[1,1,1,3] 异常</li>
<li>Make sure you apply&#x2F;link the Flex delegate before inference.</li>
<li>综上推荐配合model.save(‘path_to_model’)存为SavedModel格式，然后定义好concrete_func &#x3D; model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY ],使用tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])规避掉这两个问题</li>
</ol>
</li>
</ul>
</li>
<li><p>通过QuantizeConfig和Quantizer配合实现layer activations weights的自定义量化策略</p>
</li>
<li><p>上下文管理器用于管理某个代码块的上下文环境</p>
<ul>
<li>Python 中常见的上下文管理器包括 with open() as f 中的 open() 函数和 with tf.Session() as sess 中的 tf.Session() 函数等。</li>
<li>在 with 代码块结束后，Python 会自动调用上下文管理器的 __exit__ 方法，以确保资源的释放和清理等工作的完成。同时，上下文管理器可以在 __enter__ 方法中完成一些初始化工作。在 with 代码块内部，可以使用上下文管理器返回的对象，来操作上下文环境中的资源</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>tensorflow复现高通torch QuickSRNet 8-bit 量化</p>
<ul>
<li>android_aarch64代表的是基于64位ARM架构的Android设备，也被称为ARMv8-A架构.通常用于高端设备，如智能手机和平板电脑。</li>
<li>android_arm代表的是基于32位ARM架构的Android设备。通常用于低端设备，如廉价智能手机、平板电脑和物联网设备</li>
</ul>
</li>
<li><p>困扰了至少3周的bug： TFlite GPU Delegate init Batch size mismatch -&gt; solved</p>
<ul>
<li>根据<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/issues/34525#issuecomment-564755977">this link</a>提前规避了tflite gpu delegate不支持全连接层，即利用1*1全连接层替代</li>
<li>从0到1一点点逐个测试可能出问题的模块，最终定位在pixel norm模块(由tf.reshape和tf.keras.layers.LayerNormalization构成)，换为LayerNormalization得到解决，PSNR甚至有一点点提升:)</li>
</ul>
</li>
<li><p>奇怪的问题，在转换Functional Model为tflite模型时，import tensorflow.keras.backend as K 在模型中使用k.clip()时总是提示K未定义 -&gt; 直接更换为tf.keras.backend.clip()解决</p>
</li>
<li><p>在训练Mobile VSR小模型时，GPU利用率低的问题</p>
<ol>
<li>不是由于CPU读取处理数据慢造成的，增加线程无效</li>
<li>也不是batch size大造成的，减小batch size无效</li>
<li>想要提高GPU利用率估计有两个途径,一是增大模型而是使用nvidia DALI数据读取加速库</li>
</ol>
</li>
<li><p>感受野(receptive field) 计算</p>
<ul>
<li><p>假设输入图像大小为$W_{in}\times H_{in}$，卷积核大小为$k\times k$，步长为$s$，当前卷积层的感受野大小为$F_{in}$，则下一层的感受野大小$F_{out}$为：</p>
<p> $F_{out} &#x3D; F_{in} + (k - 1) \times \text{dilation rate}$</p>
<p>其中，$\text{dilation rate}$表示卷积核的膨胀率，如果不使用膨胀卷积，则$\text{dilation rate} &#x3D; 1$。如果下一层是池化层，则$s &#x3D; k$，并且不考虑膨胀率。</p>
<p>设输入图像大小为$224\times 224$，第一个卷积层使用$3\times 3$大小的卷积核，步长为1，不使用膨胀卷积。则第一个卷积层的感受野大小为$3$。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>results</p>
</li>
</ol>
<h4 id="milestone-0"><a href="#milestone-0" class="headerlink" title="milestone_0:"></a><strong>milestone_0:</strong></h4><hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SWRN_0</td>
<td>Origin</td>
<td>REDS</td>
<td>27.931335</td>
<td>0.7803562</td>
<td>43,472</td>
<td>25.6</td>
<td></td>
</tr>
<tr>
<td>SWRN_1</td>
<td>recon_trunk block num&#x3D;2</td>
<td>REDS</td>
<td>27.820051</td>
<td>0.77666414</td>
<td>36,512</td>
<td>26.9</td>
<td></td>
</tr>
<tr>
<td>ELSR_0(vsr 22 winner)</td>
<td>Origin</td>
<td>REDS</td>
<td>26.716854</td>
<td>0.73988235</td>
<td>3,468</td>
<td>19.3</td>
<td></td>
</tr>
<tr>
<td>RLFN_0(esr 22 winner)</td>
<td>Origin</td>
<td>REDS</td>
<td>26.78721</td>
<td>0.7389487</td>
<td>306,992</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>VapSR_0</td>
<td>Origin</td>
<td>REDS</td>
<td>28.103758</td>
<td>0.7864979</td>
<td>154,252</td>
<td>5191.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_1</td>
<td>Replace feature extraction conv and VAB’s 2 con1X1 with blueprint conv</td>
<td>REDS</td>
<td>28.02941</td>
<td>0.7845887</td>
<td>155,916</td>
<td>5798.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_2</td>
<td>Replace feature extraction conv with blueprint conv and  reduce Attention’s kernel size&#x3D;3X3</td>
<td>REDS</td>
<td>28.021387</td>
<td>0.7831156</td>
<td>131,276</td>
<td>2694.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_3</td>
<td>Correct custom realization of pixel normalization</td>
<td>REDS</td>
<td>28.018507</td>
<td>0.7836466</td>
<td>131,276</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>VapSR_3_1</td>
<td>Reduce VAB blocks from 11 to 5</td>
<td>REDS</td>
<td>27.826998</td>
<td>0.7771207</td>
<td>73,484</td>
<td>1222.0</td>
<td></td>
</tr>
<tr>
<td>VapSR_3_2</td>
<td>Realize Pixel Normalization with tf.reshape() and tf.keras.layers.LayerNormalization(); Reduce VAB blocks from 5 to 4</td>
<td>REDS</td>
<td>27.550034</td>
<td>0.7687168</td>
<td>64,108</td>
<td>error</td>
<td></td>
</tr>
<tr>
<td>VapSR_4</td>
<td>apply pruning, weights clustering to conv kernels</td>
<td>REDS</td>
<td>27.833515(suspect)</td>
<td>0.7771123(suspect)</td>
<td>32,054(64,108)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VapSR_4_2_0</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: GELU</td>
<td>REDS</td>
<td>27.666351</td>
<td>0.77187574</td>
<td>64,108</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VapSR_4_2_1</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: RELU</td>
<td>REDS</td>
<td>27.539206</td>
<td>0.7669671</td>
<td>64,108</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VapSR_4_3</td>
<td>Functional VapSR_4 with self customed pixel normalization get rid of layer normalization</td>
<td>REDS</td>
<td>27.651005</td>
<td>0.7715401</td>
<td>63,852</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em></p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: FP16  </li>
<li>Acceleration: TFLite GPU Delegate</li>
</ul>
<hr>
<h4 id="milestone-1"><a href="#milestone-1" class="headerlink" title="milestone_1:"></a><strong>milestone_1:</strong></h4><hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>VapSR_4_1</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: RELU, Attention using Partial conv</td>
<td>REDS</td>
<td>27.790268</td>
<td>0.77721727</td>
<td>59,468</td>
<td>654.0 (INT8_CPU)</td>
<td>7.462</td>
</tr>
<tr>
<td>SWAT_0</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;4)</td>
<td>REDS</td>
<td>27.842232</td>
<td>0.77754354</td>
<td>50,624</td>
<td>271.0   (FP16_CPU)</td>
<td>5.803</td>
</tr>
<tr>
<td>SWAT_1</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;2), replace fc with 1*1 conv</td>
<td>REDS</td>
<td>27.759375</td>
<td>0.77492595</td>
<td>33,984</td>
<td>252.0 (FP16_CPU)</td>
<td>3.900</td>
</tr>
<tr>
<td>SWAT_2</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv</td>
<td>REDS</td>
<td>27.760305</td>
<td>0.77487457</td>
<td>25,664</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SWAT_3</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.761642</td>
<td>0.7748446</td>
<td>25,664</td>
<td>27.8 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_3_1</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17), Partial Conv(point_wise: standard, depth_wise: groups&#x3D;out_dim), Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.754656</td>
<td>0.77461684</td>
<td>24,160</td>
<td>30.0 (FP16_TFLite GPU Delegate)</td>
<td>2.776</td>
</tr>
<tr>
<td>SWAT_3_2</td>
<td>Sliding Window, VAB Attention(receptive field&#x3D;17), Partial Conv(feature fusion maintains standard conv1*1), Channel Shuffle(mix_ratio&#x3D;2), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.74189</td>
<td>0.7742521</td>
<td>26,016</td>
<td>32.4 (FP16_TFLite GPU Delegate)</td>
<td>2.996</td>
</tr>
<tr>
<td>SWAT_4</td>
<td>Sliding Window, VAB Attention, Replace partial conv with standard convlution, Remove Channel Shuffle, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.785185</td>
<td>0.77523285</td>
<td>53,696</td>
<td>38.5 (FP16_TFLite GPU Delegate)</td>
<td>6.202</td>
</tr>
<tr>
<td>SWAT_5</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 250,000</td>
<td>REDS</td>
<td>27.811176</td>
<td>0.7763541</td>
<td>25,664</td>
<td>27.6 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_6</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 150,000, Remove convs of hidden forward&#x2F;backward</td>
<td>REDS</td>
<td>27.738842</td>
<td>0.7743317</td>
<td>21,056</td>
<td>23.6 (FP16_TFLite GPU Delegate)</td>
<td>2.417</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em></p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/02/13/OnePlus-Recovery/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/13/OnePlus-Recovery/" class="post-title-link" itemprop="url">OnePlus Recovery</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-13 12:01:08" itemprop="dateCreated datePublished" datetime="2023-02-13T12:01:08+08:00">2023-02-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 12:28:18" itemprop="dateModified" datetime="2024-12-02T12:28:18+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h2><ol>
<li>备份数据：相册 安装应用名单</li>
<li>root 解锁</li>
<li>ADB 刷入TWRP Recovery镜像</li>
<li>刷入ROM（3种方法）<ol>
<li>电脑的磁盘列表中找到手机，复制ROM至手机的内部存储，复制完成后在recovery主菜单中，点击Install，点击ROM包，滑动后进行刷入。</li>
<li>用命令将ROM推送至手机（filename.zip为ROM名称，可拖动ROM文件至命令窗口自动填入完整文件地址，或输入文件名前几个字母后按Tab键来自动补全文件名）<ul>
<li>检测设备是否连接: adb devices</li>
<li>推送ROM: adb push ROM_name.zip &#x2F;sdcard&#x2F;</li>
<li>推送完成后在recovery主菜单中点击Install，点击ROM包，滑动进行刷入。</li>
</ul>
</li>
<li>在recovery主菜单中，点击“高级”，点击ADB sideload，滑动底部按钮，在PowerShell窗口用命令：.\adb sideload ROM_name.zip</li>
</ol>
</li>
<li>刷入Magisk<ol>
<li>目的:<ul>
<li>未获得 Google「认证」的设备无法从 Play 应用商店下载安装 Netflix，Google Pay、Pokémon Go 等应用不能在已 root 的设备上正常运行，改动过系统文件的 ROM 无法通过 OEM 渠道进行正常的 OTA 更新升级……</li>
<li>Magisk 的实现方式就像是一种魔法，当被挂载的 Magisk 分区被隐藏甚至被取消挂载时，原有系统分区的完整性丝毫未损，玩需要 root 验证的游戏、运行对设备认证状态有要求的应用甚至进行需要验证系统完整性的 OTA 更新都没有任何问题。</li>
</ul>
</li>
<li>方法:<ol>
<li>在刷入前，我们先安装 Magisk App 来检查设备的信息，来确定进一步的操作。我们先到官方项目地址：<a target="_blank" rel="noopener" href="https://github.com/topjohnwu/Magisk/releases">https://github.com/topjohnwu/Magisk/releases</a> 下载 apk 文件安装。<ul>
<li>Tip:从 Magisk 22 开始，不再区分刷写用的 .zip 包与安装管理器用到的 .apk 应用安装包，二者合一且只有后缀的区别，默认提供 .apk 包，更改后缀为 .zip 后即可被刷写。</li>
</ul>
</li>
<li>打开安装后的 Magisk App，像上面的最后一张截图一样，你能看到一项名为 Ramdisk 的值。请确保此项的值为「是」「True」，我们再进行下一步</li>
</ol>
</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2023/02/07/Work-Log-0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/07/Work-Log-0/" class="post-title-link" itemprop="url">Mobile Video Super-Resolution Work Log</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-07 14:31:56" itemprop="dateCreated datePublished" datetime="2023-02-07T14:31:56+08:00">2023-02-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:23:39" itemprop="dateModified" datetime="2024-12-02T15:23:39+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Time-2023-2-7-2023-4-15"><a href="#Time-2023-2-7-2023-4-15" class="headerlink" title="Time:2023.2.7-2023.4.15"></a>Time:2023.2.7-2023.4.15</h2><h2 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h2><ol>
<li><p>PD-Quant: Post-Training Quantization based on Prediction Difference Metric（2022.12）</p>
<ul>
<li>分析优化量化参数S Z用的各个Local Metrics (MSE  or cosine distance  of the activation before and after quantization in layers)</li>
<li>PD Loss： 引入Prediction Difference决定Activation Scaling Factors</li>
<li>Distribution Correction (DC):  intermediate adjust the activation distribution on the calibration dataset</li>
</ul>
</li>
<li><p>Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation</p>
</li>
<li><p>Data-Free Network Compression via Parametric Non-uniform Mixed Precision Quantization（2022CVPR）</p>
</li>
<li><p>Computer Vision – ECCV 2022 Workshops</p>
<ul>
<li>Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution (无监督图像超分)</li>
<li>Evaluating Image Super-Resolution Performance on Mobile Devices: An Online Benchmark (SR模型直接部署基准测试)</li>
<li>Efficient Image Super-Resolution Using Vast-Receptive-Field Attention (大感受野Attention图像超分)</li>
<li>DSR: Towards Drone Image Super-Resolution (无人机图像超分)</li>
<li>Image Super-Resolution with Deep Variational Autoencoders (变分自动编码器用于SISR)</li>
<li>Light Field Angular Super-Resolution via Dense Correspondence Field Reconstruction (光场角超分辨率)</li>
<li>CIDBNet: A Consecutively-Interactive Dual-Branch Network for JPEG Compressed Image Super-Resolution (JPEG压缩图像超分)</li>
<li>XCAT - Lightweight Quantized Single Image Super-Resolution Using Heterogeneous Group Convolutions and Cross Concatenation (单图像超分)</li>
<li>RCBSR: Re-parameterization Convolution Block for Super-Resolution (结构重参数视频超分)</li>
<li>Multi-patch Learning: Looking More Pixels in the Training Phase (多patch训练策略SISR)</li>
<li>Fast Nearest Convolution for Real-Time Efficient Image Super-Resolution (Nearest Convolution替代copy原图像用于depth_to_space操作)</li>
<li>Real-Time Channel Mixing Net for Mobile Image Super-Resolution (单图像超分：channel mixing using 1*1 conv)</li>
<li>Sliding Window Recurrent Network for Efficient Video Super-Resolution (视频超分)</li>
<li>EESRNet: A Network for Energy Efficient Super-Resolution (视频超分)</li>
<li>HST: Hierarchical Swin Transformer for Compressed Image Super-Resolution (压缩图像超分)</li>
<li>Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration (压缩图像超分)</li>
</ul>
</li>
<li><p>Video Super-Resolution With Convolutional Neural Networks(2016)</p>
<ul>
<li>将当前帧与相邻帧简单concate，提升超分质量</li>
</ul>
</li>
<li><p>Frame-Recurrent Video Super-Resolution(2017)</p>
<ul>
<li>利用前帧预测的HR结果补偿当前帧超分</li>
</ul>
</li>
<li><p>Enhanced Deep Residual Networks for Single Image Super-Resolution(2017)</p>
<ul>
<li>ResBlock: 相较之前的工作减少ReLU等激活的使用</li>
<li>Upsample: conv+shuffle</li>
</ul>
</li>
<li><p>TDAN: Temporally-Deformable Alignment Network for Video Super-Resolution(CVPR2020)</p>
<ul>
<li>时序可变形卷积对齐网络用于缓解超分的伪影现象</li>
</ul>
</li>
<li><p>Efficient Reference-based Video Super-Resolution (ERVSR): Single Reference Image Is All You Need(WACV2023)</p>
<ul>
<li>单个参考帧来超分整个低分辨率视频序列，不使用每个时间步的LR帧作为参考，而只用中心时间步的一帧作为参考</li>
<li>基于注意力机制做相似性估计和对齐操作</li>
<li>动机：加速推理，减少内存消耗</li>
</ul>
</li>
<li><p>BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond(CVPR2021)</p>
</li>
<li><p>BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment(CVPR2022)</p>
</li>
<li><p>Multi-scale attention network for image super-resolution(ECCV2018)</p>
<ul>
<li>Multi-scale cross block(MSCB) 3个并行但不同dilation的卷积提取特征并融合</li>
<li>Multi-path wide-activated attention block(MWAB) 3个并行支路: 卷积 + spatial attention + channel attention concate</li>
<li>缺点: 常规的channel attention采取的global average pooling 不一定能实现正确考虑通道间相关性的目的</li>
</ul>
</li>
<li><p>Deep Video Super-Resolution using Hybrid Imaging System(2023)</p>
<ul>
<li>任务: 利用一段LR高帧率视频(main video)和一段HR低帧率视频(auxiliary video)重建HR高帧率视频</li>
<li>模型3部分：<ol>
<li>主视频超分产生基础的高清帧</li>
<li>辅助视频细节特征提取并进行对齐</li>
<li>混合视频信息聚集融合</li>
</ol>
</li>
</ul>
</li>
<li><p>STDAN: Deformable Attention Network for Space-Time Video Super-Resolution(2023)</p>
<ul>
<li>变形注意力网络 deformable attention network</li>
<li>长短距离特征插值 long short-term feature interpolation (LSTFI)</li>
<li>时空变形特征聚集 spatial–temporal deformable feature aggregation (STDFA)</li>
</ul>
</li>
<li><p>ShuffleMixer: An Efficient ConvNet for Image Super-Resolution(NTIRE2022)</p>
<ul>
<li>large convolution and channel split-shuffle operation 大卷积核搭配通道分割-混合操作</li>
<li>add the Fused-MBConv after every two shuffle mixer layers 两层shuffle-mixer层之后接Fused-MBConv层克服局部特征提取不完善的问题</li>
</ul>
</li>
<li><p>An Implicit Alignment for Video Super-Resolution (ArXiv 2023)</p>
<ul>
<li>static upsample evolution: 静态插值上采样如 bilinear、nearest插值的动态化演进</li>
<li>implicit attention based alignment integrate with local window key&amp;value position encoding and query(motion estimation&#x2F;flow) position encoding: 基于注意力隐式对齐并结合局部窗口键值位置编码和运动补偿位置编码</li>
</ul>
</li>
<li><p>Rethinking Alignment in Video Super-Resolution Transformers (NIPS 2022)</p>
<ul>
<li>矩阵点乘：tf.multiply(A,B) &#x3D; A * B</li>
<li>矩阵叉乘：tf.matmul(A,B) &#x3D; A @ B</li>
</ul>
</li>
</ol>
<h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><ol>
<li><p>发现以前文章的问题尝试改进和解决 -&gt; 单纯比较runtime必败</p>
</li>
<li><p>transformer PTQ -&gt; 暂时不考虑, 专心workshop提性能</p>
</li>
<li><p>从第一个work出paper的角度,可以考虑新的压缩方面的idea应用于MAI video super resolution</p>
<ul>
<li>dataset -&gt; train: REDS, test: REDS4(Clips 000, 011, 015, 020 of REDS training set)</li>
<li>mobile video super resolution related paper</li>
<li>frontier -&gt; Optical Flow</li>
</ul>
</li>
<li><p>尝试blind video super resolution -&gt; 放弃</p>
</li>
<li><table>
<thead>
<tr>
<th>Compared Solutions</th>
<th>Model Size, KB</th>
<th>PSNR</th>
<th>SSIM</th>
<th>Runtime, ms</th>
</tr>
</thead>
<tbody><tr>
<td>MVideoSR</td>
<td>17</td>
<td>27.34</td>
<td>0.7799</td>
<td>3.05</td>
</tr>
<tr>
<td>ZX_VIP</td>
<td>20</td>
<td>27.52</td>
<td>0.7872</td>
<td>3.04</td>
</tr>
<tr>
<td>Fighter</td>
<td>11</td>
<td>27.34</td>
<td>0.7816</td>
<td>3.41</td>
</tr>
<tr>
<td>XJTU-MIGU SUPER</td>
<td>50</td>
<td>27.77</td>
<td>0.7957</td>
<td>3.25</td>
</tr>
<tr>
<td>BOE-IOT-AIBD</td>
<td>40</td>
<td>27.71</td>
<td>0.7820</td>
<td>1.97</td>
</tr>
<tr>
<td>GenMedia Group</td>
<td>135</td>
<td>28.40</td>
<td>0.8105</td>
<td>3.10</td>
</tr>
<tr>
<td>NCUT VGroup</td>
<td>35</td>
<td>27.46</td>
<td>0.7822</td>
<td>1.39</td>
</tr>
<tr>
<td>Mortar ICT</td>
<td>75</td>
<td>22.91</td>
<td>0.7546</td>
<td>1.76</td>
</tr>
<tr>
<td>RedCat AutoX</td>
<td>62</td>
<td>27.71</td>
<td>0.7945</td>
<td>7.26</td>
</tr>
<tr>
<td>221B</td>
<td>186</td>
<td>28.19</td>
<td>0.8093</td>
<td>10.1</td>
</tr>
</tbody></table>
</li>
<li><p>了解最新的基于数据集 REDS &#x2F; Viemo-90K &#x2F; Vid4 &#x2F; UDM10 &#x2F; SPMCS &#x2F; RealVSR的最新研究进展</p>
<table>
<thead>
<tr>
<th>Paper</th>
<th>Source</th>
<th>Training Set</th>
<th>Testing Set</th>
</tr>
</thead>
<tbody><tr>
<td>Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search</td>
<td>ICCV 2021</td>
<td>DIV2K</td>
<td>Set5, Set14, B100 and Urban100</td>
</tr>
<tr>
<td>LiDeR: Lightweight Dense Residual Network for Video Super-Resolution on Mobile Devices</td>
<td>2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)</td>
<td>REDS</td>
<td>REDS</td>
</tr>
<tr>
<td>Cross-Resolution Flow Propagation for Foveated Video Super-Resolution</td>
<td>Winter Conference on Applications of Computer Vision. 2023</td>
<td>REDS</td>
<td>REDS</td>
</tr>
<tr>
<td>Online Video Super-Resolution with Convolutional Kernel Bypass Graft</td>
<td>arxiv 2022.8</td>
<td>REDS</td>
<td>REDS</td>
</tr>
<tr>
<td>Real-Time Super-Resolution for Real-World Images on Mobile Devices</td>
<td>2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR)</td>
<td>DIV2K</td>
<td>DIV2K, Set5, Set14, BSD100, Manga109, and Urban100</td>
</tr>
<tr>
<td>Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting</td>
<td>CVPR 2023</td>
<td>VSD4K</td>
<td>VSD4K</td>
</tr>
<tr>
<td>Rethinking Alignment in Video Super-Resolution Transformers</td>
<td>NeurIPS 2022</td>
<td>REDS</td>
<td>REDS</td>
</tr>
</tbody></table>
</li>
<li><p>SWAT的PSNR最好要刷到28以上, 完成 pruning, weight clustering, INT8&#x2F;FP16 quantization</p>
</li>
<li><p>测试fintune之后的tensorflow模型和tflite模型 -&gt;</p>
</li>
<li><p>对比的方法要在同一设置下 -&gt; 设置对比排行榜</p>
</li>
<li><p>实验：SWRN整体框架不变替换Partial Standard Conv加持的VAB -&gt; PSNR：27.76 无明显提高</p>
</li>
<li><p>查资料理解：attention机制怎样实现，怎样起作用，是否需要级联叠加</p>
</li>
<li><p>应用MobileOne结构重参数</p>
</li>
</ol>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><h3 id="Full-Reference"><a href="#Full-Reference" class="headerlink" title="Full-Reference"></a>Full-Reference</h3><ol>
<li>Peak Signal to Noise Ratio (PSNR)</li>
<li>Structural SIMilarity (SSIM)</li>
<li>Gradient Magnitude Similarity Deviation (GMSD)</li>
</ol>
<h3 id="No-Reference"><a href="#No-Reference" class="headerlink" title="No-Reference"></a>No-Reference</h3><ol>
<li>Naturalness Image Quality Evaluator (NIQE)</li>
<li>Blind&#x2F;Referenceless Image Spatial QUality Evaluator (BRISQUE)</li>
<li>Distortion Identification-based Image Verity and INtegrity Evalutation (DIIVINE)</li>
<li>BLind Image Integrity Notator using DCT-Statistics (BLIINDS)</li>
</ol>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Milestone-0"><a href="#Milestone-0" class="headerlink" title="Milestone_0"></a>Milestone_0</h3><table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>VapSR_4_1</td>
<td>Functional VapSR_4 with pixel norm realized by layer normalization, VAB activation: RELU, Attention using Partial conv</td>
<td>REDS</td>
<td>27.790268</td>
<td>0.77721727</td>
<td>59,468</td>
<td>654.0 (INT8_CPU)</td>
<td>7.462</td>
</tr>
<tr>
<td>SWAT_0</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;4)</td>
<td>REDS</td>
<td>27.842232</td>
<td>0.77754354</td>
<td>50,624</td>
<td>271.0   (FP16_CPU)</td>
<td>5.803</td>
</tr>
<tr>
<td>SWAT_1</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;2), replace fc with 1*1 conv</td>
<td>REDS</td>
<td>27.759375</td>
<td>0.77492595</td>
<td>33,984</td>
<td>252.0 (FP16_CPU)</td>
<td>3.900</td>
</tr>
<tr>
<td>SWAT_2</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv</td>
<td>REDS</td>
<td>27.760305</td>
<td>0.77487457</td>
<td>25,664</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SWAT_3</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.761642</td>
<td>0.7748446</td>
<td>25,664</td>
<td>27.8 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_3_1</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17), Partial Conv(point_wise: standard conv, depth_wise: group conv), Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.761642</td>
<td>0.7748446</td>
<td>25,664</td>
<td>27.8 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_3_2</td>
<td>Sliding Window, VAB Attention(large receptive field&#x3D;17), Partial Conv(point_wise: standard conv, depth_wise: group conv), Channel Shuffle(mix_ratio&#x3D;2), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.74189</td>
<td>0.7742521</td>
<td>26,016</td>
<td>32.4 (FP16_TFLite GPU Delegate)</td>
<td>2.996</td>
</tr>
<tr>
<td>SWAT_4</td>
<td>Sliding Window, VAB Attention, Replace partial conv with standard convlution, Remove Channel Shuffle, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.785185</td>
<td>0.77523285</td>
<td>53,696</td>
<td>38.5 (FP16_TFLite GPU Delegate)</td>
<td>6.202</td>
</tr>
<tr>
<td>SWAT_5</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 250,000</td>
<td>REDS</td>
<td>27.811176</td>
<td>0.7763541</td>
<td>25,664</td>
<td>27.6 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
<tr>
<td>SWAT_6</td>
<td>Sliding Window, VAB Attention, Partial Conv, Modified Channel Shuffle (mix_ratio:1), Remove convs of hidden forward&#x2F;backward</td>
<td>REDS</td>
<td>27.738842</td>
<td>0.7743317</td>
<td>21,056</td>
<td>- (FP16_TFLite GPU Delegate)</td>
<td>2.417</td>
</tr>
<tr>
<td>SWAT_7</td>
<td>Sliding Window, 3 branchs VAB Attention, Partial Conv, Remove Channel Shuffle, Replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.645552</td>
<td>0.77121794</td>
<td>18,144</td>
<td>- (FP16_TFLite GPU Delegate)</td>
<td>2.090</td>
</tr>
<tr>
<td>SWAT_8</td>
<td>Sliding Window, VAB Attention modified 2</td>
<td>REDS</td>
<td>27.782675</td>
<td>0.77573705</td>
<td>45,424</td>
<td>- (FP16_TFLite GPU Delegate)</td>
<td>5.200</td>
</tr>
<tr>
<td>SWAT_9</td>
<td>Sliding Window, Non Activation Block</td>
<td>REDS</td>
<td>27.636255</td>
<td>0.7709387</td>
<td>23,648</td>
<td>288.0 (FP16_TFLite GPU Delegate)</td>
<td>2.113</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em>  </p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<h3 id="Milestone-1"><a href="#Milestone-1" class="headerlink" title="Milestone_1"></a>Milestone_1</h3><table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SWAT_3_3</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;13 with channel shuffle[Dense(unints)]), Partial Conv(standard conv), Replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.761633</td>
<td>0.7752705</td>
<td>27,472</td>
<td>30.3 (FP16_TFLite GPU Delegate)</td>
<td>3.165</td>
</tr>
<tr>
<td>SWAT_3_4</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;13 without channel shuffle[Dense(unints)], stack 2 blocks), Partial Conv(standard conv), Replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.80347</td>
<td>0.77701694</td>
<td>32,832</td>
<td>40.9 (FP16_TFLite GPU Delegate)</td>
<td>3.798</td>
</tr>
<tr>
<td>SWAT_3_5</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17 without channel shuffle[Dense(unints)], stack 2 blocks, pointwise conv for channel fusion without PConv), Partial Conv(standard conv), Replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.840628</td>
<td>0.7774375</td>
<td>37,312</td>
<td>39.4 (FP16_TFLite GPU Delegate)</td>
<td>4.302</td>
</tr>
<tr>
<td>SWAT_3_6</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17 without channel shuffle[Dense(unints)], stack 2 blocks, pointwise conv for channel fusion without PConv), Partial Conv(standard conv), Replace pixel normalization with layer normalization, Shallow feature extraction using standard conv</td>
<td>REDS</td>
<td>27.8165</td>
<td>0.7774126</td>
<td>42,624</td>
<td>40.7 (FP16_TFLite GPU Delegate)</td>
<td>4.916</td>
</tr>
<tr>
<td>SWAT_3_7</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17 without channel shuffle[Dense(unints)], stack 2 blocks, pointwise conv for channel fusion without PConv), Partial Conv(standard conv), Replace pixel normalization with layer normalization, Remove concat and unpack of hidden state</td>
<td>REDS</td>
<td>27.182861</td>
<td>0.7562948</td>
<td>29,136</td>
<td>29.0 (FP16_TFLite GPU Delegate)</td>
<td>3.357</td>
</tr>
<tr>
<td>SWAT_3_8</td>
<td>Sliding Window, VAB Attention(large reception field&#x3D;17 without channel shuffle[Dense(unints)], stack 2 blocks, pointwise conv for channel fusion without PConv), Partial Conv(standard conv), Replace pixel normalization with layer normalization, Remove concat and unpack of hidden state, Increase channels of fusion attention</td>
<td>REDS</td>
<td>27.564552</td>
<td>0.7688081</td>
<td>56,032</td>
<td>39.1 (FP16_TFLite GPU Delegate)</td>
<td>6.456</td>
</tr>
<tr>
<td>SWAT_3_9</td>
<td>Sliding Window, VAB Attention and IMDB hybrid</td>
<td>REDS</td>
<td>27.95189</td>
<td>0.7806478</td>
<td>53,312</td>
<td>42.8 (FP16_TFLite GPU Delegate)</td>
<td>6.367</td>
</tr>
<tr>
<td>SWAT_3_10</td>
<td>Sliding Window, Finetuned VAB Attention</td>
<td>REDS</td>
<td>27.846352</td>
<td>0.77762717</td>
<td>53,512</td>
<td>49.0 (FP16_TFLite GPU Delegate)</td>
<td>6.170</td>
</tr>
<tr>
<td>ABPN_0</td>
<td>Origin</td>
<td>REDS</td>
<td>27.92307</td>
<td>0.779504</td>
<td>62,048</td>
<td>38.1&#x2F;35.7 (INT8&#x2F;FP16_TFLite GPU Delegate)</td>
<td>7.137</td>
</tr>
<tr>
<td>ABPN_1</td>
<td>GenMedia Group Modified(L1 Charbonnier loss; crop_size:64)</td>
<td>REDS</td>
<td>27.858198</td>
<td>0.7780704</td>
<td>58,304</td>
<td>37.1&#x2F;33.0 (INT8&#x2F;FP16_TFLite GPU Delegate)</td>
<td>6.699</td>
</tr>
<tr>
<td>ABPN_2</td>
<td>GenMedia Group Modified(MAE loss; crop_size:96)</td>
<td>REDS</td>
<td>27.875465</td>
<td>0.7783027</td>
<td>58,304</td>
<td>37.1&#x2F;33.0 (INT8&#x2F;FP16_TFLite GPU Delegate)</td>
<td>6.699</td>
</tr>
<tr>
<td>AFAVSR_0</td>
<td>Multiple frames aggregation attention (num_feat&#x3D;48, d_atten&#x3D;64, num_blocks&#x3D;2)</td>
<td>REDS</td>
<td>27.837406</td>
<td>0.77741796</td>
<td>68,368</td>
<td>44.5 (FP16_TFLite GPU Delegate)</td>
<td>7.872</td>
</tr>
<tr>
<td>AFAVSR_1</td>
<td>Multiple frames aggregation attention (num_feat&#x3D;16, d_atten&#x3D;32, num_blocks&#x3D;8)</td>
<td>REDS</td>
<td>27.829765</td>
<td>0.7763255</td>
<td>44,016</td>
<td>36.6 (FP16_TFLite GPU Delegate)</td>
<td>5.069</td>
</tr>
<tr>
<td>AFAVSR_2</td>
<td>Multiple frames aggregation attention (num_feat&#x3D;16, d_atten&#x3D;32, num_blocks&#x3D;2)</td>
<td>REDS</td>
<td></td>
<td></td>
<td></td>
<td>(FP16_TFLite GPU Delegate)</td>
<td></td>
</tr>
<tr>
<td>AFAVSR_3</td>
<td>All batch frames aggregation attention (num_feat&#x3D;32, d_atten&#x3D;64, num_blocks&#x3D;2)</td>
<td>REDS</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SORT_0</td>
<td>Sliding Window, IMDB</td>
<td>REDS</td>
<td>27.738451</td>
<td>0.77409536</td>
<td>17,356</td>
<td>20.6 (FP16_TFLite GPU Delegate)</td>
<td>2.084</td>
</tr>
<tr>
<td>SORT_1</td>
<td>Sliding Window, IMDB, ConvTail num_out_channel&#x3D;48</td>
<td>REDS</td>
<td>27.75588</td>
<td>0.7749552</td>
<td>19,660</td>
<td>21.6 (FP16_TFLite GPU Delegate)</td>
<td>2.351</td>
</tr>
<tr>
<td>SORT_2</td>
<td>Sliding Window, IMDB, multi-branch distillation channel num hyperparameter tunning</td>
<td>REDS</td>
<td>27.93981</td>
<td>0.7808094</td>
<td>45,264</td>
<td>35.6 (FP16_TFLite GPU Delegate)</td>
<td>5.385</td>
</tr>
<tr>
<td>SORT_3</td>
<td>Sliding Window, IMDB, multi-branch distillation channel num hyperparameter tunning, Replace SEL with CCA( Contrast-Aware Channel Attention)</td>
<td>REDS</td>
<td>27.867216</td>
<td>0.7790734</td>
<td>39,144</td>
<td>35.3 (FP16_TFLite GPU Delegate)</td>
<td>4.414</td>
</tr>
<tr>
<td>SORT_4</td>
<td>Sliding Window, Modified IMDB equipped with channel attention mechanism</td>
<td>REDS</td>
<td>27.769545</td>
<td>0.7755401</td>
<td>39,120</td>
<td>41.3(FP16_TFLite GPU Delegate)</td>
<td>5.725</td>
</tr>
<tr>
<td>SORT_5</td>
<td>Sliding Window, Modified IMDB equipped with larger channel width and channel reduction&#x2F;aggregation using 1*1 convs</td>
<td>REDS</td>
<td>28.13419</td>
<td>0.78656757</td>
<td>166,944</td>
<td>85.9(FP16_TFLite GPU Delegate)</td>
<td>19.566</td>
</tr>
<tr>
<td>SORT_6</td>
<td>Sliding Window, Modified IMDB equipped with dynamic channel width</td>
<td>REDS</td>
<td>27.944357</td>
<td>0.7809873</td>
<td>48,216</td>
<td>- (FP16_TFLite GPU Delegate)</td>
<td>-</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em>  </p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<h3 id="Milestone-2"><a href="#Milestone-2" class="headerlink" title="Milestone_2"></a>Milestone_2</h3><table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>VSR_0</td>
<td>Sliding Window, Non Activation Block</td>
<td>REDS</td>
<td>27.673386</td>
<td>0.7725643</td>
<td>26,368</td>
<td>57.9 (FP16_TFLite GPU Delegate)</td>
<td>2.417</td>
</tr>
<tr>
<td>VSR_1</td>
<td>Attention Alignment_0, Non Activation Block</td>
<td>REDS</td>
<td>27.508242</td>
<td>0.76671493</td>
<td>17,440</td>
<td>42.0 (FP16_TFLite GPU Delegate)</td>
<td>1.677</td>
</tr>
<tr>
<td>VSR_2</td>
<td>Attention Alignment_1, Non Activation Block,Rectify BSConvolution</td>
<td>REDS</td>
<td>27.53437</td>
<td>0.7678055</td>
<td>17,776</td>
<td>error (FP16_TFLite GPU Delegate)</td>
<td>2.035</td>
</tr>
<tr>
<td>VSR_3</td>
<td>VSR_2 Ablation: Attention Alignment_1</td>
<td>REDS</td>
<td>27.414068</td>
<td>0.76361054</td>
<td>17,413</td>
<td>60.3 (FP16_TFLite GPU Delegate)</td>
<td>1.793</td>
</tr>
<tr>
<td>VSR_4</td>
<td>VSR_2 -&gt; modify Non Activation Block using partial conv</td>
<td>REDS</td>
<td>27.784992</td>
<td>0.7769825</td>
<td>43,120</td>
<td>error (FP16_TFLite GPU Delegate)</td>
<td>4.958</td>
</tr>
<tr>
<td>VSR_5</td>
<td>VSR_4 Ablation: RGB out channels sharing upsample result</td>
<td>REDS</td>
<td>27.835686</td>
<td>0.7776796</td>
<td>47,728</td>
<td>- (FP16_TFLite GPU Delegate)</td>
<td>5.491</td>
</tr>
<tr>
<td>VSR_6</td>
<td>VSR_5 Finetune: Non Activation Block channel numbers modify</td>
<td>REDS</td>
<td>27.783165</td>
<td>0.7768693</td>
<td>28,976</td>
<td>error (FP16_TFLite GPU Delegate)</td>
<td>3.699</td>
</tr>
<tr>
<td>VSR_7</td>
<td>Light weight hidden states attention alignment; Blue Print convolution for shallow feature extraction; Multi-Stage ExcavatoR(MSER) combined with partial convolution and simplified channel attention</td>
<td>REDS</td>
<td>27.470276</td>
<td>0.7664948</td>
<td>81,806</td>
<td>66.1 (FP16_TFLite GPU Delegate)</td>
<td>7.938</td>
</tr>
<tr>
<td>VSR_8</td>
<td>Light weight hidden states attention alignment; Blue Print convolution for shallow feature extraction; Nonlinear activation free block</td>
<td>REDS</td>
<td>27.91092</td>
<td>0.77971315</td>
<td>66,312</td>
<td>64.7 (FP16_TFLite GPU Delegate)</td>
<td>7.269</td>
</tr>
<tr>
<td>VSR_9</td>
<td>vsr_9 ablation: feature alignment</td>
<td>REDS</td>
<td>27.91092</td>
<td>0.77971315</td>
<td>39,792</td>
<td>44.5 (FP16_TFLite GPU Delegate)</td>
<td>4.218</td>
</tr>
<tr>
<td>VSR_10</td>
<td>motivation: IMDB + PartialConv + VapSR + BSConv</td>
<td>REDS</td>
<td>27.963232</td>
<td>0.780958</td>
<td>44,256</td>
<td>48.6 (FP16_TFLite GPU Delegate)</td>
<td>5.103</td>
</tr>
<tr>
<td>VSR_11</td>
<td>VSR_10 ablation: hidden state conv using bias</td>
<td>REDS</td>
<td>27.948818</td>
<td>0.7809571</td>
<td>44,288</td>
<td>47.2 (FP16_TFLite GPU Delegate)</td>
<td>5.103</td>
</tr>
<tr>
<td>VSR_12</td>
<td>VSR_10 ablation: hidden state process using modified IMDB</td>
<td>REDS</td>
<td>27.953104</td>
<td>0.7807622</td>
<td>57,696</td>
<td>62.8 (FP16_TFLite GPU Delegate)</td>
<td>6.649</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em>  </p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<h3 id="Milestone-3"><a href="#Milestone-3" class="headerlink" title="Milestone_3"></a>Milestone_3</h3><table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>MVSR_0</td>
<td>modified IMDB IMDB + PartialConv + VapSR + BSConv; deprecate hidden state forward and backward; light weight feature alignment</td>
<td>REDS</td>
<td>27.915539</td>
<td>0.7799377</td>
<td>35,777</td>
<td>38.8 (FP16_TFLite GPU Delegate)</td>
<td>4.068</td>
</tr>
<tr>
<td>MVSR_1</td>
<td>modified IMDB IMDB + PartialConv + VapSR + BSConv; deprecate hidden state forward and backward; light weight frame alignment</td>
<td>REDS</td>
<td>27.932716</td>
<td>0.7810435</td>
<td>34,473</td>
<td>44.3 (FP16_TFLite GPU Delegate)</td>
<td>3.976</td>
</tr>
<tr>
<td>MVSR_2</td>
<td>MVSR_1 Ablation: light weight frame alignment</td>
<td>REDS</td>
<td>27.929586</td>
<td>0.78039753</td>
<td>34,208</td>
<td>35.4 (FP16_TFLite GPU Delegate)</td>
<td>3.944</td>
</tr>
<tr>
<td>MVSR_3</td>
<td>MVSR_1 Ablation: large receptive field in SMDB -&gt; reduce: 3x3 + 3x3 dilated</td>
<td>REDS</td>
<td>27.892586</td>
<td>0.7790079</td>
<td>32,169</td>
<td>41.1 (FP16_TFLite GPU Delegate)</td>
<td>3.711</td>
</tr>
<tr>
<td>MVSR_4</td>
<td>MVSR_2 Ablation: large receptive field in SMDB -&gt; increase: 7x7 + 7x7 dilated</td>
<td>REDS</td>
<td>27.958328</td>
<td>0.78145003</td>
<td>37,664</td>
<td>42.4 (FP16_TFLite GPU Delegate)</td>
<td>4.343</td>
</tr>
<tr>
<td>MVSR_5</td>
<td>MVSR_1 Ablation: large receptive field in SMDB -&gt; increase: 7x7 + 7x7 dilated</td>
<td>REDS</td>
<td>27.936714</td>
<td>0.7809204</td>
<td>37,929</td>
<td>49.8 (FP16_TFLite GPU Delegate)</td>
<td>4.375</td>
</tr>
<tr>
<td>MVSR_6</td>
<td>modified IMDB IMDB + PartialConv based pixel attention version_0 + VapSR + BSConv; light weight frame alignment</td>
<td>REDS</td>
<td>27.884369</td>
<td>0.7790964</td>
<td>34,473</td>
<td>44.4 (FP16_TFLite GPU Delegate)</td>
<td>4.246</td>
</tr>
<tr>
<td>MVSR_7</td>
<td>modified IMDB IMDB + PartialConv based pixel attention version_1 + VapSR + BSConv; light weight frame alignment</td>
<td>REDS</td>
<td>27.858534</td>
<td>0.77831227</td>
<td>35,769</td>
<td>44.5 (FP16_TFLite GPU Delegate)</td>
<td>4.387</td>
</tr>
<tr>
<td>MVSR_8</td>
<td>MVSR_1 Ablation: SEL -&gt; Channel Attention</td>
<td>REDS</td>
<td>27.610485</td>
<td>0.7696045</td>
<td>29,145</td>
<td>40.5 (FP16_TFLite GPU Delegate)</td>
<td>3.001</td>
</tr>
<tr>
<td>MVSR_9</td>
<td>MVSR_1 Ablation: Channel fuse + SEL -&gt; FlashModule + Channel fuse</td>
<td>REDS</td>
<td>28.043566</td>
<td>0.7842476</td>
<td>96,249</td>
<td>72.8 (FP16_TFLite GPU Delegate)</td>
<td>10.684</td>
</tr>
<tr>
<td>MVSR_10</td>
<td>Partial conv idea applied to MSDB and Attention(i.e. SEL)</td>
<td>REDS</td>
<td>27.86422</td>
<td>0.7783118</td>
<td>27,081</td>
<td>41.2 (FP16_TFLite GPU Delegate)</td>
<td>3.031</td>
</tr>
<tr>
<td>MVSR_11</td>
<td>MVSR_10 fintune: deperecae MSDB’s channel fuse; add MDSB blocks</td>
<td>REDS</td>
<td>27.90566</td>
<td>0.7793118</td>
<td>32,553</td>
<td>48.7 (FP16_TFLite GPU Delegate)</td>
<td>3.634</td>
</tr>
<tr>
<td>MVSR_12</td>
<td>MVSR_11 ablation: MSDB’s group convolution -&gt; standard convolution</td>
<td>REDS</td>
<td>27.953104</td>
<td>0.7807622</td>
<td>68,169</td>
<td>38.4 (FP16_TFLite GPU Delegate)</td>
<td>7.737</td>
</tr>
<tr>
<td>MVSR_13</td>
<td>MVSR_12 AttentionAlign module evolution</td>
<td>REDS</td>
<td>27.966156</td>
<td>0.7809557</td>
<td>68,157</td>
<td>39.8 (FP16_TFLite GPU Delegate)</td>
<td>7.735</td>
</tr>
<tr>
<td>MVSR_13_1</td>
<td>MVSR_13 evolution: ConvTail used for increasing dimension -&gt; BSConv</td>
<td>REDS</td>
<td>27.879667</td>
<td>0.7790071</td>
<td>62,541</td>
<td>40.6 (FP16_TFLite GPU Delegate)</td>
<td>7.080</td>
</tr>
<tr>
<td>MVSR_13_2</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 1&#x2F;4</td>
<td>REDS</td>
<td>27.877321</td>
<td>0.7783568</td>
<td>37,517</td>
<td>37.1 (FP16_TFLite GPU Delegate)</td>
<td>4.152</td>
</tr>
<tr>
<td>MVSR_13_3</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 1&#x2F;8</td>
<td>REDS</td>
<td>27.79014</td>
<td>0.77567685</td>
<td>29,829</td>
<td>35.5 (FP16_TFLite GPU Delegate)</td>
<td>3.240</td>
</tr>
<tr>
<td>MVSR_13_4</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 3&#x2F;4</td>
<td>REDS</td>
<td>27.955465</td>
<td>0.78150684</td>
<td>119,149</td>
<td>84.1 (FP16_TFLite GPU Delegate)</td>
<td>13.663</td>
</tr>
<tr>
<td>MVSR_13_4_revalid</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 3&#x2F;4</td>
<td>REDS</td>
<td>27.956861</td>
<td>0.7814712</td>
<td>119,149</td>
<td>84.1 (FP16_TFLite GPU Delegate)</td>
<td>13.663</td>
</tr>
<tr>
<td>MVSR_13_5</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 7&#x2F;8</td>
<td>REDS</td>
<td>27.993414</td>
<td>0.7823691</td>
<td>152,277</td>
<td>103.0 (FP16_TFLite GPU Delegate)</td>
<td>17.506</td>
</tr>
<tr>
<td>MVSR_13_6</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 3&#x2F;8</td>
<td>REDS</td>
<td>27.948065</td>
<td>0.78071</td>
<td>50,293</td>
<td>39.0 (FP16_TFLite GPU Delegate)</td>
<td>5.651</td>
</tr>
<tr>
<td>MVSR_13_7</td>
<td>MVSR_13 ablation: fractional&#x2F;partial ratio 1&#x2F;2 -&gt; 5&#x2F;8</td>
<td>REDS</td>
<td>27.983498</td>
<td>0.7823881</td>
<td>91,109</td>
<td>86.4 (FP16_TFLite GPU Delegate)</td>
<td>10.406</td>
</tr>
<tr>
<td>MVSR_14</td>
<td>MVSR_13 ablation: - frame attention align -&gt; standard conv 1 x 1 act as frame information propogation operator</td>
<td>REDS</td>
<td>27.930904</td>
<td>0.78043896</td>
<td>68,272</td>
<td>29.7 (FP16_TFLite GPU Delegate)</td>
<td>7.746</td>
</tr>
<tr>
<td>MVSR_15</td>
<td>MVSR_13 ablation: MSDB block number 4 -&gt; 3</td>
<td>REDS</td>
<td>27.91523</td>
<td>0.77966064</td>
<td>52,965</td>
<td>33.4 (FP16_TFLite GPU Delegate)</td>
<td>6.016</td>
</tr>
<tr>
<td>MVSR_16</td>
<td>MVSR_13 ablation: No partial&#x2F;fractional; No BSconv (Blueprint Separable conv); No receptive field decomposition</td>
<td>REDS</td>
<td>27.928417</td>
<td>0.7801167</td>
<td>920,381</td>
<td>399.0 (FP16_TFLite GPU Delegate)</td>
<td>106.045</td>
</tr>
<tr>
<td>MVSR_17</td>
<td>MVSR_13 evolution: MSDB using standard conv 3 x 3, PPA using split large receptive field conv 5 x 5 + 5 x 5 dilated</td>
<td>REDS</td>
<td>27.902325</td>
<td>0.7794845</td>
<td>47,101</td>
<td>36.8 (FP16_TFLite GPU Delegate)</td>
<td>5.313</td>
</tr>
<tr>
<td>MVSR_18</td>
<td>MVSR_17 ablation: BSconv</td>
<td>REDS</td>
<td>27.893446</td>
<td>0.77926654</td>
<td>47,325</td>
<td>34.4 (FP16_TFLite GPU Delegate)</td>
<td>5.340</td>
</tr>
<tr>
<td>MVSR_19</td>
<td>MVSR_13 evolution: MSDB blocks 4 -&gt; 3; Enlarge receptive field of PPA 3 -&gt; 17</td>
<td>REDS</td>
<td>27.914143</td>
<td>0.7799854</td>
<td>60,861</td>
<td>35.9 (FP16_TFLite GPU Delegate)</td>
<td>6.924</td>
</tr>
<tr>
<td>MVSR_20</td>
<td>MVSR_13 ablation: No receptive field decomposition</td>
<td>REDS</td>
<td>27.93524</td>
<td>0.78071207</td>
<td>251,613</td>
<td>119.0 (FP16_TFLite GPU Delegate)</td>
<td>28.875</td>
</tr>
<tr>
<td>MVSR_21</td>
<td>MVSR_13 ablation: No frame align; No fractional&#x2F;partial; No BSconv; No receptive field decomposition</td>
<td>REDS</td>
<td>27.941408</td>
<td>0.7807615</td>
<td>920,128</td>
<td>400.0 (FP16_TFLite GPU Delegate)</td>
<td>106.014</td>
</tr>
<tr>
<td>MVSR_21_1</td>
<td>MVSR_13 ablation: No frame align (directly extraction from 3 consecutive frames); No fractional&#x2F;partial; No BSconv; No receptive field decomposition</td>
<td>REDS</td>
<td>27.913836</td>
<td>0.779473</td>
<td>920,992</td>
<td>399.0 (FP16_TFLite GPU Delegate)</td>
<td>106.114</td>
</tr>
<tr>
<td>MVSR_22</td>
<td>MVSR_13 ablation: No BSconv; No receptive field decomposition</td>
<td>REDS</td>
<td>27.936152</td>
<td>0.7799803</td>
<td>251,837</td>
<td>118.0 (FP16_TFLite GPU Delegate)</td>
<td>28.902</td>
</tr>
<tr>
<td>MVSR_23</td>
<td>MVSR_13 ablation: PFE PPA Standard conv -&gt; Depthwise conv</td>
<td>REDS</td>
<td>27.867388</td>
<td>0.7784562</td>
<td>32,541</td>
<td>49.5 (FP16_TFLite GPU Delegate)</td>
<td>3.632</td>
</tr>
<tr>
<td>MVSR_24</td>
<td>MVSR_13 ablation: - Partial&#x2F;Fractional Extraction</td>
<td>REDS</td>
<td>27.952333</td>
<td>0.78090274</td>
<td>186,141</td>
<td>94.3 (FP16_TFLite GPU Delegate)</td>
<td>21.449</td>
</tr>
<tr>
<td>MVSR_24_revalid</td>
<td>MVSR_13 ablation: - Partial&#x2F;Fractional Extraction (keep fc)</td>
<td>REDS</td>
<td>27.940563</td>
<td>0.7804851</td>
<td>190,493</td>
<td>102.0 (FP16_TFLite GPU Delegate)</td>
<td>21.935</td>
</tr>
<tr>
<td>MVSR_25</td>
<td>MVSR_13 ablation: - BSConv</td>
<td>REDS</td>
<td>27.929697</td>
<td>0.780183</td>
<td>68,381</td>
<td>38.8 (FP16_TFLite GPU Delegate)</td>
<td>7.762</td>
</tr>
<tr>
<td>MVSR_26</td>
<td>MVSR_13 ablation: - Large Receptive Field Decomposition</td>
<td>REDS</td>
<td>27.972654</td>
<td>0.7818956</td>
<td>251,613</td>
<td>119.0 (FP16_TFLite GPU Delegate)</td>
<td>28.875</td>
</tr>
<tr>
<td>MVSR_27</td>
<td>MVSR_13 ablation: - FC in PFE, PPA</td>
<td>REDS</td>
<td>27.945955</td>
<td>0.78067327</td>
<td>63,805</td>
<td>37.8 (FP16_TFLite GPU Delegate)</td>
<td>7.249</td>
</tr>
</tbody></table>
<hr>
<p><em>AI benchmark setting for Runtime test:</em>  </p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: INT8&#x2F;FP16  </li>
<li>Acceleration: CPU&#x2F;TFLite GPU Delegate</li>
</ul>
<h2 id="Benchmark-0"><a href="#Benchmark-0" class="headerlink" title="Benchmark_0"></a>Benchmark_0</h2><table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Source</th>
<th>Dataset</th>
<th>Test PSNR</th>
<th>Test SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Diggers</td>
<td>Real-Time Video Super-Resolution based on Bidirectional RNNs(2021 SOTA)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.98</td>
<td>-</td>
<td>39,640</td>
<td>-</td>
</tr>
<tr>
<td>2</td>
<td>VSR_12</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.981062</td>
<td>0.7824855</td>
<td>57,696</td>
<td>62.8</td>
</tr>
<tr>
<td>3</td>
<td>MVSR_4</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.958328</td>
<td>0.78145003</td>
<td>37,664</td>
<td>42.4</td>
</tr>
<tr>
<td>4</td>
<td>MVSR_12</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.953104</td>
<td>0.7807622</td>
<td>68,169</td>
<td>38.4</td>
</tr>
<tr>
<td>5</td>
<td>SORT_2</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.93981</td>
<td>0.7808094</td>
<td>45,264</td>
<td>35.6</td>
</tr>
<tr>
<td>6</td>
<td>SWRN</td>
<td>Sliding Window Recurrent Network for Efficient Video Super-Resolution (2022 SOTA)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.92</td>
<td>0.77</td>
<td>43,472</td>
<td>31.0</td>
</tr>
<tr>
<td>7</td>
<td>MVSR_11</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.90566</td>
<td>0.7793118</td>
<td>32,553</td>
<td>48.7</td>
</tr>
<tr>
<td>8</td>
<td>SWAT_3_5</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.840628</td>
<td>0.7774375</td>
<td>37,312</td>
<td>39.4</td>
</tr>
<tr>
<td>9</td>
<td>EESRNet</td>
<td>EESRNet: A Network for Energy Efficient Super-Resolution(2022)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.84</td>
<td>-</td>
<td>62,550</td>
<td>-</td>
</tr>
<tr>
<td>10</td>
<td>LiDeR</td>
<td>LiDeR: Lightweight Dense Residual Network for Video Super-Resolution on Mobile Devices (2022)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.51</td>
<td>0.76</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>11</td>
<td>EVSRNet</td>
<td>EVSRNet：Efficient Video Super-Resolution with Neural Architecture Search(2021)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.42</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>12</td>
<td>RCBSR</td>
<td>RCBSR: Re-parameterization Convolution Block for Super-Resolution(2022)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.28</td>
<td>0.775</td>
<td>-</td>
<td>-</td>
</tr>
</tbody></table>
<h2 id="Benchmark-1"><a href="#Benchmark-1" class="headerlink" title="Benchmark_1"></a>Benchmark_1</h2><table>
<thead>
<tr>
<th>Model</th>
<th>Source</th>
<th>Dataset</th>
<th>Test PSNR</th>
<th>Test SSIM</th>
<th>Params</th>
</tr>
</thead>
<tbody><tr>
<td>SSL-uni</td>
<td>Structured Sparsity Learning for Efficient Video Super-Resolution (CVPR2023)</td>
<td>REDS(train:266 test:4)</td>
<td>30.24</td>
<td>0.86</td>
<td>500,000</td>
</tr>
</tbody></table>
<h2 id="PaperWriting"><a href="#PaperWriting" class="headerlink" title="PaperWriting"></a>PaperWriting</h2><h3 id="No-1"><a href="#No-1" class="headerlink" title="No.1"></a>No.1</h3><ol>
<li>BSConvU as shallow feature extraction</li>
<li>Recurrent neural network for feature information freedom flow cross frames</li>
<li>multi distilation module through dynamic routing large ERF attention  </li>
<li>Bilineared RGB channels share same upsample result</li>
<li>Nearest conv for shorter residual inference time compared with bilinear residual</li>
</ol>
<h3 id="No-2"><a href="#No-2" class="headerlink" title="No.2"></a>No.2</h3><ol>
<li>Motivation: 移动端视频超分 Inference Time ↓, PSNR ↑, SSIM ↑</li>
<li>只用当前处理LR帧的前一个预测HR帧做参考补偿当前帧 -&gt; 拍摄的同时实时超分,不受只能对拍摄完成的视频进行超分的限制</li>
<li>假设模型中间的feature maps对输出结果不是同等贡献度，如何进行高贡献度的feature maps聚集aggregation -&gt; 做Partial Convolution accelerate inference(分析)</li>
<li>减少模型中的activation -&gt; 利用Multiply产生非线性映射的能力</li>
<li>RGB三通道共享上采样补偿 -&gt; 常规模型的RGB三通道上采样补偿是否存在高度一致性，若存在则可以共享以起到降低计算量加速推理的效果(分析)</li>
<li>蓝图卷积作为浅层特征提取 -&gt; 效果反而比标准卷积最终的效果好</li>
<li>多尺度特征(降采样到不同尺度)基于注意力机制融合 &lt;- motivation: 灵长类动物视觉皮层同一区域不同神经元感受野不同，类比到模型内则是同一层内从不同尺度&#x2F;感受野捕获更精确的空间信息或更多的纹理信息</li>
<li>短距离shortcut的fusion -&gt; 加速推理</li>
</ol>
<h3 id="No-3"><a href="#No-3" class="headerlink" title="No.3"></a>No.3</h3><ol>
<li>Motivation: 移动端视频超分 Inference Time ↓, PSNR ↑, SSIM ↑</li>
<li>辅助前后向传播的隐藏状态做对齐(auxiliary forward&#x2F;backward hidden states for feature alignment) -&gt; 提升超分结果PSNR</li>
<li>假设模型中间的feature maps对输出结果不是同等贡献度，如何进行高贡献度的feature maps聚集aggregation -&gt; 做Partial Convolution accelerate inference(分析)</li>
<li>减少模型中的activation -&gt; 利用Multiply产生非线性映射的能力,加速推理</li>
<li>考虑动态深度(adaptive existing) -&gt; 加速推理 -&gt; deprecated</li>
</ol>
<h2 id="PaperReference"><a href="#PaperReference" class="headerlink" title="PaperReference"></a>PaperReference</h2><ol>
<li>Rethinking Alignment in Video Super-Resolution Transformers(NIPS 2022) -&gt; VIT 视频超分(VSR)中帧&#x2F;特征对齐不是必要操作</li>
<li>An Implicit Alignment for Video Super-Resolution (ArXiv 2023) -&gt; bilinear interpolation&#x2F;resample 改进</li>
<li>Video Super-Resolution Transformer</li>
<li>Efficient Reference-based Video Super-Resolution (ERVSR): Single Reference Image Is All You Need (WACV 2023) -&gt; 帧序列中间帧作为参考帧辅助当前帧超分</li>
<li>MULTI-STAGE FEATURE ALIGNMENT NETWORK FOR VIDEO SUPER-RESOLUTION</li>
<li>ELSR: Extreme Low-Power Super Resolution Network For Mobile Devices</li>
<li>LiDeR: Lightweight Dense Residual Network for Video Super-Resolution on Mobile Devices</li>
<li>Look Back and Forth: Video Super-Resolution with Explicit Temporal Difference Modeling</li>
<li>COLLAPSIBLE LINEAR BLOCKS FOR SUPER-EFFICIENT SUPER RESOLUTION</li>
<li>Revisiting Temporal Alignment for Video Restoration</li>
<li>BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment</li>
<li>EVSRNet：Efficient Video Super-Resolution with Neural Architecture Search</li>
<li>BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond</li>
<li>Revisiting Temporal Modeling for Video Super-resolution  -&gt; MAI 第一届VSR 官方baseline</li>
<li>TDAN: Temporally-Deformable Alignment Network for Video Super-Resolution (CVPR 2020)</li>
<li>Video Super-resolution with Temporal Group Attention (CVPR 2020)</li>
<li>3DSRnet: Video Super-resolution using 3D Convolutional Neural Networks</li>
<li>Frame-Recurrent Video Super-Resolution</li>
<li>Video Super-Resolution With Convolutional Neural Networks</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/12/06/Model-Quantization-Papers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/12/06/Model-Quantization-Papers/" class="post-title-link" itemprop="url">Model Quantization Papers</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-12-06 19:21:11" itemprop="dateCreated datePublished" datetime="2022-12-06T19:21:11+08:00">2022-12-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:24:38" itemprop="dateModified" datetime="2024-12-02T15:24:38+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>鼻祖：Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</li>
<li>综述：<ol>
<li>Quantizing deep convolutional networks for efficient inference: A whitepaper</li>
<li>A White Paper on Neural Network Quantization</li>
</ol>
</li>
<li>上手：<ol>
<li>ZeroQ: A Novel Zero Shot Quantization Framework</li>
<li>HAWQ-V3: Dyadic Neural Network Quantization</li>
<li>Up or Down? Adaptive Rounding for Post-Training Quantization</li>
</ol>
</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Title</th>
<th>Class</th>
</tr>
</thead>
<tbody><tr>
<td>ZeroQ</td>
<td>DFQ</td>
</tr>
<tr>
<td>SQuant</td>
<td>DFQ</td>
</tr>
<tr>
<td>ACIQ</td>
<td>PTQ</td>
</tr>
<tr>
<td>GDFQ</td>
<td>DFQ</td>
</tr>
</tbody></table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/11/17/Kill-Issues-Log/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/11/17/Kill-Issues-Log/" class="post-title-link" itemprop="url">Logs of killing issues</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-17 11:12:15" itemprop="dateCreated datePublished" datetime="2022-11-17T11:12:15+08:00">2022-11-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:26:28" itemprop="dateModified" datetime="2024-12-02T15:26:28+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <table>
<thead>
<tr>
<th>Issue</th>
<th>Method</th>
</tr>
</thead>
<tbody><tr>
<td>PytorchCV package</td>
<td>Image Classification and Segmentation Models</td>
</tr>
<tr>
<td>Parameter-Argument</td>
<td>Defining-Calling</td>
</tr>
<tr>
<td>Positional</td>
<td>传参时前面不带 “变量名&#x3D;”, 顺序不可变</td>
</tr>
<tr>
<td>Keyword</td>
<td>传参时前面加上 “变量名&#x3D;”, 顺序可变</td>
</tr>
<tr>
<td>Class</td>
<td>实例化后用self指代</td>
</tr>
<tr>
<td>Method</td>
<td>类中定义的函数</td>
</tr>
<tr>
<td>Self</td>
<td>类的方法与普通的函数只有一个特别的区别——必须有一个额外的第一个参数名称, 按照惯例它的名称是self</td>
</tr>
<tr>
<td>model.modules(), model.named_modules(), model.children(), model.named_children(), model.parameters()</td>
<td>返回iterable可遍历；具有__iter__()或__getitem__()方法的对象，Python就认为它是一个iterable</td>
</tr>
<tr>
<td>卷积神经网络基本原理</td>
<td>MAC(Multiply Accumulates)乘加</td>
</tr>
<tr>
<td>FLOPs</td>
<td>is abbreviation of floating operations which includes mul &#x2F; add &#x2F; div … etc.</td>
</tr>
<tr>
<td>FLOPS</td>
<td>floating point operations per second</td>
</tr>
<tr>
<td>MACs</td>
<td>stands for multiply–accumulate operation that performs a &lt;- a + (b x c).</td>
</tr>
<tr>
<td>《TVM: End-to-End Optimization Stack for Deep Learning》</td>
<td>TVM是陈天奇领导的一个DL加速框架项目。它处于DL框架（如tensorflow、pytorch）和硬件后端（如CUDA、OpenCL）之间，兼顾了前者的易用性和后者的执行效率。</td>
</tr>
<tr>
<td>Pytorch Hook 函数</td>
<td>hook 函数用以获取我们不方便获得的一些中间变量</td>
</tr>
<tr>
<td>Magic Method: __call__()</td>
<td>将对象当方法使用</td>
</tr>
<tr>
<td>Magic Method: __new__()</td>
<td>创建类实例的静态方法</td>
</tr>
<tr>
<td>Magic Method: __repr__()</td>
<td>直接输出某个实例化对象，默认情况下输出是“类名+object at+内存地址”，可重写获得想要的属性信息</td>
</tr>
<tr>
<td>Magic Method: __del__()</td>
<td>销毁对象</td>
</tr>
<tr>
<td>Magic Method: __dir__()</td>
<td>列出对象的所有属性名、方法名</td>
</tr>
<tr>
<td>Magic Method: __dict__()</td>
<td>查看对象内部所有属性名和属性值组成的字典dict</td>
</tr>
<tr>
<td>conda配置文件.condarc</td>
<td>.condarc是conda 应用程序的配置文件，在用户家目录（windows：C:usersusername，linux：&#x2F;home&#x2F;username&#x2F;）</td>
</tr>
<tr>
<td>conda查看配置</td>
<td>conda config –show</td>
</tr>
<tr>
<td>conda添加更新镜像源</td>
<td>conda config –add channels …</td>
</tr>
<tr>
<td>conda删除更新镜像源</td>
<td>conda config –remove channels …</td>
</tr>
<tr>
<td>conda 代理</td>
<td>conda config –set proxy_servers.http … config –set proxy_servers.https …</td>
</tr>
<tr>
<td>深度学习—激活函数</td>
<td>Sigmoid、tanh、ReLU、ReLU6及变体P-R-Leaky、ELU、SELU、Swish、Mish、Maxout、hard-sigmoid、hard-swish</td>
</tr>
<tr>
<td>CUDA Toolkit</td>
<td>Nvidia 官方提供的 CUDA Toolkit 是一个完整的工具安装包，其中提供了 Nvidia 驱动程序、开发 CUDA 程序相关的开发工具包等可供安装的选项</td>
</tr>
<tr>
<td>cudatoolkit</td>
<td>Anaconda 在安装 Pytorch 等会使用到 CUDA 的框架时，会自动为用户安装 cudatoolkit，其主要包含应用程序在使用 CUDA 相关的功能时所依赖的动态链接库。在安装了 cudatoolkit 后，只要系统上存在与当前的 cudatoolkit 所兼容的 Nvidia 驱动，则已经编译好的 CUDA 相关的程序就可以直接运行，而不需要安装完整的 Nvidia 官方提供的 CUDA Toolkit</td>
</tr>
<tr>
<td>linux之ls -l命令</td>
<td>得到一个目录下的文件和子目录的详细信息，一共包含9列</td>
</tr>
<tr>
<td>Linux中bashrc位置</td>
<td>&#x2F;etc&#x2F;.bashrc</td>
</tr>
<tr>
<td>.bashrc用途</td>
<td>个性化指令；设置环境变量,所有环境变量名都是大写，Linux区分大小写</td>
</tr>
<tr>
<td>.bashrc 路径修改</td>
<td>“export PATH&#x3D;$PATH:路径” ，在原来PATH的后面继续添加了新的路径，在运行特定指令时，系统会逐个位置去寻找文件。 $PATH 表示原先设定的路径，不能遗漏。不同于DOS&#x2F;Windows，Unix类系统的环境变量的路径用冒号:分割，而不是分号;</td>
</tr>
<tr>
<td>.bashrc修改生效</td>
<td>source &#x2F;etc&#x2F;.bashrc</td>
</tr>
<tr>
<td>.bashrc文件没了怎么办？</td>
<td>从如下路径拷贝一份原始的.bashrc文件到用户home目录下:cp &#x2F;etc&#x2F;skel&#x2F;.bashrc ~&#x2F;</td>
</tr>
<tr>
<td>nvcc: command not found</td>
<td>1. nvcc安装在&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin；2.添加路径 export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib；export PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin；3. 更新配置文件 source ~&#x2F;.bashrc</td>
</tr>
<tr>
<td>linux中的“~”、“&#x2F;”、“.&#x2F;”</td>
<td>~” ：表示主目录，也就是当前登录用户的用户目录。“&#x2F;” ：是指根目录：就是所有目录最顶层的目录。“.&#x2F;” ：表示当前目录。“..” ：表示上级目录</td>
</tr>
<tr>
<td>nvcc -V</td>
<td>查看当前CUDA的版本，即实际安装的CUDA版本</td>
</tr>
<tr>
<td>nvidia-smi</td>
<td>不仅可以查看当前NVIDIA驱动的版本，还可以查询与此驱动相匹配的CUDA版本，虽是匹配，但是CUDA的版本可以略低于此时驱动匹配的CUDA版本，因此，我们可以安装版本高一点的驱动，来兼容不同版本的CUDA！</td>
</tr>
<tr>
<td>NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running</td>
<td>内核自动升级导致的新内核无法启动驱动, 最终重装显卡驱动, cuda, cudnn解决，参考如下链接：<a target="_blank" rel="noopener" href="https://qii404.me/2021/07/03/ubuntu-install-nvidia-driver.html">https://qii404.me/2021/07/03/ubuntu-install-nvidia-driver.html</a></td>
</tr>
<tr>
<td>linux 命令 wq</td>
<td>write and quit</td>
</tr>
<tr>
<td>linux 命令 wq!</td>
<td>forcely write and quit</td>
</tr>
<tr>
<td>linux 命令 x</td>
<td>storage and quit</td>
</tr>
<tr>
<td>linux netcat</td>
<td>查看端口 22 是否在主机 192.168.56.10 上打开 –&gt; nc -zv 192.168.1.15 22</td>
</tr>
<tr>
<td>linux 解压缩zip文件</td>
<td>unzip…</td>
</tr>
<tr>
<td>linux下路径名中含空格如何处理</td>
<td>1.使用转义字符“\” 2.将路径名加双引号”” 或 单引号‘’</td>
</tr>
<tr>
<td>linux sudo: command not found</td>
<td>apt-get install sudo</td>
</tr>
<tr>
<td>linux 命令 echo</td>
<td>1.打印输出 echo -e “hello\tworld” 2.覆盖echo Hello World &gt; log.txt3.追加 echo Hello World &gt;&gt; log.txt</td>
</tr>
<tr>
<td>解决每次打开终端都需要source .bashrc问题</td>
<td>登录Linux时，执行顺序可以总结为：&#x2F;etc&#x2F;profile→ ( ~&#x2F;.bash_profile | ~&#x2F;.bash_login | ~&#x2F;.profile)→ ~&#x2F;.bashrc →&#x2F;etc&#x2F;bashrc 故只需在&#x2F;etc&#x2F;profile 或 ~&#x2F;.bash_profile 文件中添加：</td>
</tr>
</tbody></table>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># ~/.bash_profile</span></span><br><span class="line"><span class="keyword">if</span> [ -f ~/.bashrc ]; <span class="keyword">then</span></span><br><span class="line">    . ~/.bashrc                </span><br><span class="line"><span class="keyword">fi</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># /etc/profile</span></span><br><span class="line"> <span class="keyword">if</span> [ -f /etc/.bashrc ]; <span class="keyword">then</span> </span><br><span class="line">     . /etc/.bashrc</span><br><span class="line"> <span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<hr>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Method</th>
</tr>
</thead>
<tbody><tr>
<td>args 是 arguments 的缩写</td>
<td><em>args就是就是传递一个可变参数列表给函数实参，</em>args 必须放在 **kwargs 的前面</td>
</tr>
<tr>
<td>kwargs 是 keyword arguments 的缩写</td>
<td>**kwargs则是将一个可变的关键字参数的字典传给函数实参</td>
</tr>
<tr>
<td>Python startswith()</td>
<td>检查字符串是否是以指定子字符串开头</td>
</tr>
<tr>
<td>Python os.path.join()</td>
<td>用于路径拼接，可以传入多个路径</td>
</tr>
<tr>
<td>Python tuple</td>
<td>元组：有序且不可更改的集合，tup&#x3D;(1,2,3,4)</td>
</tr>
<tr>
<td>Python list</td>
<td>列表：有序，list&#x3D;[1,2,3,4]</td>
</tr>
<tr>
<td>Python dictionary</td>
<td>字典：无序，dic&#x3D;{‘a’:12,‘b’:34}</td>
</tr>
<tr>
<td>Python set()</td>
<td>创建一个无序不重复元素集</td>
</tr>
<tr>
<td>Python lambda</td>
<td>lambda 函数是一种小的匿名函数。lambda 函数可接受任意数量的参数，但只能有一个表达式。语法 lambda arguments : expression</td>
</tr>
<tr>
<td>Python pdb</td>
<td>pdb是ptyhon内置的一个调试库</td>
</tr>
<tr>
<td>python print 格式符%</td>
<td>print (“His name is %s”%(“Aviad”))；print (“He is %d years old”%(25))；print (“His height is %.2f m”%(1.83234)</td>
</tr>
<tr>
<td>python print 格式化字符串f</td>
<td>print(f’{A}的类型为{type(A)}’)</td>
</tr>
<tr>
<td>python print 转义字符 \</td>
<td>\n表示换行，\t表示制表符，\r表示回车，\f表示换页</td>
</tr>
<tr>
<td>python print print+format组合</td>
<td>print(“{1} {0} {1}”.format(“hello”, “world”) ) # 设置指定位置,输出为’world hello world’</td>
</tr>
<tr>
<td>Python3 assert</td>
<td>语法格式：assert expression 等价于if not expression:    raise AssertionError</td>
</tr>
<tr>
<td>Python logging库</td>
<td>常用的记录日志库</td>
</tr>
<tr>
<td>Python Try…Except</td>
<td>Debug</td>
</tr>
<tr>
<td>Python a is b</td>
<td>a is b , 这是一个同一性运算符。用于比较两个对象的物理id。如果相同则返回True否则返回False.Python为了优化效率,内置了小整数对象池和简单字符串对象池。小整数对象池包括[-5, 256]。两变量如a&#x3D;2 b&#x3D;2,a is b –&gt;return TRUE</td>
</tr>
<tr>
<td>Python a &#x3D;&#x3D; b</td>
<td>a &#x3D;&#x3D; b , 这是一个比较运算符,用于比较两个对象的value(值)是否相同,相同则返回True 否则返回False</td>
</tr>
<tr>
<td>python super().<strong>init</strong>()</td>
<td>super().<strong>init</strong>() 就是调用父类的init方法， 同样可以使用super()去调用父类的其他方法。</td>
</tr>
<tr>
<td>torch.arange(start,end)</td>
<td>produces values in [start, end)</td>
</tr>
<tr>
<td>Python sum(iterable,start)</td>
<td>sum&#x3D;iterable的和+start的值</td>
</tr>
<tr>
<td>Python “<strong>name</strong>“</td>
<td>__name__是python的一个内置类属性，它存储模块的名称。python的模块既可以被调用，也可以独立运行。而被调用时__name__存储的是py文件名(模块名称)，独立运行时存储的是”<strong>main</strong>“。</td>
</tr>
<tr>
<td>Python list(set(a))</td>
<td>set(a)将列表a转换为集合，集合是一个包含不重复元素的无序序列，然后再使用list将集合转换为列表</td>
</tr>
<tr>
<td>Python apply()</td>
<td>apply(func,*args,**kwargs)</td>
</tr>
<tr>
<td>Python *</td>
<td>*用在tuple变量之前作为函数参数，可将tuple&#x2F;list 转化为多个参数传入函数</td>
</tr>
<tr>
<td>Python **</td>
<td>调用函数时，**用在dict变量之前作为函数参数，可将dict转化为多个关键字参数传入函数</td>
</tr>
<tr>
<td>Python 单下划线开头</td>
<td>半私有变量</td>
</tr>
<tr>
<td>Python 双下划线开头</td>
<td>私有变量</td>
</tr>
<tr>
<td>Python 双下划线开头、双下划线结尾</td>
<td>Python内置属性名或者魔法方法名。是Python自己实现的属性和方法，一般不允许自定义类似此种命名方式的属性或者方法。</td>
</tr>
<tr>
<td>Python copy()</td>
<td>不管多么复杂的数据结构，浅拷贝都只会copy一层。如列表是三层表示的，类似c中指针的指针</td>
</tr>
<tr>
<td>Python deepcopy()</td>
<td>将整个变量内存全部复制一遍，新变量与原变量没有任何关系。</td>
</tr>
<tr>
<td>Python import…</td>
<td>导入一个模块，使用：模块.函数</td>
</tr>
<tr>
<td>Python from…import…</td>
<td>导入了一个模块中的一个函数，使用：直接使用函数名使用就可以了</td>
</tr>
<tr>
<td>Python @staticmethod</td>
<td>静态方法。不传入代表实例对象的self参数，并且不强制要求传递任何参数，可以被类直接调用。静态方法是独立于类的一个单独函数，只是寄存在一个类名下。静态方法就是类对外部函数的封装，有助于优化代码结构和提高程序的可读性。</td>
</tr>
<tr>
<td>Python @classmethod</td>
<td>类方法。不传入self示例本身，而是传入cls，代表这个类自身，可以来调用类的属性，类的方法，实例化对象等。类方法是将类本身作为操作对象。当我们需要和类直接进行交互，而不需要和实例进行交互时，自然也就不需要传入实例本身</td>
</tr>
<tr>
<td>Python @abstractmethod</td>
<td>抽象方法。用于程序接口的控制。含有abstractmethod 方法的类不能实例化，继承了含abstractmethod方法的子类必须复写所有abstractmethod装饰的方法，未被装饰的不重写</td>
</tr>
<tr>
<td>Python @property</td>
<td>将一个方法伪装成属性。被修饰的特性方法，内部可以实现处理逻辑，但对外提供统一的调用方式（访问方式很友好）</td>
</tr>
<tr>
<td>Python class</td>
<td>类： 采用 Class 作为关键字进行定义的代码块，表示的是一种类别</td>
</tr>
<tr>
<td>Python object</td>
<td>对象： 实例化之后的类，对类中的形参进行了赋值，赋予其真正的含义或数值</td>
</tr>
<tr>
<td>Python method</td>
<td>方法： 使用 def 作为关键词，定义在类内的函数</td>
</tr>
<tr>
<td>Python function</td>
<td>函数： 使用 def 作为关键词，但是没有在类内进行定义，即 定义在类外</td>
</tr>
<tr>
<td>Python attribute</td>
<td>属性： 类内的称呼，其实就是类内的变量，同一个类内的不同方法内的变量都是这个类的属性，也就是这个类的变量</td>
</tr>
<tr>
<td>Python None</td>
<td>与C不同，在python中是没有NULL，但存在相近意义的None。None表示空值，它是一个特殊 Python 对象, None的类型是NoneType</td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Method</th>
</tr>
</thead>
<tbody><tr>
<td>torch.nn.Parameter()</td>
<td>torch.nn.Parameter是继承自torch.Tensor的子类，其主要作用是作为nn.Module中的可训练参数使用。它与torch.Tensor的区别就是nn.Parameter会自动被认为是module的可训练参数，即加入到parameter()这个迭代器中去；而module中非nn.Parameter()的普通tensor是不在parameter中的。注意到，nn.Parameter的对象的requires_grad属性的默认值是True，即是可被训练的，这与torth.Tensor对象的默认值相反。</td>
</tr>
<tr>
<td>Pytorch .item()</td>
<td>.item()用于在只包含一个元素的tensor中提取值，注意是只包含一个元素，否则的话使用.tolist()</td>
</tr>
<tr>
<td>Pytorch model.train()</td>
<td>启用 Batch Normalization 和 Dropout。</td>
</tr>
<tr>
<td>Pytorch model.eval()</td>
<td>不启用 Batch Normalization 和 Dropout。eval模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播（back probagation)。</td>
</tr>
<tr>
<td>Pytorch torch.no_grad()</td>
<td>with torch.no_grad()则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用。它的作用是将该with语句包裹起来的部分停止梯度的更新，从而节省了GPU算力和显存，但是并不会影响dropout和BN层的行为。</td>
</tr>
<tr>
<td>Pytorch torch.max(input,dim)</td>
<td>dim：input每个元素参与比较的维度</td>
</tr>
<tr>
<td>Pytorch torch.tensor()</td>
<td>Constructs a tensor with no autograd history (also known as a “leaf tensor”, see Autograd mechanics) by copying data.</td>
</tr>
<tr>
<td>Pytorch torch.Tensor()</td>
<td>A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.</td>
</tr>
<tr>
<td>Pytorch torch.autograd.Variable()</td>
<td>Autograd的核心类，浅封装（thin wrapper）了Tensor，用于整合实现反向传播。torch0.4后张量与自动微分变量整合，tensor直接当作自动微分变量使用，旦Variable仍可使用</td>
</tr>
<tr>
<td>Pytorch 自定义autograd中的Function</td>
<td>自定义pytorch中动态图的算子(operator)，也就是动态图的“边”，需要继承torch.autograd.Function类，并实现forward与backward方法。在使用自定义的算子时，需要使用apply方法。</td>
</tr>
<tr>
<td>Pytorch torch.save(net,path)</td>
<td>保存模型,模型&#x3D;网络结构+网络参数</td>
</tr>
<tr>
<td>Pytorch torch.save(net.state_dict(),path)</td>
<td>保存网络参数</td>
</tr>
<tr>
<td>Pytorch中什么时候调用forward()函数</td>
<td>Module类是nn模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。Module中定义了__call__()函数，该函数调用了forward()函数，前向传播时会自动调用__call__()函数亦即自动调用forward()</td>
</tr>
<tr>
<td>Pytorch nn.Sequential()</td>
<td>把定义的conv fc relu等层包装起来作为一个整体</td>
</tr>
<tr>
<td>Pytorch torch.squeeze()</td>
<td>torch.squeeze(input, dim&#x3D;None, *, out&#x3D;None) → Tensor 对输入的张量进行处理，如果张量维度里面有大小为1 的部分，那我们就移除，否则保留.dim可以指定特定的某一维度判断是否为1并进行压缩，若不指定则对input_tensor所有为1的维度进行压缩</td>
</tr>
<tr>
<td>Pytorch import torch import torch.nn as nn</td>
<td>起到缩写效果： 如果只用import torch，就要用torch.nn.Conv2d这样的代码。如果写成import torch.nn as nn，后面就可以简写成nn.Con2d。两种写法效果都一样，用import …as…  只是起了个别名写代码时可以更精炼。</td>
</tr>
<tr>
<td>Pytorch torch.cuda.is_available()</td>
<td>查看是否有可用GPU</td>
</tr>
<tr>
<td>Pytorch torch.cuda.device_count()</td>
<td>查看GPU数量</td>
</tr>
<tr>
<td>Pytorch torch.cuda.current_device()</td>
<td>查看当前使用的cuda编号</td>
</tr>
<tr>
<td>Pytorch torch.cuda.get_device_capability(device)</td>
<td>查看指定GPU容量</td>
</tr>
<tr>
<td>Pytorch torch.cuda.get_device_name(device)</td>
<td>查看指定GPU名称</td>
</tr>
<tr>
<td>Pytorch torch.cuda.manual_seed(seed)</td>
<td>设置随机种子</td>
</tr>
<tr>
<td>Pytorch register_buffer()</td>
<td>model中需要设置一些不更新的参数,同时希望通过model.state_dict()将参数保存下来，就用到register_buffer(),buffer也可以通过requires_grad获取其梯度信息，但是optimizer进行更新的是parameter,buffer不会更新</td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Method</th>
</tr>
</thead>
<tbody><tr>
<td>深度学习之embedding层</td>
<td>通过矩阵乘法实现降维，信息不变，按照某种映射关系将原本矩阵的信息转换到了一个新的维度的矩阵里面，节省存储空间。也可以逆向升维</td>
</tr>
<tr>
<td>ImageNet 数据集</td>
<td>ImageNet 是一个计算机视觉系统识别项目，是目前世界上最大的图像识别数据库。此项目由斯坦福大学李飞飞等教授于 2009 年发起.ImageNet 中目前共有 14,197,122 幅图像，总共分为 21,841 个类别（synsets），通常我们所说的 ImageNet 数据集其实是指 ISLVRC2012 比赛用的子数据集，其中 train 有 1,281,167 张照片和标签，共 1000 类，大概每类 1300 张图片，val 有 50,000 副图像，每类 50 个数据，test 有 100,000 副图片，每类 100 个数据。</td>
</tr>
<tr>
<td>海森矩阵</td>
<td>Hessian Matrix 二阶导数矩阵</td>
</tr>
<tr>
<td>Ubuntu中切换Python版本</td>
<td>1.列出可用的 Python 替代版本：update-alternatives –list python 2.列出的 Python 版本中选择进行切换：update-alternatives –config python</td>
</tr>
<tr>
<td>目标检测&#x2F;图像分割评价单张图片标准</td>
<td>IOU(Intersection-Over-Union)交并比</td>
</tr>
<tr>
<td>目标检测&#x2F;图像分割评价一套算法标准</td>
<td>在整个数据集测试结果 准确率（Pixel Accuracy）：检测出来物体占待检测总体（包含检测出和未检测出）的比例；精确度（Pixel Precision）：检测出来正确的物体占检测出物体总体的比例</td>
</tr>
<tr>
<td>LSTMs</td>
<td>Long Short Term Memory networks</td>
</tr>
<tr>
<td>参数量Params</td>
<td>input_feature_map:f&#x3D;(B,c1,H,W),conv_kernel:k * k,bias&#x3D;True且使用BN,即附加两个可学习参数alpha和beta, Params&#x3D;c1<em>c2</em>k<em>k+3</em>c2</td>
</tr>
<tr>
<td>参数量Params</td>
<td>fc 输入神经元数M,输出神经元数N,bias为True时params&#x3D;M*N+N</td>
</tr>
<tr>
<td>计算量FLOPs</td>
<td>乘加次数-&gt;输出的每个pixel的得到需要多少次乘加</td>
</tr>
</tbody></table>
<hr>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody><tr>
<td>APU（Accelerated Processing Unit，加速处理单元）</td>
<td>最早由AMD提出并生产制作的具有概念性的理念产品。加速芯片对数据图像的处理能力。</td>
</tr>
<tr>
<td>NPU（Neural-network Processing Unit，神经网络处理单元）</td>
<td>可以自行处理某些数据，将接受到的多元化的数据分担给其他单元处理</td>
</tr>
<tr>
<td>GPU（Graphics Processing Unit，图形处理单元）</td>
<td>专门处理图像数据，也能为CPU分担部分工作。</td>
</tr>
<tr>
<td>CPU（Central Processing Unit，中央处理单元）</td>
<td>系统的运算能力，电子产品的核心。负责处理指令和一切逻辑性数据。</td>
</tr>
</tbody></table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/09/21/CUDA-Tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/21/CUDA-Tutorial/" class="post-title-link" itemprop="url">CUDA Tutorial</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-21 16:38:13" itemprop="dateCreated datePublished" datetime="2022-09-21T16:38:13+08:00">2022-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:16:50" itemprop="dateModified" datetime="2024-12-02T15:16:50+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol>
<li>CPU体系架构概述</li>
<li>并行程序设计概述</li>
<li>CUDA开发环境搭建和工具配置</li>
<li>GPU体系架构概述</li>
<li>GPU编程模型</li>
<li>CUDA编程（1）</li>
<li>CUDA编程（2）</li>
<li>CUDA编程（3）</li>
<li>CUDA程序分析和调试工具</li>
<li>CUDA程序基本优化</li>
<li>CUDA程序深入优化</li>
<li>最新NVIDA GPU 和 CUDA特性</li>
</ol>
<h2 id="1-CPU体系概述"><a href="#1-CPU体系概述" class="headerlink" title="1.CPU体系概述"></a>1.CPU体系概述</h2><ol>
<li><p>桌面级应用以访存 分支操作 数据搬来搬去为主，数值计算占比很低</p>
</li>
<li><p>取指 译码 执行 访存，流水线 pipeline 指令级并行 减小时钟周期但是增加了延迟和芯片面积。带来的问题：</p>
<ul>
<li>具有依赖关系的指令 执行顺序</li>
<li>分支怎么处理</li>
<li>流水线长度<br> 旁路 Bypassing<br> 停滞 Stalls<br> 分支 Branches<br> 分支预测 Branches Prediction</li>
<li>+现代预测器准确度&gt;90%</li>
<li>-面积增加 延迟增加<br> 分支断定 Predication GPU中使用了分支断定</li>
</ul>
</li>
<li><p>提升IPC(instructions per cycle) 超标量(Superscalar)</p>
<ul>
<li>寄存器重命名(RegisterRenaming)</li>
<li>乱序执行(Out-of-Order Execution) 重排指令获得最大吞吐率</li>
<li>+IPC接近理想状态</li>
<li>-面积增加 功耗增加</li>
</ul>
</li>
<li><p>CPU内部的并行性</p>
<ul>
<li>指令级并行 Instruction-level Parallelism (ILP)<ul>
<li>超标量Superscalar</li>
<li>乱序执行Out-of-Order</li>
</ul>
</li>
<li>数据级并行 Data-level Parallelism (DLP)<ul>
<li>矢量计算Vectors</li>
</ul>
</li>
<li>线程级并行 Thread-level Parallelism (TLP)<ul>
<li>同步多线程 Simultaneous Mulitithreading (SMT)</li>
<li>多核 Multicore</li>
<li>锁、一致性和同一性 Locks,Coherence and Consisitency<ul>
<li>问题：多线程读写同一块数据  解决办法：加锁</li>
<li>问题：谁的数据是正确的？ Coherence 解决办法：缓存一致性协议</li>
<li>问题：什么样的数据是正确的？ Consistency 解决办法：存储器同一性模型</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>能量墙&#x2F;存储墙</p>
</li>
</ol>
<ul>
<li>结论</li>
</ul>
<ol>
<li>CPU为串行程序优化<ul>
<li>Piplines, Branch Prediction, Superscalar, Out-of-Order(OoO)</li>
<li>Reduce execution time with high clock speeds and high utilization</li>
</ul>
</li>
<li>缓慢的内存带宽(存储器带宽)将会是大问题</li>
<li>并行处理是方向</li>
</ol>
<h2 id="2-并行程序设计概述"><a href="#2-并行程序设计概述" class="headerlink" title="2.并行程序设计概述"></a>2.并行程序设计概述</h2><ul>
<li>概念和名词<ul>
<li>Flynn 矩阵<ul>
<li>SISD: Single Instruction, Single Data</li>
<li>SIMD: Single Instruction, Multiple Data</li>
<li>MISD: Multiple Instruction, Single Data</li>
<li>MIMD: Multiple Instruction, Multiple Data</li>
</ul>
</li>
<li>Task (任务)</li>
<li>Parallel Task (并行任务)</li>
<li>Serial Execution (串行执行)</li>
<li>Parallel Execution (并行执行)</li>
<li>Shared Memory (共享存储)</li>
<li>Distributed Memory (分布式存储)</li>
<li>Communications (通信)</li>
<li>Synchronization (同步)</li>
<li>Granularity (粒度)</li>
<li>Observed Speedup (加速比)</li>
<li>Parallel Overhead (并行开销)</li>
<li>Scalability (可扩展性)</li>
</ul>
</li>
<li>并行编程模型<ul>
<li>共享存储模型 Shared Memory Model</li>
<li>线程模型 Threads Model</li>
<li>消息传递模型 Message Passing Model</li>
<li>数据并行模型 Data Parallel Model</li>
</ul>
</li>
<li>设计并行处理程序和系统<ul>
<li>自动和手动并行</li>
<li>理解问题和程序</li>
<li>分块分割 数据分块，任务分割</li>
<li>通信 可扩展性重要影响因素</li>
<li>同步</li>
<li>数据依赖</li>
<li>负载均衡</li>
<li>粒度</li>
<li>I&#x2F;O</li>
<li>成本</li>
<li>性能分析和优化</li>
</ul>
</li>
<li>Amdahl’s Law<ul>
<li>程序可能的加速比取决于可以被并行化的部分,并行化的可扩展性有极限 取决于可并行部分的比例</li>
</ul>
</li>
</ul>
<h2 id="3-CUDA开发环境搭建"><a href="#3-CUDA开发环境搭建" class="headerlink" title="3.CUDA开发环境搭建"></a>3.CUDA开发环境搭建</h2><ul>
<li>windows cuda zone</li>
<li>linux</li>
</ul>
<h2 id="4-GPU体系架构概述"><a href="#4-GPU体系架构概述" class="headerlink" title="4.GPU体系架构概述"></a>4.GPU体系架构概述</h2><ul>
<li>为什么需要GPU(Graphic Processing Unit)<ul>
<li>GPU 是异构 众核 处理器，针对吞吐优化<ul>
<li>高效的GPU任务具备的条件<ul>
<li>具有成千上万的独立工作<ul>
<li>尽量利用大量的ALU单元</li>
<li>大量的片元切换掩藏延迟</li>
</ul>
</li>
<li>可以共享指令流<ul>
<li>适用于SIMD处理</li>
</ul>
</li>
<li>最好是计算密集的任务<ul>
<li>通信和计算开销比例合适</li>
<li>不要受制于访存带宽</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>三种方法提升GPU的处理速度<ul>
<li>1.Use many “slimmed down cores” to run in parallel</li>
<li>2.Pack cores full of ALUs(by sharing instruction stream across groups of fragments)<ul>
<li>Option 1: Explicit SIMD vector instructions</li>
<li>Option 2: Implicit sharing managed by hardware</li>
</ul>
</li>
<li>3.Avoid latency stalls by interleaving execution of many groups of fragments</li>
</ul>
</li>
<li>实际GPU设计举例<ul>
<li>NVIDIA GTX 480：Fermi</li>
<li>NVIDIA GTX 680: Kepler</li>
</ul>
</li>
<li>GPU的存储器设计</li>
</ul>
<h2 id="5-GPU编程模型"><a href="#5-GPU编程模型" class="headerlink" title="5.GPU编程模型"></a>5.GPU编程模型</h2><ul>
<li><p>内容</p>
<ul>
<li>CPU和GPU互动模式</li>
<li>GPU线程组织模型（不停强化）</li>
<li>GPU存储模型</li>
<li>基本的编程问题</li>
</ul>
</li>
<li><p>CPU-GPU交互</p>
<ul>
<li>各自的物理内存空间</li>
<li>通过PCIE总线互连（8GB&#x2F;s~16GB&#x2F;s）</li>
<li>交互开销较大</li>
</ul>
</li>
<li><p>线程组织架构说明</p>
<ul>
<li>一个kernel具有大量线程</li>
<li>线程被划分成线程块’Blocks’<ul>
<li>一个block内部的线程可以共享’Shared Memory’</li>
<li>可以同步 ‘_syhcthreads()’</li>
</ul>
</li>
<li>kernel启动一个’grid’,包含若干线程块<ul>
<li>用户设定</li>
</ul>
</li>
<li>线程和线程块具有唯一标识</li>
</ul>
</li>
<li><p>编程模型</p>
<ul>
<li>常规意义的GPU用于处理图形图像</li>
<li>操作用于像素，每个像素的操作都类似</li>
<li>可以应用SIMD(single instruction multiple data)</li>
<li>Single Instruction Multiple Thread(SIMT)<ul>
<li>GPU版本的SIMD</li>
<li>大量线程模型获得高度并行</li>
<li>线程切换获得延迟掩藏</li>
<li>多个线程执行相同指令流</li>
<li>GPU上大量线程承载和调度</li>
</ul>
</li>
</ul>
</li>
<li><p>CUDA编程模式： Extended C</p>
<ul>
<li>Declspecs (Dclaration Specifier) 声明规范<ul>
<li>global, device, shared, local, constant</li>
</ul>
</li>
<li>关键词<ul>
<li>threadIdx, blockIdx</li>
</ul>
</li>
<li>Intrinsics<ul>
<li>__syncthreads</li>
</ul>
</li>
<li>运行期API<ul>
<li>Memory, symbol, execution, management</li>
</ul>
</li>
<li>函数调用</li>
</ul>
</li>
</ul>
<h2 id="6-CUDA编程-1"><a href="#6-CUDA编程-1" class="headerlink" title="6.CUDA编程(1)"></a>6.CUDA编程(1)</h2><ul>
<li><p>GPU架构概览</p>
<ul>
<li>GPU特别适用于<ul>
<li>密集计算，高度可并行计算</li>
<li>图形学</li>
</ul>
</li>
<li>晶体管主要用于：<ul>
<li>执行计算</li>
<li>而不是缓存数据，控制指令流</li>
</ul>
</li>
</ul>
</li>
<li><p>GPU计算的历史</p>
<ul>
<li>2001&#x2F;2002 研究人员把GPU当作数据并行协处理器<ul>
<li>GPGPU这个新领域从此诞生</li>
</ul>
</li>
<li>2007 NVIDIA发布CUDA<ul>
<li>CUDA 全称Compute Uniform Device Architecture 统一计算设备架构</li>
<li>GPGPU 发展成GPU Computing</li>
</ul>
</li>
<li>2008 Khronos 发布 OpenCL 规范</li>
</ul>
</li>
<li><p>CUDA的一些信息</p>
<ul>
<li>层次化线程集合 A hierarchy of thread groups</li>
<li>共享存储 Shared memories</li>
<li>同步 Barrier synchronization</li>
</ul>
</li>
<li><p>CUDA术语</p>
<ul>
<li>Host - 即主机端 通常指CPU<ul>
<li>采用ANSI标准C语言编程</li>
</ul>
</li>
<li>Device - 即设备端 通常指GPU(数据可并行)<ul>
<li>采用ANSI标准C的扩展语言编程</li>
</ul>
</li>
<li>Host和Device 拥有各自的存储器</li>
<li>CUDA编程<ul>
<li>包括主机端和设备端两部分代码</li>
</ul>
</li>
<li>Kernel - 数据并行处理函数，类似于OpenCL的shader</li>
<li>通过调用kernel函数在设备端创建轻量级线程<ul>
<li>线程由硬件负责创建并调度</li>
</ul>
</li>
<li>CUDA核函数(kernels)<ul>
<li>在N个不同的CUDA线程上并行执行</li>
</ul>
</li>
<li>线程层次 Thread Hierarchies<ul>
<li>Grid - 一维或多维线程块(block)<ul>
<li>一维或二维</li>
</ul>
</li>
</ul>
</li>
<li>Block - 一组线程<ul>
<li>一维，二维或三维<ul>
<li>例如索引数组，矩阵，体</li>
</ul>
</li>
</ul>
</li>
<li>一个Grid内每个Block的线程数是一样的</li>
<li>block内部的每个线程可以<ul>
<li>同步 synchronize</li>
<li>访问共享存储器 shared memory</li>
</ul>
</li>
<li>线程块之间彼此独立执行<ul>
<li>任意顺序：并行或串行</li>
<li>被任意数量的处理器以任意顺序调度</li>
<li>处理器的数量具有可扩展性</li>
</ul>
</li>
<li>Host 可以从device往返传输数据<ul>
<li>global memory全局存储器<ul>
<li>cudaMalloc() 在设备端分配global memory</li>
<li>cudaFree() 释放存储空间</li>
<li>cudaMemcpy() 内存传输<ul>
<li>Host to host</li>
<li>Host to device cudaMemcpyHostToDevice</li>
<li>Device to host cudaMemcpyDeviceToHost</li>
<li>Device to device</li>
</ul>
</li>
</ul>
</li>
<li>Constant memory常量存储器</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="7-CUDA编程-2"><a href="#7-CUDA编程-2" class="headerlink" title="7.CUDA编程(2)"></a>7.CUDA编程(2)</h2><ul>
<li>目录<ul>
<li><p>内置类型和函数 Built-ins and functions</p>
<ul>
<li>函数的声明<ul>
<li><strong>global</strong> void KernelFunc(),返回值必须是void. Executed on the:device Only callable from the:host</li>
<li><strong>device</strong> float DeviceFunc(),曾经默认内联，现在有些变化. Executed on the:device Only callable from the:device</li>
<li><strong>host</strong> float HostFunc() Executed on the:host Only callable from the:host</li>
</ul>
</li>
<li>Global和device函数<ul>
<li>尽量少用递归（不鼓励）</li>
<li>不要用静态变量</li>
<li>少用malloc（现在允许但不鼓励）</li>
<li>小心通过指针实现的函数调用</li>
</ul>
</li>
<li>向量数据类型<ul>
<li>type name<ul>
<li>char[1-4], uchar[1-4]</li>
<li>short[1-4], ushort[1-4]</li>
<li>int[1-4], uint[1-4]</li>
<li>long[1-4], ulong[1-4]</li>
<li>longlong[1-4], ulonglong[1-4]</li>
<li>float[1-4]</li>
<li>double1, double2</li>
</ul>
</li>
<li>同时适用于host 和 device 代码<ul>
<li>通过函数make_&lt;type name&gt;构造</li>
<li>通过.x, .y, .z, .w 访问</li>
</ul>
</li>
</ul>
</li>
<li>数学函数<ul>
<li>Intrinsic function 内建函数<ul>
<li>仅面向 Device设备端</li>
<li>更快但精度降低</li>
<li>以__为前缀，例如：__exp, __log,__pow,…</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>线程同步 Synchronizing threads</p>
<ul>
<li>块内线程可以同步<ul>
<li>调用__syncthreads 创建一个barrier栅栏</li>
<li>每个线程在调用点等待块内所有线程执行到这个地方，然后所有线程继续执行后续指令</li>
</ul>
</li>
<li>要求线程执行时间尽量接近 -&gt; 防止块内大部分 线程等待时间超长，降低效率</li>
<li>为什么只在一个块内同步 -&gt; 全局同步开销大</li>
<li>__syncthreads()会导致暂停 死锁</li>
</ul>
</li>
<li><p>线程调度 Scheduling threads</p>
<ul>
<li>术语 Streaming Processor(SP) Streaming Multi-Processor(SM)</li>
<li>G80架构<ul>
<li>16个SMs</li>
<li>每个含8个SPs,总共128个SPs</li>
<li>每个SM驻扎多达768个线程</li>
<li>总共同时执行12,288个线程</li>
</ul>
</li>
<li>GT200架构<ul>
<li>30个SMs</li>
<li>每个含8个SPs,总共含240个SPs</li>
<li>每个SM驻扎多达8个block,或1024个线程</li>
<li>同时执行，多达240个block，或30,720个线程</li>
</ul>
</li>
<li>Warp -块内的一组线程<ul>
<li>G80&#x2F;GT200 -32个线程</li>
<li>运行于同一个SM</li>
<li>线程调度的基本单位</li>
<li>threadIdx值连续</li>
<li>一个实现细节 -理论上从硬件上保证每个warp内的线程执行到相同位置</li>
</ul>
</li>
<li>SM implements zero-overhead warp scheduling<ul>
<li>At any time,only one of the warps is executed by SM</li>
<li>Warps whose next instruction has its operands ready for consumption are eligible for execution</li>
<li>All threads in a warp execute the same instruction when selected</li>
</ul>
</li>
</ul>
</li>
<li><p>存储模型 Memory model</p>
<ul>
<li><p>Device code can:</p>
</li>
<li><p>R&#x2F;W per-thread register</p>
</li>
<li><p>R&#x2F;W per-thread local memory</p>
</li>
<li><p>R&#x2F;W per-block shared memory</p>
</li>
<li><p>R&#x2F;W per-grid global memory</p>
</li>
<li><p>Read Only per-grid constant memory</p>
</li>
<li><p>Host code can</p>
</li>
<li><p>R&#x2F;W per-grid global and constant memory</p>
</li>
<li><p>寄存器Registers</p>
<ul>
<li>每个线程专用</li>
<li>快速，片上，可读写</li>
</ul>
</li>
<li><p>局部存储器Local Memory</p>
<ul>
<li>存储于global memory 作用域是每个线程</li>
<li>用于存储自动变量数组 通过常量索引访问</li>
</ul>
</li>
<li><p>共享存储器Shared Memory</p>
<ul>
<li>每个块</li>
<li>快速，片上，可读写</li>
<li>全速随机访问</li>
</ul>
</li>
<li><p>全局存储器Global Memory</p>
<ul>
<li>长延时（100个周期）</li>
<li>片外，可读写</li>
<li>随机访问影响性能</li>
<li>Host主机端可读写</li>
</ul>
</li>
<li><p>常量存储器Constant Memory</p>
<ul>
<li>短延时，高带宽，当所有线程访问同一位置时只读</li>
<li>存储区global memory 但是有缓存</li>
<li>Host主机端可读写</li>
</ul>
</li>
<li><table>
<thead>
<tr>
<th align="left">变量声明</th>
<th align="left">存储器</th>
<th align="left">作用域</th>
<th align="left">生命期</th>
</tr>
</thead>
<tbody><tr>
<td align="left">必须是单独的自动变量而不能是数组</td>
<td align="left">register</td>
<td align="left">thread</td>
<td align="left">kernel</td>
</tr>
<tr>
<td align="left">自动变量数组</td>
<td align="left">local</td>
<td align="left">thread</td>
<td align="left">kernel</td>
</tr>
<tr>
<td align="left">__shared__int sharedVar;</td>
<td align="left">shared</td>
<td align="left">block</td>
<td align="left">kernel</td>
</tr>
<tr>
<td align="left">__device__int globalVar;</td>
<td align="left">global</td>
<td align="left">grid</td>
<td align="left">application</td>
</tr>
<tr>
<td align="left">__constant__int constantVar;</td>
<td align="left">constant</td>
<td align="left">grid</td>
<td align="left">application</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>重访 Matrix multiply</p>
</li>
<li><p>原子函数 Atomic functions</p>
</li>
</ul>
</li>
</ul>
<h2 id="8-CUDA编程-3"><a href="#8-CUDA编程-3" class="headerlink" title="8.CUDA编程(3)"></a>8.CUDA编程(3)</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/09/07/Model-Compression-Overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/07/Model-Compression-Overview/" class="post-title-link" itemprop="url">Model Compression Overview</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-07 10:39:59" itemprop="dateCreated datePublished" datetime="2022-09-07T10:39:59+08:00">2022-09-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 12:31:38" itemprop="dateModified" datetime="2024-12-02T12:31:38+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><h3 id="1-Designing-efficient-NN-model-architectures"><a href="#1-Designing-efficient-NN-model-architectures" class="headerlink" title="1.Designing efficient NN model architectures"></a>1.Designing efficient NN model architectures</h3><p>present situation  </p>
<ol>
<li>手动优化微观结构如内核类型(深度卷积或低秩分解)</li>
<li>手动优化宏观结构如模块(residual、inception)</li>
<li>自动优化如Automated machine learning (AutoML) and Neural Architecture Search (NAS)</li>
</ol>
<h3 id="2-Co-designing-NN-architecture-and-hardware-together"><a href="#2-Co-designing-NN-architecture-and-hardware-together" class="headerlink" title="2.Co-designing NN architecture and hardware together"></a>2.Co-designing NN architecture and hardware together</h3><p>硬件与nn结构共同设计或者针对不同的硬件平台调整神经网络架构。主要mootivation是nn不同组件的开销是依赖于硬件的。</p>
<h3 id="3-Pruning"><a href="#3-Pruning" class="headerlink" title="3.Pruning"></a>3.Pruning</h3><ol>
<li><p>unstructured pruning  </p>
<ul>
<li>motivation: removes neurons with with small sensitivity, wherever they occur  </li>
<li>positive: little impact on the generalization performance</li>
<li>negative: leads to sparse matrix operations, which are known to be hard to accelerate, and which are typically memory-bound</li>
</ul>
</li>
<li><p>structured pruning  </p>
<ul>
<li>motivation: a group of parameters (e.g., entire convolutional filters) is removed.  </li>
<li>positive: still permitting dense matrix operations.  </li>
<li>negative: aggressive structured pruning often leads to significant accuracy degradation.</li>
</ul>
</li>
</ol>
<h3 id="4-Knowledge-distillation"><a href="#4-Knowledge-distillation" class="headerlink" title="4.Knowledge distillation"></a>4.Knowledge distillation</h3><ul>
<li>motivation: training a large model and then using it as a teacher to train a more compact model.  </li>
<li>positive:  mix knowledge distillation with prior method(i.e.也就是quantization and pruning ) has succeed</li>
<li>negative:  a major challenge here is to achieve a high compression ratio with distillation alone.non-negligible accuracy degradation with aggressive compression.</li>
</ul>
<h3 id="5-Quantization"><a href="#5-Quantization" class="headerlink" title="5.Quantization"></a>5.Quantization</h3><ul>
<li>present situation: has shown great and consistent success in both training and inference.this survey focused on inference.</li>
<li>shortcoming: very difficult to go below half-precision without significant tuning, and most of the recent quantization research has focused on inference.</li>
</ul>
<h3 id="6-Similarity-of-Quantization-and-Neuroscience"><a href="#6-Similarity-of-Quantization-and-Neuroscience" class="headerlink" title="6.Similarity of Quantization and Neuroscience"></a>6.Similarity of Quantization and Neuroscience</h3><ul>
<li>motivation: work in neuroscience that suggests that the human brain stores information in a discrete&#x2F;quantized form, rather than in a continuous form.</li>
</ul>
<h2 id="II-GENERAL-HISTORY-OF-QUANTIZATION"><a href="#II-GENERAL-HISTORY-OF-QUANTIZATION" class="headerlink" title="II. GENERAL HISTORY OF QUANTIZATION"></a>II. GENERAL HISTORY OF QUANTIZATION</h2><h2 id="III-BASIC-CONCEPTS-OF-QUANTIZATION"><a href="#III-BASIC-CONCEPTS-OF-QUANTIZATION" class="headerlink" title="III. BASIC CONCEPTS OF QUANTIZATION"></a>III. BASIC CONCEPTS OF QUANTIZATION</h2><ul>
<li>Problem Setup and Notations  </li>
<li>Uniform Quantization  </li>
<li>Symmetric and Asymmetric Quantization</li>
<li>Range Calibration Algorithms: Static vs Dynamic Quantization  </li>
<li>Quantization Granularity</li>
<li>Non-Uniform Quantization</li>
<li>Fine-tuning Methods  <ul>
<li>Quantization-Aware Training</li>
<li>Stochastic Quantization</li>
</ul>
</li>
</ul>
<h2 id="IV-ADVANCED-CONCEPTS-QUANTIZATION-BELOW-8-BITS"><a href="#IV-ADVANCED-CONCEPTS-QUANTIZATION-BELOW-8-BITS" class="headerlink" title="IV. ADVANCED CONCEPTS: QUANTIZATION BELOW 8 BITS"></a>IV. ADVANCED CONCEPTS: QUANTIZATION BELOW 8 BITS</h2><ul>
<li>Simulated and Integer-only Quantization</li>
<li>Mixed-Precision Quantization</li>
<li>Hardware Aware Quantization</li>
<li>Distillation-Assisted Quantization</li>
<li>Extreme Quantization<ul>
<li>Quantization Error Minimization</li>
<li>Improved Loss function</li>
<li>Improved Training Method</li>
</ul>
</li>
<li>Vector Quantization</li>
</ul>
<h2 id="V-QUANTIZATION-AND-HARDWARE-PROCESSORS"><a href="#V-QUANTIZATION-AND-HARDWARE-PROCESSORS" class="headerlink" title="V. QUANTIZATION AND HARDWARE PROCESSORS"></a>V. QUANTIZATION AND HARDWARE PROCESSORS</h2><h2 id="VI-FUTURE-DIRECTIONS-FOR-RESEARCH-IN-QUANTIZATION"><a href="#VI-FUTURE-DIRECTIONS-FOR-RESEARCH-IN-QUANTIZATION" class="headerlink" title="VI. FUTURE DIRECTIONS FOR RESEARCH IN QUANTIZATION"></a>VI. FUTURE DIRECTIONS FOR RESEARCH IN QUANTIZATION</h2><ul>
<li>Quantization Software</li>
<li>Hardware and NN Architecture Co-Design</li>
<li>Coupled Compression Methods</li>
<li>Quantized Training</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/09/06/Tensor-Dimensions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/06/Tensor-Dimensions/" class="post-title-link" itemprop="url">Tensor Dimensions</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-06 19:10:30" itemprop="dateCreated datePublished" datetime="2022-09-06T19:10:30+08:00">2022-09-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 12:23:00" itemprop="dateModified" datetime="2024-12-02T12:23:00+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-number-of-tensor’s-dimension"><a href="#1-number-of-tensor’s-dimension" class="headerlink" title="1.number of tensor’s dimension"></a>1.number of tensor’s dimension</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python </span><br><span class="line">import torch </span><br><span class="line">t=torch.ones(1,2,3,4) <span class="comment">#创建dim=4,size=[1,2,3,4]的tensor</span></span><br><span class="line">t <span class="comment">#显示t</span></span><br><span class="line">t.size() <span class="comment">#显示t的size</span></span><br></pre></td></tr></table></figure>

<p>通过tensor t的显示可以看出，tensor的dim&#x3D;从外到遇到第一个元素的 ‘[‘ 个数</p>
<h2 id="2-index-of-tensor’s-dimension"><a href="#2-index-of-tensor’s-dimension" class="headerlink" title="2.index of tensor’s dimension"></a>2.index of tensor’s dimension</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t.flatten(start_dim=<span class="number">0</span>).size()</span><br><span class="line">t.flatten(start_dim=<span class="number">1</span>).size()</span><br><span class="line">t.flatten(start_dim=<span class="number">2</span>).size()</span><br><span class="line">t.flatten(start_dim=<span class="number">3</span>).size()</span><br><span class="line">t.flatten(start_dim=-<span class="number">1</span>).size()</span><br></pre></td></tr></table></figure>

<p>通过把t从各个dim展平判断出tensor t的dim下标依次是[0,1,2,3],另外dim 3&#x3D;&#x3D;-1。</p>
<h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><p>关于torch.flatten()此method介绍见：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten">https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/09/05/Week-Report/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/05/Week-Report/" class="post-title-link" itemprop="url">Week Report</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-05 13:29:19" itemprop="dateCreated datePublished" datetime="2022-09-05T13:29:19+08:00">2022-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 15:24:25" itemprop="dateModified" datetime="2024-12-02T15:24:25+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="进度汇报（2022-8-31-2022-9-4）"><a href="#进度汇报（2022-8-31-2022-9-4）" class="headerlink" title="进度汇报（2022.8.31-2022.9.4）"></a>进度汇报（2022.8.31-2022.9.4）</h2><ol>
<li>读zeroq量化代码并在实验室集群上调试，跑通了对resnet18的压缩</li>
<li>读赵随意师兄的BaselineIR的部分代码，并放在集群上进行了训练，<br>解决了数据集路径错误、训练中途闪退问题。</li>
</ol>
<h2 id="本周计划（2022-8-31-2022-9-4）"><a href="#本周计划（2022-8-31-2022-9-4）" class="headerlink" title="本周计划（2022.8.31-2022.9.4）"></a>本周计划（2022.8.31-2022.9.4）</h2><ol>
<li>阅读最新Quantization综述以及部分zero-shot quantization最新论文，整理思路。</li>
<li>解决zeroq量化代码中看不懂的部分</li>
</ol>
<hr>
<h2 id="进度汇报（2022-9-5-2022-9-11）"><a href="#进度汇报（2022-9-5-2022-9-11）" class="headerlink" title="进度汇报（2022.9.5-2022.9.11）"></a>进度汇报（2022.9.5-2022.9.11）</h2><ol>
<li>对照论文理解了zeroq代码，并对distill data部分的total_loss进行修改,实验证明在分类任务上原total_loss的部分成分对量化后的精度影响极小，top-1 accuracy 波动在0.01%左右。</li>
<li>阅读投稿JMLC一篇文章的手稿和相关论文，对疑问点进行评论。</li>
<li>阅读2021模型量化最新综述。</li>
</ol>
<h2 id="本周计划（2022-9-5-2022-9-11）"><a href="#本周计划（2022-9-5-2022-9-11）" class="headerlink" title="本周计划（2022.9.5-2022.9.11）"></a>本周计划（2022.9.5-2022.9.11）</h2><ol>
<li>floating point quantization: 阅读论文FP8 Quantization: The Power of the Exponent(2022.8)及相关论文，尝试代码实现。</li>
<li>fixed point quantization: 阅读论文Post training 4-bit quantization of convolutional networks for rapid-deployment(2019)</li>
</ol>
<hr>
<h2 id="进度汇报（2022-9-12-2022-9-18）"><a href="#进度汇报（2022-9-12-2022-9-18）" class="headerlink" title="进度汇报（2022.9.12-2022.9.18）"></a>进度汇报（2022.9.12-2022.9.18）</h2><ol>
<li>阅读论文FP8 Quantization: The Power of the Exponent(2022.8)，尝试代码实现</li>
<li>阅读QPyTorch: A Low-Precision Arithmetic Simulation Framework(2019.10),并阅读了框架部分源码</li>
<li>中翻英 teaching statement</li>
</ol>
<h2 id="本周计划（2022-9-12-2022-9-18）"><a href="#本周计划（2022-9-12-2022-9-18）" class="headerlink" title="本周计划（2022.9.12-2022.9.18）"></a>本周计划（2022.9.12-2022.9.18）</h2><ol>
<li>floating point quantization: 继续FP8 coding相关部分学习</li>
<li>fixed point quantization: 阅读论文Post training 4-bit quantization of convolutional networks for rapid-deployment(2019)</li>
</ol>
<hr>
<h2 id="进度汇报（2022-9-19-2022-9-25）"><a href="#进度汇报（2022-9-19-2022-9-25）" class="headerlink" title="进度汇报（2022.9.19-2022.9.25）"></a>进度汇报（2022.9.19-2022.9.25）</h2><ol>
<li>floating point quantization: FP8 coding -&gt; 进度缓慢,暂时放下</li>
<li>fixed point quantization:<ul>
<li>阅读论文Post training 4-bit quantization of convolutional networks for rapid-deployment(2019)，理解ACIQ</li>
<li>阅读论文SQUANT(2022)</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2022-9-19-2022-9-25）"><a href="#本周计划（2022-9-19-2022-9-25）" class="headerlink" title="本周计划（2022.9.19-2022.9.25）"></a>本周计划（2022.9.19-2022.9.25）</h2><ol>
<li>SQUANT 源码理解</li>
<li>审稿</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-9-26-2022-10-9）"><a href="#进度汇报-（2022-9-26-2022-10-9）" class="headerlink" title="进度汇报 （2022.9.26-2022.10.9）"></a>进度汇报 （2022.9.26-2022.10.9）</h2><ol>
<li>SQUANT 源码理解</li>
<li>审稿</li>
</ol>
<h2 id="本周计划（2022-9-26-2022-10-9）"><a href="#本周计划（2022-9-26-2022-10-9）" class="headerlink" title="本周计划（2022.9.26-2022.10.9）"></a>本周计划（2022.9.26-2022.10.9）</h2><ol>
<li>重温zeroq,和21年最新综述</li>
<li>重构SQuant代码,作为以后工作的baseline</li>
<li>审稿</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-10-10-2022-10-16）"><a href="#进度汇报-（2022-10-10-2022-10-16）" class="headerlink" title="进度汇报 （2022.10.10-2022.10.16）"></a>进度汇报 （2022.10.10-2022.10.16）</h2><ol>
<li>理清了SQuant代码中是如何控制tensor shape的转变来进行kernel-wise channel-wise不同粒度的量化,对代码中的杂乱、无效部分做了重构</li>
<li>审稿:EC0065699_O_基于改进YOLOX的公路路面裂缝检测网络</li>
</ol>
<h2 id="本周计划（2022-10-10-2022-10-16）"><a href="#本周计划（2022-10-10-2022-10-16）" class="headerlink" title="本周计划（2022.10.10-2022.10.16）"></a>本周计划（2022.10.10-2022.10.16）</h2><ol>
<li>重温zeroq和21年最新综述</li>
<li>QAT量化感知训练入门，在MNIST数据集上跑通一个demo</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-10-17-2022-10-23）"><a href="#进度汇报-（2022-10-17-2022-10-23）" class="headerlink" title="进度汇报 （2022.10.17-2022.10.23）"></a>进度汇报 （2022.10.17-2022.10.23）</h2><ol>
<li>回顾ZeroQ和21年最新综述,组会准备</li>
<li>特殊环境去雨项目:填写开题表格,做PPT</li>
<li>QAT量化感知训练入门,在MNIST数据集上的demo尚未跑通</li>
</ol>
<h2 id="本周计划（2022-10-17-2022-10-23）"><a href="#本周计划（2022-10-17-2022-10-23）" class="headerlink" title="本周计划（2022.10.17-2022.10.23）"></a>本周计划（2022.10.17-2022.10.23）</h2><ol>
<li>实验:将ZeroQ蒸馏出的数据用在SQuant中间层激励的剪切范围确定上,看效果</li>
<li>跑通MNIST数据集上的QAT量化demo</li>
<li>完成学科前沿作业;准备数理统计考试</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-10-24-2022-10-30）"><a href="#进度汇报-（2022-10-24-2022-10-30）" class="headerlink" title="进度汇报 （2022.10.24-2022.10.30）"></a>进度汇报 （2022.10.24-2022.10.30）</h2><ol>
<li><p>实验:将ZeroQ蒸馏出的数据用在SQuant中间层激励的剪切范围确定上,效果如下：</p>
<table>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="left">Model</th>
<th align="left">Dataset</th>
<th align="left">W-bit</th>
<th align="left">A-bit</th>
<th align="left">Top-1 Accuracy</th>
<th align="left">Top-5 Accuracy</th>
<th align="left">Activation Clip Range Setting</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Gaussian_data(μ&#x3D;0,σ&#x3D;1)</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">73.012%</td>
<td align="left">91.036%</td>
<td align="left">sigma &#x3D; 25</td>
</tr>
<tr>
<td align="left">Gaussian_data(μ&#x3D;0,σ&#x3D;1)</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">73.066%</td>
<td align="left">90.990%</td>
<td align="left">sigma &#x3D; 30(较sigma&#x3D;25增大了clip range)</td>
</tr>
<tr>
<td align="left">ZeroQ_Refined_Data</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">72.854%</td>
<td align="left">91.008%</td>
<td align="left">sigma &#x3D; 25</td>
</tr>
<tr>
<td align="left">ZeroQ_Refined_Data</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">67.308%</td>
<td align="left">87.524%</td>
<td align="left">sigma &#x3D; 0(clip range:[0, max])</td>
</tr>
</tbody></table>
</li>
<li><p>在MNIST数据集上跑通了的量化感知训练(QAT)的demo</p>
</li>
<li><p>阅读论文 Data-Free Quantization Through Weight Equalization and Bias Correction (2019)</p>
</li>
</ol>
<h2 id="本周计划（2022-10-24-2022-10-30）"><a href="#本周计划（2022-10-24-2022-10-30）" class="headerlink" title="本周计划（2022.10.24-2022.10.30）"></a>本周计划（2022.10.24-2022.10.30）</h2><ol>
<li>fix上周实验出现的中间层activation异常偏大的bug</li>
<li>实验评估：在原模型上统计clip range与叠加量化clip error和round error之后统计clip range, 二者对量化结果精度的影响</li>
<li>处理作业，准备考试</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-10-31-2022-11-06）"><a href="#进度汇报-（2022-10-31-2022-11-06）" class="headerlink" title="进度汇报 （2022.10.31-2022.11.06）"></a>进度汇报 （2022.10.31-2022.11.06）</h2><ol>
<li><p>实验:在原模型上统计clip range与叠加量化clip error和round error之后统计clip range,精度对比：</p>
<table>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="left">Model</th>
<th align="left">Dataset</th>
<th align="left">W-bit</th>
<th align="left">A-bit</th>
<th align="left">Top-1 Accuracy</th>
<th align="left">Top-5 Accuracy</th>
<th align="left">Activation Clip Range Setting</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Gaussian_data(μ&#x3D;0,σ&#x3D;1)+叠加量化error统计clip range</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">73.012%</td>
<td align="left">91.036%</td>
<td align="left">sigma &#x3D; 25</td>
</tr>
<tr>
<td align="left">Gaussian_data(μ&#x3D;0,σ&#x3D;1)+原模型统计clip range</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">72.394%</td>
<td align="left">90.656%</td>
<td align="left">sigma &#x3D; 25</td>
</tr>
<tr>
<td align="left">ZeroQ_refined_data+叠加量化error统计clip range</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">72.854%</td>
<td align="left">91.008%</td>
<td align="left">sigma &#x3D; 25</td>
</tr>
<tr>
<td align="left">ZeroQ_refined_data+原模型统计clip range</td>
<td align="left">Resnet18</td>
<td align="left">ImageNet</td>
<td align="left">8bit</td>
<td align="left">8bit</td>
<td align="left">72.836%</td>
<td align="left">90.948%</td>
<td align="left">sigma &#x3D; 25</td>
</tr>
</tbody></table>
<p>结论：在原模型上统计clip range相较于叠加了量化error再统计clip range,最终的精度出现轻微下降，符合猜测</p>
</li>
<li><p>解决了上周实验出现的中间层activation异常偏大的bug</p>
</li>
<li><p>初步改出了fp32模拟量化到fp8的代码,在resnet18上实验效果不理想</p>
</li>
</ol>
<h2 id="本周计划（2022-10-31-2022-11-06）"><a href="#本周计划（2022-10-31-2022-11-06）" class="headerlink" title="本周计划（2022.10.31-2022.11.06）"></a>本周计划（2022.10.31-2022.11.06）</h2><ol>
<li>结合相关论文,改进fp8模拟量化</li>
<li>组会准备;考试准备</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-11-07-2022-11-13）"><a href="#进度汇报-（2022-11-07-2022-11-13）" class="headerlink" title="进度汇报 （2022.11.07-2022.11.13）"></a>进度汇报 （2022.11.07-2022.11.13）</h2><ol>
<li>组会准备;处理交代的改稿工作</li>
<li>数理统计考试准备</li>
</ol>
<h2 id="本周计划（2022-11-07-2022-11-13）"><a href="#本周计划（2022-11-07-2022-11-13）" class="headerlink" title="本周计划（2022.11.07-2022.11.13）"></a>本周计划（2022.11.07-2022.11.13）</h2><ol>
<li>矩阵理论考试准备</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-11-14-2022-11-20）"><a href="#进度汇报-（2022-11-14-2022-11-20）" class="headerlink" title="进度汇报 （2022.11.14-2022.11.20）"></a>进度汇报 （2022.11.14-2022.11.20）</h2><ol>
<li>处理考试相关</li>
<li>整理之前解决问题的很多网页记录；学习docker</li>
</ol>
<h2 id="本周计划（2022-11-14-2022-11-20）"><a href="#本周计划（2022-11-14-2022-11-20）" class="headerlink" title="本周计划（2022.11.14-2022.11.20）"></a>本周计划（2022.11.14-2022.11.20）</h2><ol>
<li>阅读浮点量化相关论文，改进FP8量化</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-11-21-2022-11-27）"><a href="#进度汇报-（2022-11-21-2022-11-27）" class="headerlink" title="进度汇报 （2022.11.21-2022.11.27）"></a>进度汇报 （2022.11.21-2022.11.27）</h2><ol>
<li>FP8: 对于fixed FP8 formats即在sign:exponet:mantissa固定在1:4:3或1:5:2的情况下，如果像IEEE754 FP32一样固定尾数偏移，得不到论文中展示的接近或 优于IN8的表现，采用类似INT8的缩放策略亦未奏效。</li>
<li>审稿</li>
</ol>
<h2 id="本周计划（2022-11-21-2022-11-27）"><a href="#本周计划（2022-11-21-2022-11-27）" class="headerlink" title="本周计划（2022.11.21-2022.11.27）"></a>本周计划（2022.11.21-2022.11.27）</h2><ol>
<li>课程考试、作业处理</li>
<li>阅读论文:On-Device Training Under 256KB Memory</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-12-05-2022-12-11）"><a href="#进度汇报-（2022-12-05-2022-12-11）" class="headerlink" title="进度汇报 （2022.12.05-2022.12.11）"></a>进度汇报 （2022.12.05-2022.12.11）</h2><ol>
<li>调研了解模型剪枝和稀疏化</li>
<li>尝试在手机上部署NCNN mobilenetssd(demo)</li>
</ol>
<h2 id="本周计划（2022-12-05-2022-12-11）"><a href="#本周计划（2022-12-05-2022-12-11）" class="headerlink" title="本周计划（2022.12.05-2022.12.11）"></a>本周计划（2022.12.05-2022.12.11）</h2><ol>
<li>审稿</li>
<li>准备计算机体系结构考试</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-12-12-2022-12-18）"><a href="#进度汇报-（2022-12-12-2022-12-18）" class="headerlink" title="进度汇报 （2022.12.12-2022.12.18）"></a>进度汇报 （2022.12.12-2022.12.18）</h2><ol>
<li>ICASSP 审稿</li>
<li>了解NCNN,在手机上部署demo: mobilenetssd yolov7</li>
</ol>
<h2 id="本周计划（2022-12-12-2022-12-18）"><a href="#本周计划（2022-12-12-2022-12-18）" class="headerlink" title="本周计划（2022.12.12-2022.12.18）"></a>本周计划（2022.12.12-2022.12.18）</h2><ol>
<li>读论文：PD-Quant(2022.12)</li>
<li>调研了解Mixed Precision Quantization相关工作</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-12-19-2022-12-26）"><a href="#进度汇报-（2022-12-19-2022-12-26）" class="headerlink" title="进度汇报 （2022.12.19-2022.12.26）"></a>进度汇报 （2022.12.19-2022.12.26）</h2><ol>
<li>读论文：PD-Quant(2022.12)</li>
<li>调研了解Mixed Precision Quantization相关工作</li>
</ol>
<h2 id="本周计划（2022-12-19-2022-12-26）"><a href="#本周计划（2022-12-19-2022-12-26）" class="headerlink" title="本周计划（2022.12.19-2022.12.26）"></a>本周计划（2022.12.19-2022.12.26）</h2><ol>
<li>将PD-Quant引入的量化损失Metric与之前工作结合,尝试复现</li>
<li>继续调研Mixed Precision Quantization相关</li>
</ol>
<hr>
<h2 id="进度汇报-（2022-12-27-2023-1-1）"><a href="#进度汇报-（2022-12-27-2023-1-1）" class="headerlink" title="进度汇报 （2022.12.27-2023.1.1）"></a>进度汇报 （2022.12.27-2023.1.1）</h2><ol>
<li>在量化参数scaling factor和offset选取上借鉴PD-Quant引入当前层activation量化在最终预测结果引起的差异。不仅考虑当前层activation量化前后差异，还引入当前层的activation量化在后面若干层累积后引起的差异。</li>
<li>改稿</li>
</ol>
<h2 id="后期计划（2022-12-27-2023-1-1）"><a href="#后期计划（2022-12-27-2023-1-1）" class="headerlink" title="后期计划（2022.12.27-2023.1.1）"></a>后期计划（2022.12.27-2023.1.1）</h2><p>继续完善量化引入全局累积差异的代码，提升性能</p>
<hr>
<h2 id="进度汇报-（2023-2-8-2023-2-12）"><a href="#进度汇报-（2023-2-8-2023-2-12）" class="headerlink" title="进度汇报 （2023.2.8-2023.2.12）"></a>进度汇报 （2023.2.8-2023.2.12）</h2><ul>
<li>workshop (NTIRE 2023 Efficient Super-Resolution Challenge)</li>
</ul>
<ol>
<li><p>Efficient Super-Resolution Challenge(ESR):train rfdn baseline,test 结果如下</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val Time [ms]</th>
<th>Params [M]</th>
<th>FLOPs [G]</th>
<th>Acts [M]</th>
<th>Mem [M]</th>
<th>Conv</th>
</tr>
</thead>
<tbody><tr>
<td>trained_rfdn_best</td>
<td>DIV2K_val(801-900)</td>
<td>28.73</td>
<td>37.62</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_1</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>41.38</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_2</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>43.86</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
<tr>
<td>RFDN_baseline_3</td>
<td>DIV2K_val(801-900)</td>
<td>29.04</td>
<td>37.59</td>
<td>0.433</td>
<td>27.10</td>
<td>112.03</td>
<td>788.13</td>
<td>64</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="后期计划（2023-2-8-2023-2-12）"><a href="#后期计划（2023-2-8-2023-2-12）" class="headerlink" title="后期计划（2023.2.8-2023.2.12）"></a>后期计划（2023.2.8-2023.2.12）</h2><ol>
<li>rfdn基础上改进模型</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-2-13-2023-2-19）"><a href="#进度汇报-（2023-2-13-2023-2-19）" class="headerlink" title="进度汇报 （2023.2.13-2023.2.19）"></a>进度汇报 （2023.2.13-2023.2.19）</h2><ul>
<li>workshop (MAI 2023 Video Super Resolution)<ol>
<li>先train 2022官方仓库 MRRN baseline<ul>
<li><p>环境配置</p>
<ol>
<li><p>Python 3.8.10</p>
</li>
<li><p>Tensorflow 2.9.0</p>
<ul>
<li>查看tensorflow cuda cudnn python 版本对照表： <a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source_windows">https://www.tensorflow.org/install/source_windows</a></li>
</ul>
</li>
<li><p>Cuda 11.2</p>
</li>
<li><p>Cudnn v8.7.0</p>
<ul>
<li>官网：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></li>
<li>uname -m 查看cpu架构，cudnn有不同架构的版本 x86_64 PPC SBSA</li>
<li>tar -xvf解压缩后用以下命令安装并赋予所有用户读取权限</li>
</ul>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> path_to_cudnn/include/cudnn*    /usr/local/cuda/include</span><br><span class="line">sudo <span class="built_in">cp</span> path_to_cudnn/lib64/libcudnn*    /usr/local/cuda/lib64</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /usr/local/cuda/include/cudnn*   /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure>

<ul>
<li>Cudnn和Cuda 安装完需在&#x2F;etc&#x2F;profile配置环境变量PATH和LD_LIBRARY_PATH</li>
</ul>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda/lib64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/local/cuda/bin</span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=<span class="variable">$CUDA_HOME</span>:/usr/local/cuda</span><br></pre></td></tr></table></figure>

<ul>
<li>可将文件夹 &#x2F;usr&#x2F;local&#x2F;cuda-11.2 与 &#x2F;usr&#x2F;local&#x2F;cuda 软连接起来</li>
</ul>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /usr/local/cuda-11.2 /usr/local/cuda</span><br></pre></td></tr></table></figure>

<ul>
<li>用下面的命令查看cudnn版本,新版本查看cuDNN版本的命令为</li>
</ul>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2  <span class="comment"># -A 选项用来指定匹配成功的行之后显示2行内容</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>结果</p>
<ol>
<li>用默认config.yml训练太慢了大约需要1周时间，中途停掉了</li>
<li>用改进后config.yml训练。8小时左右训练完成，但是loss很大</li>
<li>结合往年此赛道总结文章放弃训练提供的mobilernn baseline 思考其它基于cnn的模型</li>
</ol>
</li>
</ul>
</li>
<li>从NTIRE 2022 efficient super-resolution challenge选取baseline运用剪枝蒸馏等改进到移动端<ul>
<li>选取2022 NTIRE ESR冠军方案RLFN(Byte Dance)作为baseline,先将其模型转换为 tensorflow 版本在 REDS 数据集上直接进行VSR的测试 -&gt; 中间软件依赖兼容性问题放弃RLFN torch-&gt;onnx-&gt;tensorflow路线</li>
<li>直接用tensorflow 重构 RLFN -&gt; train完精度不够’psnr’: 25.651411, ‘ssim’: 0.6954131，需要调试改进</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="本周计划"><a href="#本周计划" class="headerlink" title="本周计划"></a>本周计划</h2><ol>
<li>workshop 改进<ul>
<li>Pruning via NNI</li>
<li>Quantization via NNI</li>
<li>Hyper Parameter Optimization via NNI</li>
</ul>
</li>
<li>基金研究基础改小错误</li>
<li>考试复习</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-2-20-2023-2-26）"><a href="#进度汇报-（2023-2-20-2023-2-26）" class="headerlink" title="进度汇报 （2023.2.20-2023.2.26）"></a>进度汇报 （2023.2.20-2023.2.26）</h2><ul>
<li>workshop (MAI 2023 Video Super Resolution)<ol>
<li>RLFN精度提升 ‘psnr’: 25.57 -&gt; 25.91, ‘ssim’: 0.69 -&gt; 0.71; 重构的baseline结构可能存在问题，需要与原作保持一致，恢复精度</li>
<li>尝试基于RNN的方案 SWRN, ‘psnr’: 28.19, ‘ssim’: 0.8093;</li>
<li>3D卷积用于视频超分调研，后期可以考虑3D卷积重参数化加速推理</li>
</ol>
</li>
</ul>
<h2 id="本周计划（2023-2-20-2023-2-26）"><a href="#本周计划（2023-2-20-2023-2-26）" class="headerlink" title="本周计划（2023.2.20-2023.2.26）"></a>本周计划（2023.2.20-2023.2.26）</h2><ol>
<li>workshop<ul>
<li>恢复baseline模型tensorflow版本的PSNR</li>
<li>有余力 剪枝加速推理</li>
</ul>
</li>
<li>考试复习</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-2-27-2023-3-5）"><a href="#进度汇报-（2023-2-27-2023-3-5）" class="headerlink" title="进度汇报 （2023.2.27-2023.3.5）"></a>进度汇报 （2023.2.27-2023.3.5）</h2><ol>
<li>workshop (MAI 2023 Video Super Resolution)<ul>
<li>目前PSNR、SSIM比较满意的是VapSR_2,但是推理时间太长,需要优化</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
</tr>
</thead>
<tbody><tr>
<td>SWRN_0</td>
<td>Origin</td>
<td>REDS</td>
<td>27.931335</td>
<td>0.7803562</td>
<td>43,472</td>
<td>25.6</td>
</tr>
<tr>
<td>SWRN_1</td>
<td>recon_trunk block num&#x3D;2</td>
<td>REDS</td>
<td>27.820051</td>
<td>0.77666414</td>
<td>36,512</td>
<td>26.9</td>
</tr>
<tr>
<td>ELSR_0(vsr 22 winner)</td>
<td>Origin</td>
<td>REDS</td>
<td>26.716854</td>
<td>0.73988235</td>
<td>3,468</td>
<td>19.3</td>
</tr>
<tr>
<td>VapSR_0</td>
<td>Origin</td>
<td>REDS</td>
<td>28.103758</td>
<td>0.7864979</td>
<td>154,252</td>
<td>5191.0</td>
</tr>
<tr>
<td>VapSR_1</td>
<td>Replace feature extraction conv and VAB’s 2 con1X1 with blueprint conv</td>
<td>REDS</td>
<td>28.02941</td>
<td>0.7845887</td>
<td>155,916</td>
<td>5798.0</td>
</tr>
<tr>
<td>VapSR_2</td>
<td>Replace feature extraction conv with blueprint conv and  reduce Attention’s kernel size&#x3D;3X3</td>
<td>REDS</td>
<td>28.021387</td>
<td>0.7831156</td>
<td>131,276</td>
<td>2694.0</td>
</tr>
</tbody></table>
<hr>
<p>  <em>AI benchmark setting for Runtime test:</em>  </p>
<ul>
<li>Input Values range(min,max): 0,255  </li>
<li>Inference Mode: FP16  </li>
<li>Acceleration: TFLite GPU Delegate</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-2-27-2023-3-5）"><a href="#本周计划（2023-2-27-2023-3-5）" class="headerlink" title="本周计划（2023.2.27-2023.3.5）"></a>本周计划（2023.2.27-2023.3.5）</h2><ol>
<li>workshop<ul>
<li>利用tensorflow model optimization toolkit(TFMOT)进行模型压缩,看效果</li>
<li>寻找新idea</li>
</ul>
</li>
<li>两门考试复习</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-3-6-2023-3-12）"><a href="#进度汇报-（2023-3-6-2023-3-12）" class="headerlink" title="进度汇报 （2023.3.6-2023.3.12）"></a>进度汇报 （2023.3.6-2023.3.12）</h2><ol>
<li>workshop (MAI 2023 Video Super Resolution)<ul>
<li>tensorflow model optimization toolkit(TFMOT) Pruning： tensorflow中对于子类化的模型(Subclassed Model)剪枝支持不好，代码有些问题，目前只对模型头部特征提取的蓝图卷积部分应用到了，对于中间参数量最大的部分没有应用成功，推理时间降低很少。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
</tr>
</thead>
<tbody><tr>
<td>VapSR_2</td>
<td>Replace feature extraction conv with blueprint conv and  reduce Attention’s kernel size&#x3D;3X3</td>
<td>REDS</td>
<td>28.021387</td>
<td>0.7831156</td>
<td>131,276</td>
<td>2694.0</td>
</tr>
<tr>
<td>VapSR_3</td>
<td>Pruning</td>
<td>REDS</td>
<td>28.021387</td>
<td>0.7831156</td>
<td>131,276</td>
<td>2673.0</td>
</tr>
</tbody></table>
<hr>
</li>
</ol>
<h2 id="本周计划（2023-3-6-2023-3-12）"><a href="#本周计划（2023-3-6-2023-3-12）" class="headerlink" title="本周计划（2023.3.6-2023.3.12）"></a>本周计划（2023.3.6-2023.3.12）</h2><ol>
<li>workshop<ul>
<li>将模型重构为函数式模型(Functional Model)再进行剪枝</li>
<li>模型量化部署</li>
</ul>
</li>
<li>审稿</li>
<li>组会准备</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-3-13-2023-3-19）"><a href="#进度汇报-（2023-3-13-2023-3-19）" class="headerlink" title="进度汇报 （2023.3.13-2023.3.19）"></a>进度汇报 （2023.3.13-2023.3.19）</h2><ol>
<li>workshop (MAI 2023 Video Super Resolution)<ul>
<li>tensorflow model optimization toolkit(TFMOT) Pruning, Weight Clustering：成功对模型的全部卷积层应用了50%的剪枝和总体10个权重聚类中心的聚类,参数量较之前只对特征提取部分应用剪枝下降很多</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
</tr>
</thead>
<tbody><tr>
<td>VapSR_3</td>
<td>Pruning feature extraction part</td>
<td>REDS</td>
<td>28.021387</td>
<td>0.7831156</td>
<td>131,276</td>
<td>-</td>
</tr>
<tr>
<td>VapSR_4</td>
<td>apply pruning, weights clustering to conv kernels</td>
<td>REDS</td>
<td>27.3111</td>
<td>0.7376</td>
<td>32,054</td>
<td>-</td>
</tr>
</tbody></table>
<hr>
</li>
</ol>
<h2 id="本周计划（2023-3-13-2023-3-19）"><a href="#本周计划（2023-3-13-2023-3-19）" class="headerlink" title="本周计划（2023.3.13-2023.3.19）"></a>本周计划（2023.3.13-2023.3.19）</h2><ol>
<li>workshop<ul>
<li>模型8bit量化实现</li>
<li>寻找降低runtime的新方法</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-3-20-2023-3-26）"><a href="#进度汇报-（2023-3-20-2023-3-26）" class="headerlink" title="进度汇报 （2023.3.20-2023.3.26）"></a>进度汇报 （2023.3.20-2023.3.26）</h2><ol>
<li>workshop (MAI 2023 Video Super Resolution)<ul>
<li>tensorflow model optimization toolkit(TFMOT) Pruning, Weight Clustering -&gt; INT8 Quantization Aware Training (QAT) -&gt; tflite<ul>
<li>问题1： QAT掉点严重 {‘psnr’: 27.666351, ‘ssim’: 0.77187574} -&gt; {‘psnr’: 27.008348, ‘ssim’: 0.7406609}</li>
<li>问题2： 转换为tflite模型过程没问题，手机上用AI Benchmark测试runtime一直报输入type&#x2F;shape mismatch, 定位不到bug在哪里,怀疑是软件不能自定义输入的dtype导致的<ul>
<li>寻求AI Benchmark论坛的帮助</li>
<li>考虑借鉴tensorflow在andriod上的超分案例，测试转换过来的tflite模型</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>work<ul>
<li>参考CVPR2023”Run, Don’t Walk: Chasing Higher FLOPS for Faster Neural Networks”移植了partial convolution (PConv)模块</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-3-20-2023-3-26）"><a href="#本周计划（2023-3-20-2023-3-26）" class="headerlink" title="本周计划（2023.3.20-2023.3.26）"></a>本周计划（2023.3.20-2023.3.26）</h2><ol>
<li>workshop<ul>
<li>解决以上问题</li>
</ul>
</li>
<li>work<ul>
<li>提升在REDS数据集上的PSNR</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-3-27-2023-4-2）"><a href="#进度汇报-（2023-3-27-2023-4-2）" class="headerlink" title="进度汇报 （2023.3.27-2023.4.2）"></a>进度汇报 （2023.3.27-2023.4.2）</h2><ol>
<li>workshop (MAI 2023 Video Super Resolution)<ul>
<li>解决了在手机上对tflite模型推理只能使用CPU无法应用TFLite GPU Delegate 和 NNAPI加速的问题</li>
<li>最新结果</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SWAT_3</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization</td>
<td>REDS</td>
<td>27.761642</td>
<td>0.7748446</td>
<td>25,664</td>
<td>27.8 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
</tbody></table>
<hr>
</li>
<li>work<ul>
<li>移动端视频超分的模型基本确定，并同步进行了部分对比实验，后续需要对剪枝&#x2F;权重聚类&#x2F;量化部分进行微调</li>
<li>文献搜集，了解移动端视频超分的现状和最新进展</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-3-27-2023-4-2）"><a href="#本周计划（2023-3-27-2023-4-2）" class="headerlink" title="本周计划（2023.3.27-2023.4.2）"></a>本周计划（2023.3.27-2023.4.2）</h2><ol>
<li>workshop<ul>
<li>对训练好的模型的量化策略进行调整，看能否进一步降低推理时间</li>
<li>尝试新的训练损失函数&#x2F;训练策略，看能否进一步提升PSNR SSIM</li>
</ul>
</li>
<li>work<ul>
<li>整理思路，论文撰写</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-4-3-2023-4-9）"><a href="#进度汇报-（2023-4-3-2023-4-9）" class="headerlink" title="进度汇报 （2023.4.3-2023.4.9）"></a>进度汇报 （2023.4.3-2023.4.9）</h2><ol>
<li>workshop (MAI 2023 Video Super Resolution)<ul>
<li>在用L1 charbonnier损失进行预训练后，继续使用L2损失训练 -&gt; PSNR：27.76 -&gt; 27.81 上升</li>
<li>改进注意力模块：1.增大感受野 2.部分卷积用分组卷积替代 -&gt; Params: 25,664 -&gt; 24,160 下降 FLOPs: 2.949 -&gt; 2.776 下降，但是runtime反而上涨了 27.8 -&gt; 30.0 tflite对分组卷积的支持不好</li>
<li>目前最好结果</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SWAT_5</td>
<td>Sliding Window, VAB Attention, Partial Conv, Channel Shuffle(mix_ratio&#x3D;1), replace fc with 1*1 conv, replace pixel normalization with layer normalization, enlarge train step numbers to 250,000</td>
<td>REDS</td>
<td>27.811176</td>
<td>0.7763541</td>
<td>25,664</td>
<td>27.6 (FP16_TFLite GPU Delegate)</td>
<td>2.949</td>
</tr>
</tbody></table>
<hr>
</li>
<li>work<ul>
<li>搜集在REDS数据集上完全相同实验设置的paper</li>
<li>剪枝&#x2F;权重聚类的代码之前在基于单帧的单输入单输出模型上跑通，现模型多输入多输出，进行调整后现已跑通</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-4-3-2023-4-9）"><a href="#本周计划（2023-4-3-2023-4-9）" class="headerlink" title="本周计划（2023.4.3-2023.4.9）"></a>本周计划（2023.4.3-2023.4.9）</h2><ol>
<li>work<ul>
<li>汇总完全相同实验设置的paper的结果</li>
<li>完成模型量化部分</li>
<li>撰写论文</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-4-17-2023-4-23）"><a href="#进度汇报-（2023-4-17-2023-4-23）" class="headerlink" title="进度汇报 （2023.4.17-2023.4.23）"></a>进度汇报 （2023.4.17-2023.4.23）</h2><ol>
<li>video super-resolution work<ul>
<li>Channel&#x2F;Spatial&#x2F;Pixel Attention RNN调研</li>
<li>Dynamic pruning&#x2F;Sparsity调研<ol>
<li>Dynamic Channel Pruning: Feature Boosting and Suppression (ICLR 2019)  <ul>
<li>method: subsample feature map to scalar -&gt; channel saliency predictor (fully connect) -&gt; multiple winners take all channel select (Top-k select)</li>
<li>summary:  MAC saving, Memory Usage saving but cann’t contribute to inference latency saving. Fail to achieve real-world acceleration because their hardware-incompatible channel sparsity results in <strong>repeatedly indexing and copying selected filters to a new contiguous memory for multiplication</strong>.</li>
</ul>
</li>
<li>Dynamic Slimmable Network (CVPR 2021)<ul>
<li>method: In-place distillation with In-place Ensemble Bootstrapping (IEB) scheme to train Dynamic Supernet  -&gt; sandwich gate sparsification (SGS) to train Dynamic Slimming Gate</li>
<li>summary: dynamic slice-able conv achieved by <strong>double-headed dynamic gate</strong> which can achieve <strong>practical acceleration</strong> for filters remain contiguous and static during dynamic weight selection.</li>
</ul>
</li>
</ol>
</li>
<li>Information multi-distillation Block (IMDB) 超分实现，推理延迟降低到20ms</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Dataset</th>
<th>Val PSNR</th>
<th>Val SSIM</th>
<th>Params</th>
<th>Runtime on oneplus7T [ms]</th>
<th>FLOPs [G]</th>
</tr>
</thead>
<tbody><tr>
<td>SORT_0</td>
<td>Sliding Window, IMDB</td>
<td>REDS</td>
<td>27.738451</td>
<td>0.77409536</td>
<td>17,356</td>
<td>20.6 (FP16_TFLite GPU Delegate)</td>
<td>2.084</td>
</tr>
</tbody></table>
<hr>
</li>
</ol>
<h2 id="本周计划（2023-4-17-2023-4-23）"><a href="#本周计划（2023-4-17-2023-4-23）" class="headerlink" title="本周计划（2023.4.17-2023.4.23）"></a>本周计划（2023.4.17-2023.4.23）</h2><ol>
<li>video super-resolution work<ul>
<li>Dynamic routing 尝试改造现有模型(一周时间出效果，PSNR⬆ -&gt; 28.00, Runtime⬇ -&gt; 20ms)</li>
</ul>
</li>
<li>审稿</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-5-8-2023-5-14）"><a href="#进度汇报-（2023-5-8-2023-5-14）" class="headerlink" title="进度汇报 （2023.5.8-2023.5.14）"></a>进度汇报 （2023.5.8-2023.5.14）</h2><ol>
<li>video super-resolution work<ul>
<li>video frame selection 调研</li>
<li>SWAT SORT hyperparameter fine tuning</li>
<li>Dynamic routing was deprecated</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-5-8-2023-5-14）"><a href="#本周计划（2023-5-8-2023-5-14）" class="headerlink" title="本周计划（2023.5.8-2023.5.14）"></a>本周计划（2023.5.8-2023.5.14）</h2><ol>
<li>video super-resolution work<ul>
<li>写论文，调模型</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-5-15-2023-5-21）"><a href="#进度汇报-（2023-5-15-2023-5-21）" class="headerlink" title="进度汇报 （2023.5.15-2023.5.21）"></a>进度汇报 （2023.5.15-2023.5.21）</h2><ol>
<li>video super-resolution work<ul>
<li>论文introduction撰写了一部分</li>
<li>模型加入Non Activation Block 和 前一帧HR预测做对齐辅助(former predicted HR frame acting as auxiliary aligned frame)</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-5-15-2023-5-21）"><a href="#本周计划（2023-5-15-2023-5-21）" class="headerlink" title="本周计划（2023.5.15-2023.5.21）"></a>本周计划（2023.5.15-2023.5.21）</h2><ol>
<li>video super-resolution work<ul>
<li>模型改进：Information Multi Distillation Block(IMDB) modify -&gt; high psnr contribution channels aggregation + partial conv</li>
<li>模型改进：light weight feature alignment 轻量级特征对齐</li>
<li>论文撰写：Introduction + Related Work</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-5-22-2023-5-28）"><a href="#进度汇报-（2023-5-22-2023-5-28）" class="headerlink" title="进度汇报 （2023.5.22-2023.5.28）"></a>进度汇报 （2023.5.22-2023.5.28）</h2><ol>
<li><p>video super-resolution work</p>
<ul>
<li><p>论文撰写</p>
</li>
<li><p>模型微调：</p>
<ol>
<li>Activation Free Block 利用乘法产生代替激活函数产生非线性 -&gt; PSNR ↓, SSIM ↓</li>
<li>利用注意力机制对与主网络超分输出做残差连接的bilinear上采样进行对齐操作, 对齐的目标是前一帧HR预测帧(former predicted HR frame acting as auxiliary align frame) -&gt; PSNR ↑, SSIM ↑</li>
</ol>
</li>
<li><p>Benchmark</p>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Source</th>
<th>Dataset</th>
<th>Test PSNR</th>
<th>Test SSIM</th>
<th>Params</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Diggers</td>
<td>Real-Time Video Super-Resolution based on Bidirectional RNNs(2021 SOTA)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.98</td>
<td>-</td>
<td>39,640</td>
</tr>
<tr>
<td>2</td>
<td>VSR_12</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.981062</td>
<td>0.7824855</td>
<td>57,696</td>
</tr>
<tr>
<td>3</td>
<td>SORT_2</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.93981</td>
<td>0.7808094</td>
<td>45,264</td>
</tr>
<tr>
<td>4</td>
<td>SWRN</td>
<td>Sliding Window Recurrent Network for Efficient Video Super-Resolution (2022 SOTA)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.92</td>
<td>0.77</td>
<td>43,472</td>
</tr>
<tr>
<td>5</td>
<td>MVSR_0</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.915539</td>
<td>0.7799377</td>
<td>35,777</td>
</tr>
<tr>
<td>6</td>
<td>SWAT_3_5</td>
<td>Ours</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.840628</td>
<td>0.7774375</td>
<td>37,312</td>
</tr>
<tr>
<td>7</td>
<td>EESRNet</td>
<td>EESRNet: A Network for Energy Efficient Super-Resolution(2022)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.84</td>
<td>-</td>
<td>62,550</td>
</tr>
<tr>
<td>8</td>
<td>LiDeR</td>
<td>LiDeR: Lightweight Dense Residual Network for Video Super-Resolution on Mobile Devices (2022)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.51</td>
<td>0.76</td>
<td>-</td>
</tr>
<tr>
<td>9</td>
<td>EVSRNet</td>
<td>EVSRNet：Efficient Video Super-Resolution with Neural Architecture Search(2021)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.42</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>10</td>
<td>RCBSR</td>
<td>RCBSR: Re-parameterization Convolution Block for Super-Resolution(2022)</td>
<td>REDS(train_videos: 240, test_videos: 30)</td>
<td>27.28</td>
<td>0.775</td>
<td>-</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>pose&#x2F;gesture recognition deploy</p>
<ul>
<li>了解trt_pose类似项目的部署</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-5-22-2023-5-28）"><a href="#本周计划（2023-5-22-2023-5-28）" class="headerlink" title="本周计划（2023.5.22-2023.5.28）"></a>本周计划（2023.5.22-2023.5.28）</h2><ol>
<li>video super-resolution work<ul>
<li>论文撰写</li>
<li>模型微调：参考An Implicit Alignment for Video Super-Resolution(arxiv 2023.04), 尝试对帧对齐模块的注意力机制加入位置编码</li>
</ul>
</li>
<li>pose&#x2F;gesture recognition deploy<ul>
<li>调研摄像头视角跟随目标的相关项目,尝试部署</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-5-29-2023-6-04）"><a href="#进度汇报-（2023-5-29-2023-6-04）" class="headerlink" title="进度汇报 （2023.5.29-2023.6.04）"></a>进度汇报 （2023.5.29-2023.6.04）</h2><ol>
<li>video super-resolution work<ul>
<li>论文撰写</li>
<li>模型微调：<ol>
<li>light-weight attention based frame alignment(基于attention的轻量帧对齐): PSNR ↑ 0.003, SSIM ↑ 0.006,  Runtime ↑ 9 ms</li>
</ol>
</li>
</ul>
</li>
<li>pose&#x2F;gesture recognition deploy<ul>
<li>调研摄像头视角跟随目标的相关项目</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-5-29-2023-6-04）"><a href="#本周计划（2023-5-29-2023-6-04）" class="headerlink" title="本周计划（2023.5.29-2023.6.04）"></a>本周计划（2023.5.29-2023.6.04）</h2><ol>
<li>video super-resolution work<ul>
<li>论文撰写</li>
</ul>
</li>
<li>pose&#x2F;gesture recognition deploy<ul>
<li>尝试部署视角跟随目标</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-6-05-2023-6-11）"><a href="#进度汇报-（2023-6-05-2023-6-11）" class="headerlink" title="进度汇报 （2023.6.05-2023.6.11）"></a>进度汇报 （2023.6.05-2023.6.11）</h2><ol>
<li>video super-resolution work<ul>
<li>论文撰写</li>
<li>消融实验</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-6-05-2023-6-11）"><a href="#本周计划（2023-6-05-2023-6-11）" class="headerlink" title="本周计划（2023.6.05-2023.6.11）"></a>本周计划（2023.6.05-2023.6.11）</h2><ol>
<li>video super-resolution work<ul>
<li>论文撰写</li>
<li>消融实验</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-6-12-2023-7-01）"><a href="#进度汇报-（2023-6-12-2023-7-01）" class="headerlink" title="进度汇报 （2023.6.12-2023.7.01）"></a>进度汇报 （2023.6.12-2023.7.01）</h2><ol>
<li>video super-resolution work<ul>
<li>论文撰写、修改、提交</li>
</ul>
</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-7-03-2023-7-16）"><a href="#进度汇报-（2023-7-03-2023-7-16）" class="headerlink" title="进度汇报 （2023.7.03-2023.7.16）"></a>进度汇报 （2023.7.03-2023.7.16）</h2><ol>
<li>video super-resolution on mobile<ul>
<li>项目代码整理，上传github</li>
</ul>
</li>
<li>Jetson Nano 部署 ZeroDCE,远远达不到实时性要求,处理单张512×512图片暗光增强耗时 &gt; 2 min。具体结果如下：<img src="/Week_Report/ZeroDCE_test_result.jpg" alt="Elapsed Time(ms)/photo"></li>
<li>NeurIPS 审稿</li>
<li>补充PPT: 模型压缩部署部分</li>
</ol>
<h2 id="本周计划（2023-7-03-2023-7-16）"><a href="#本周计划（2023-7-03-2023-7-16）" class="headerlink" title="本周计划（2023.7.03-2023.7.16）"></a>本周计划（2023.7.03-2023.7.16）</h2><ol>
<li>调研了解最新量化进展，寻找下个工作方向</li>
<li>8-bit 浮点数量化项目(FP8 quantization)高通已开源，测试了解下有无follow的空间</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-7-17-2023-7-23）"><a href="#进度汇报-（2023-7-17-2023-7-23）" class="headerlink" title="进度汇报 （2023.7.17-2023.7.23）"></a>进度汇报 （2023.7.17-2023.7.23）</h2><ol>
<li>Jetson Nano 部署暗光增强 ZeroDCE++,处理单张512×512图片耗时约10ms,但有波动(最高4931.46 ms&#x2F;张),基本满足实时性要求。<ul>
<li><img src="/Week_Report/ZeroDCE++_test_result.png" alt="Elapsed Time(ms)/photo"></li>
</ul>
</li>
</ol>
<h2 id="今后计划（2023-7-17-2023-7-23）"><a href="#今后计划（2023-7-17-2023-7-23）" class="headerlink" title="今后计划（2023.7.17-2023.7.23）"></a>今后计划（2023.7.17-2023.7.23）</h2><ol>
<li>休假(威海 潍坊 邯郸)</li>
<li>ChinaMM云南行(昆明 丽江 大理)</li>
</ol>
<hr>
<h2 id="进度汇报-（2023-8-07-2023-8-13）"><a href="#进度汇报-（2023-8-07-2023-8-13）" class="headerlink" title="进度汇报 （2023.8.07-2023.8.13）"></a>进度汇报 （2023.8.07-2023.8.13）</h2><ol>
<li>Jetson Nano 部署 Face Tracking,结合之前的Pose Estimation 达不到实时30 frame&#x2F;s的要求<ul>
<li><img src="/Week_Report/face_detection_screenshot.png" alt="Elapsed Time(ms)/photo"></li>
<li><img src="/Week_Report/pose_estimation_screenshot.png" alt="Elapsed Time(ms)/photo"></li>
</ul>
</li>
<li>PRCV审稿</li>
<li>FP8 Quantization 调研<ul>
<li>FP8 Quantization: The Power of the Exponent (Qualcomm_NeurIPS 2022)<ol>
<li>FP8更适应离群值多的场景</li>
<li>PTQ时精度优于INT8，QAT时精度比INT8略差</li>
</ol>
</li>
<li>FP8 FORMATS FOR DEEP LEARNING (NVIDIA&#x2F;Arm&#x2F;Intel_ArXiv 2022.09) -&gt; 训练推理统一数据格式FP8<ol>
<li>FP8 可以加速训练和减少训练所需的资源，同时方便部署且可以保证训练出的精度</li>
<li>INT8 量化模型通常需要进行校准或微调，训练与推理数据类型不一致不便于部署，且通常精度会下降</li>
</ol>
</li>
<li>FP8 versus INT8 for efficient deep learning inference (Qualcomm_ArXiv 2023.06) -&gt; FP8 目前在性能和精度上不能取代INT8推理，目前INT4-INT8-INT16是边缘端推理的最优解<ol>
<li>PTQ时在离群值显著的情况下，FP8相较INT8有精度优势; 通常这种情况可以通过W8A16混合精度以及QAT来解决</li>
<li>FP8推理硬件开销大, FP8 MAC 单元效率比 INT8 低50%至180%</li>
<li>为了更高效，已经有一些INT4量化的工具, 但到目前为止并没有FP4相关的工作</li>
</ol>
</li>
<li>Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models (MSRA_ArXiv 2023.05) -&gt; Layer wise混合精度LLM</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-8-07-2023-8-13）"><a href="#本周计划（2023-8-07-2023-8-13）" class="headerlink" title="本周计划（2023.8.07-2023.8.13）"></a>本周计划（2023.8.07-2023.8.13）</h2><ol>
<li>开题报告 -&gt; success</li>
<li>FANI andriod app -&gt; false</li>
<li>实习简历 -&gt; success</li>
<li>天空之眼广域持续监视系统 PPT -&gt; success</li>
<li>尝试魔改 Qualcomm_FP8 + SQuant -&gt; pending</li>
<li>尝试集成Face Tracking 和 Pose Estimation实现相机角度跟随人体并进行姿态估计 -&gt; pending</li>
<li>大湾区算法比赛： 视频插帧 + 单目深度估计 -&gt; pending</li>
<li>ACwing 算法课 -&gt; pending</li>
<li>CSAPP 课程+实验 -&gt; pending</li>
<li>Dipoorlet MQBench 使用 -&gt; pending</li>
</ol>
<h2 id="进度汇报-（2023-8-14-2023-8-27）"><a href="#进度汇报-（2023-8-14-2023-8-27）" class="headerlink" title="进度汇报 （2023.8.14-2023.8.27）"></a>进度汇报 （2023.8.14-2023.8.27）</h2><ol>
<li>机载广域持续监视方案调研,PPT制作</li>
<li>开题报告</li>
<li>jetson nano 项目：Face Tracking + Pose Estimation<ul>
<li>原有基于nvidia官方trt_pose项目的姿态估计推理速度慢,现基于Shanghai AI Lab 2023最新的轻量姿态估计项目RTMPose进行部署</li>
<li>调研了解商汤MMdeploy 和 MMPose项目，编译安装相关依赖并在jetson nano上搭建了部署环境</li>
<li>完成了驱动舵机调整摄像头位置的C++代码，后续通过ctypes库实现在py文件中调用此部分调整摄像头姿态的C++代码</li>
</ul>
</li>
</ol>
<h2 id="本周计划（2023-8-14-2023-8-27）"><a href="#本周计划（2023-8-14-2023-8-27）" class="headerlink" title="本周计划（2023.8.14-2023.8.27）"></a>本周计划（2023.8.14-2023.8.27）</h2><ol>
<li>完成jetson nano 项目：Face Tracking + Pose Estimation</li>
<li>调研了解TensorRT&#x2F;TNN&#x2F;MNN&#x2F;NCNN等推理框架，重点尝试运用TensorRT加速RTMPose的推理</li>
<li>参加大湾区算法比赛： 视频插帧 + 单目深度估计</li>
<li>实习简历投递</li>
</ol>
<h2 id="进度汇报（2023-8-28-2023-9-10）"><a href="#进度汇报（2023-8-28-2023-9-10）" class="headerlink" title="进度汇报（2023.8.28-2023.9.10）"></a>进度汇报（2023.8.28-2023.9.10）</h2><ol>
<li>完成jetson nano 项目：Face Tracking + Pose Estimation -&gt; 70%</li>
<li>调研了解TensorRT&#x2F;TNN&#x2F;MNN&#x2F;NCNN等推理框架，重点尝试运用TensorRT加速RTMPose的推理 -&gt; 0%</li>
<li>参加大湾区算法比赛： 视频插帧 + 单目深度估计 -&gt; 0%</li>
</ol>
<h2 id="本周计划（2023-8-28-2023-9-10）"><a href="#本周计划（2023-8-28-2023-9-10）" class="headerlink" title="本周计划（2023.8.28-2023.9.10）"></a>本周计划（2023.8.28-2023.9.10）</h2><ol>
<li>实习简历投递(30-40份)</li>
<li>调研ChatGPT的各种应用</li>
<li>参加大湾区算法比赛： 视频插帧 + 单目深度估计，baseline搭建</li>
<li>撰写FANI专利,整理已有专利状态</li>
</ol>
<h2 id="进度汇报（2023-9-11-2023-9-17）"><a href="#进度汇报（2023-9-11-2023-9-17）" class="headerlink" title="进度汇报（2023.9.11-2023.9.17）"></a>进度汇报（2023.9.11-2023.9.17）</h2><ol>
<li>ICDM投稿FANI专利撰写</li>
<li>组内已有专利状态整理</li>
<li>大湾区算法比赛： 视频插帧 -&gt; 0%, 单目深度估计 -&gt; 10%</li>
</ol>
<h2 id="本周计划（2023-9-11-2023-9-17）"><a href="#本周计划（2023-9-11-2023-9-17）" class="headerlink" title="本周计划（2023.9.11-2023.9.17）"></a>本周计划（2023.9.11-2023.9.17）</h2><ol>
<li>实习简历投递(30-40份)</li>
<li>调研ChatGPT的各种应用</li>
<li>大湾区算法比赛： 视频插帧 -&gt; 50% (baseline搭建); 单目深度估计 -&gt; 50% (baseline搭建)</li>
</ol>
<h2 id="进度汇报（2023-9-18-2023-10-08）"><a href="#进度汇报（2023-9-18-2023-10-08）" class="headerlink" title="进度汇报（2023.9.18-2023.10.08）"></a>进度汇报（2023.9.18-2023.10.08）</h2><ol>
<li>大湾区单目深度估计比赛：<ul>
<li>数据的理解存在偏差，涉及共计6个不同数据集的ground truth, label的标签意义未能理解清(如单位mm还是m, skymask, validmask等等)</li>
<li>选择部分结构清晰(仅包含imgs, gts)的数据集送入目前的SOTA模型 ZoeDepth 对其 metric bins module 进行微调，结果训练后的精度比原作只在 NYU Depth V2 数据集上进行微调的效果还差</li>
<li>目前的提交的结果：A榜 42&#x2F;60, B榜决定最终排名尚未出结果</li>
</ul>
</li>
<li>视频超分量化</li>
<li>国奖申请答辩&#x2F;助教申请&#x2F;调研准备组会PPT</li>
<li>实习申请（累计投递40+）</li>
</ol>
<h2 id="本周计划（2023-9-18-2023-10-08）"><a href="#本周计划（2023-9-18-2023-10-08）" class="headerlink" title="本周计划（2023.9.18-2023.10.08）"></a>本周计划（2023.9.18-2023.10.08）</h2><ol>
<li>实习投递+笔试面试</li>
<li>推进SOTA视频超分模型量化 -&gt; 目标可部署在 OnePlus 7T 上</li>
</ol>
<h2 id="进度汇报（2023-10-09-2023-10-15）"><a href="#进度汇报（2023-10-09-2023-10-15）" class="headerlink" title="进度汇报（2023.10.09-2023.10.15）"></a>进度汇报（2023.10.09-2023.10.15）</h2><ol>
<li>Video Super-Resolution Quantization<ul>
<li>基于目前在Vid4、Vimeo90k、REDS数据集上SOTA模型 BasicVSR++ 进行channel-wise distribution-aware 量化pipeline的搭建（目前尚没有视频超分量化超分方向的baseline，代码难度较大）</li>
<li>尝试引入在其它视频感知任务（Human Pose Estimation，Semantic Segmentation，Video Object Segmentation）有效上的方法，参考 ICCV2023 ResQ 将网络中相邻帧的激活之间的残差用于量化，更小的方差有利于缩小量化误差<ul>
<li>附图：<img src="Week_Report/ResQ.png" alt="ResQ Motivation" width="500" height="300"></li>
</ul>
</li>
</ul>
</li>
<li>ICDM注册提交</li>
</ol>
<h2 id="本周计划（2023-10-09-2023-10-15）"><a href="#本周计划（2023-10-09-2023-10-15）" class="headerlink" title="本周计划（2023.10.09-2023.10.15）"></a>本周计划（2023.10.09-2023.10.15）</h2><ol>
<li>BasicVSR++&#x2F;VRT&#x2F;RVRT + ResQ&#x2F;CADyQ&#x2F;DAQ -&gt; VSR quantization pipeline construction</li>
</ol>
<h2 id="进度汇报（2023-10-16-2023-10-22）"><a href="#进度汇报（2023-10-16-2023-10-22）" class="headerlink" title="进度汇报（2023.10.16-2023.10.22）"></a>进度汇报（2023.10.16-2023.10.22）</h2><ol>
<li>Video Super-Resolution Quantization<ul>
<li>参考 GPTQ 完成了 BasicVSR++ (未涉及ViT)量化的基础部分</li>
<li>阅读论文,了解其它几个SOTA模型(ViTs)是否有需要单独改进的模块：<ul>
<li>CVPR2022: TTVSR</li>
<li>NIPS2022: PSRT, RVRT</li>
<li>CVPR2023: IART</li>
</ul>
</li>
</ul>
</li>
<li>SelecQ latex 排版调整,期刊注册提交</li>
<li>学校HPC实例到期, 实验室浪潮集群上 Docker 镜像搭建</li>
</ol>
<h2 id="本周计划（2023-10-16-2023-10-22）"><a href="#本周计划（2023-10-16-2023-10-22）" class="headerlink" title="本周计划（2023.10.16-2023.10.22）"></a>本周计划（2023.10.16-2023.10.22）</h2><ol>
<li>结合 ResQ 采用 PaddlePaddleSlim 改进视频超分 BasicVSR++ 量化模块</li>
<li>深度神经网络课程PPT制作</li>
<li>专利修改</li>
<li>一番摆事实讲道理: 老师同意 sensetime 实习3个月, 开心到爆炸 :)</li>
</ol>
<h2 id="进度汇报（2023-10-23-2023-10-29）"><a href="#进度汇报（2023-10-23-2023-10-29）" class="headerlink" title="进度汇报（2023.10.23-2023.10.29）"></a>进度汇报（2023.10.23-2023.10.29）</h2><ol>
<li>Video Super-Resolution Quantization<ul>
<li>试用百度 paddleslim 分别用静态动态量化（PTQ）对 BasicVSR++ 进行量化</li>
</ul>
</li>
<li>深度神经网络课程PPT制作</li>
<li>拔智齿~ -&gt; 耽误了一些 VSR Quantization 工作的进度</li>
</ol>
<h2 id="本周计划（2023-10-23-2023-10-29）"><a href="#本周计划（2023-10-23-2023-10-29）" class="headerlink" title="本周计划（2023.10.23-2023.10.29）"></a>本周计划（2023.10.23-2023.10.29）</h2><ol>
<li>play with paddleslim and basicvsr++</li>
</ol>
<h2 id="进度汇报-（2023-10-30-2023-11-05）"><a href="#进度汇报-（2023-10-30-2023-11-05）" class="headerlink" title="进度汇报 （2023.10.30-2023.11.05）"></a>进度汇报 （2023.10.30-2023.11.05）</h2><ol>
<li>Video Super-Resolution Quantization<ul>
<li>BasicVSR++ PTQ： 量化过程有bug正在解决<ol>
<li>BasicVSR++ torch模型转onnx模型并检查</li>
<li>激活校准，产出量化参数: scale zero_point</li>
<li>权重调整，提升量化精度</li>
<li>量化误差分析，定位量化问题</li>
</ol>
</li>
<li><strong>note:</strong> 目前 BasicVSR++ 的 PTQ 基于开源工具 Dipoorlet 进行，优点代码简洁明了易修改，相较百度框架 paddleslim 便于快捷验证idea; VSR 量化方法成熟后可进一步迁移至 paddleslim</li>
</ul>
</li>
<li>读文献找idea提升PTQ精度</li>
</ol>
<h2 id="本周计划（2023-10-30-2023-11-05）"><a href="#本周计划（2023-10-30-2023-11-05）" class="headerlink" title="本周计划（2023.10.30-2023.11.05）"></a>本周计划（2023.10.30-2023.11.05）</h2><ol>
<li>解决 dipoorlet 量化 BasicVSR++ 遇到的bug</li>
</ol>
<h2 id="进度汇报-23-11-06-23-11-12"><a href="#进度汇报-23-11-06-23-11-12" class="headerlink" title="进度汇报 (23.11.06-23.11.12)"></a>进度汇报 (23.11.06-23.11.12)</h2><ol>
<li><p>Video Super-Resolution Quantization</p>
<ul>
<li>BasicVSR++ 采用 <code>Dipoorlet</code> PTQ： 量化过程有不支持动态输入的问题, 即不支持视频随机长度(time_step)的问题, github提了issue 暂未有回复</li>
<li>BasicVSR++ 采用 <code>MQBench</code> PTQ: BasicVSR++ 模型 forward 过程存在动态控制流, 即控制流的判断条件含有运算变量(Input&#x2F;Activation)参与, 而<code>MQBench</code>调用 <code>torch.fx</code> 的 <code>symbolic_trace</code> 完成 forward 过程计算图捕捉, 其本身的限制不支持动态控制流。正尝试：<ol>
<li>把模型的动态控制流用静态的代替</li>
<li>torch 2.0 新发布的 <code>torch.compile</code> 也即 (TorchDynamo), 了解后尝试来解决模型 forward 中广泛存在的动态控制流</li>
</ol>
</li>
</ul>
</li>
<li><p>RustDesk 中继服务搭建, 降低远程桌面的延迟</p>
</li>
</ol>
<h2 id="本周计划-23-11-06-23-11-12"><a href="#本周计划-23-11-06-23-11-12" class="headerlink" title="本周计划 (23.11.06-23.11.12)"></a>本周计划 (23.11.06-23.11.12)</h2><ol>
<li>推进 VSR 模型的常规量化(Naive PTQ)的工作</li>
<li>实习相关工作</li>
</ol>
<h2 id="进度汇报-23-11-13-23-11-19"><a href="#进度汇报-23-11-13-23-11-19" class="headerlink" title="进度汇报 (23.11.13-23.11.19)"></a>进度汇报 (23.11.13-23.11.19)</h2><ol>
<li>Video Super-Resolution Quantization<ul>
<li>BasicVSR++ 采用 <code>Dipoorlet</code> PTQ： 量化过程有不支持动态输入的问题, 即不支持视频随机长度(time_step)的问题, github官方答复暂不支持</li>
</ul>
</li>
<li>Sensetime Internship<ul>
<li>与mentor沟通了解了实际业务量化部署过程中大体流程以及其重难点(torch模型 -&gt; onnx计算图中间表示 -&gt; 目标平台SDK),尚未接触实际项目</li>
<li>调研了解LLM Quantization, 之后会逐步扩展形成对 LLM -&gt; Transformer -&gt; CNN整个链条的量化部署的覆盖</li>
</ul>
</li>
</ol>
<h2 id="本周计划-23-11-13-23-11-19"><a href="#本周计划-23-11-13-23-11-19" class="headerlink" title="本周计划 (23.11.13-23.11.19)"></a>本周计划 (23.11.13-23.11.19)</h2><ol>
<li>完成 LLM Quantization 的初步调研(会将文档共享给大家), 和mentor探讨下一步该从哪种方法开始上手复现</li>
<li>准备 ICDM presentation -&gt; false</li>
</ol>
<h2 id="进度汇报-23-11-20-23-11-26"><a href="#进度汇报-23-11-20-23-11-26" class="headerlink" title="进度汇报 (23.11.20-23.11.26)"></a>进度汇报 (23.11.20-23.11.26)</h2><ol>
<li>Sensetime Internship<ul>
<li>LLM Quantization 累计调研10篇典型文章，包含QAT, weight only quantization, weight and activation quantization。给mentor做了初步的讨论汇报</li>
</ul>
</li>
</ol>
<h2 id="本周计划-23-11-20-23-11-26"><a href="#本周计划-23-11-20-23-11-26" class="headerlink" title="本周计划 (23.11.20-23.11.26)"></a>本周计划 (23.11.20-23.11.26)</h2><ol>
<li>Video Super-Resolution Quantization<ul>
<li>准备 ICDM presentation</li>
</ul>
</li>
<li>Sensetime Internship<ul>
<li>LLM Quantization 环境搭建, 在llama 7B 上跑了一下GPTQ~</li>
<li>了解 Intel neural-compressor QAT 量化</li>
<li>了解 Pytorch FX QAT 量化</li>
<li>了解 mmdeploy 量化</li>
<li>推导验证 RepVGG QAT 多支路 conv 合并后图节点量化前浮点数范围 (Real Range) 能否无损得到</li>
<li>多模态模型 codino -&gt; onnx -&gt; onnx runtime(ort) register grid_sampler 注册未支持的算子(参考MMCV MMDeploy),然后在A6000上部署推理</li>
</ul>
</li>
<li>Auto Drive<ul>
<li>两个导向一种不惜计算代价，使用各种方法提高特定数据集&#x2F;特定环境下的指标 (刷榜)，一种是轻量化资源受限情况下优化指标</li>
<li>轻量化路线具体需要针对不同的硬件平台(如 jetson orin)来进行相应的轻量化,如:<ul>
<li>设计轻量的网络结构，设计相应的算子op</li>
<li>考虑部署的inference latency,有一些比较有实际意义的探索空间,原因在于纸面的模型 FLOPs&#x2F;MACs 和实际 inference latency 之间有 gap</li>
<li>实际车辆上运行的模型受限于算力，算力小用CNN，算力大用Transformer, 大公司在往大一统方向做 (如CVPR 2023 best paper: UniAD, 但目前还没有部署到实际的平台上去,目前带我的mentor在进行部署的工作：很难的一点是如何正确的把模型转换为onnx中间表示然后去进行量化，这一步还没完成)</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="进度汇报-23-11-27-23-12-10"><a href="#进度汇报-23-11-27-23-12-10" class="headerlink" title="进度汇报 (23.11.27-23.12.10)"></a>进度汇报 (23.11.27-23.12.10)</h2><ol>
<li>School Task<ul>
<li>ICDM presentation PPT 制作&#x2F;参会</li>
<li>VSR 视频超分专利修改</li>
<li>Face Super-resolution&#x2F;Enhancement 调研</li>
</ul>
</li>
<li>Sensetime Internship<ul>
<li>LLM Quantization 环境搭建, 在 llama 7B 上跑了一下 GPTQ</li>
<li>了解 Intel neural-compressor &#x2F; Pytorch FX &#x2F; mmdeploy PTQ 量化,大致如下 以 torch fx 为例 (后续会结合代码形成较详细的量化pipeline文档)<ol>
<li>Prepare fx: fuse模型，也就是通常的优化，比如conv+bn啥的,利用fx对模型进行transform</li>
<li>Insert observer: Input&#x2F;Output&#x2F;Weight 均插入observer</li>
<li>Calibration: 输入数据进行校准，收集 weights 和 activation 的 max 和 min 等统计信息</li>
<li>Convert fx: 在 observer 位置用相应的 quantize&#x2F;dequantize module代替,并合并到原始的layer中<br>附图：<img src="Week_Report/torch.fx_ptq.PNG" alt="torch fx ptq" width="800" height="200"></li>
</ol>
</li>
<li>推导验证 RepVGG 重参数轻量化方法与 QAT 联合使用的 weight range 能否获得数学上的等价变换问题: 具体来说，多支路 conv 合并前运用 QAT 提升精度, 在多支路 conv 合并后根据 QAT 训练得到的多分支 conv weight 的浮点数范围 (Real Range) 能否等价得到合并后的 conv weight 的浮点数范围，此浮点数范围与量化&#x2F;反量化过程的 scale factor 和 zero point 基本等价可互推 -&gt; 结论无法无损等价</li>
<li>感知模型 object detection: codino -&gt; onnx -&gt; onnx runtime(ort) 推理部署： register grid_sampler op 注册未支持的算子(参考MMCV MMDeploy),然后在 CPU 上实现模型推理, 后续搭建好模型前后处理的部分后会在GPU上进行推理效果验证</li>
</ul>
</li>
</ol>
<h2 id="后续计划-23-11-27-23-12-10"><a href="#后续计划-23-11-27-23-12-10" class="headerlink" title="后续计划 (23.11.27-23.12.10)"></a>后续计划 (23.11.27-23.12.10)</h2><ol>
<li>了解并测试多个开源 LLM PTQ 方法，如 AWQ SmoothQuant ZeroQuantV2 等</li>
<li>codino 模型前后处理部分搭建, 在 GPU 上进行 onnx runtime 推理部署</li>
<li>调研了解能否用 onnx runtime 进行 QAT, 以减轻 QAT 与 部署的 op 输入&#x2F;输出&#x2F;权重参数 范围对齐的压力</li>
</ol>
<h2 id="进度汇报-23-11-11-23-12-24"><a href="#进度汇报-23-11-11-23-12-24" class="headerlink" title="进度汇报 (23.11.11-23.12.24)"></a>进度汇报 (23.11.11-23.12.24)</h2><ol>
<li><p>School Task</p>
<ul>
<li>深度学习与深度神经网络原理课程报告汇总提交</li>
</ul>
</li>
<li><p>Sensetime Internship</p>
<ul>
<li><p>obeject detection model deploy: 基于openmmlab开源框架mmcv mmdetection,花费较长时间处理代码相关细节，具体如下</p>
<ol>
<li>基于继承机制的模型config使用，模型 backbone 为 InternImage</li>
<li>模型前后处理剥离，前处理主要是resize操作,后处理模型输出为shape为[n, 5]的np.ndarray如何正确转化为 bbox 与 class 并可视化出来</li>
<li>torch 模型转 onnx 时未支持算子 grid_sampler 处理</li>
<li>相关repository:<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Sense-X/Co-DETR">https://github.com/Sense-X/Co-DETR</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/InternImage">https://github.com/OpenGVLab/InternImage</a></li>
</ul>
</li>
</ol>
</li>
<li><p>quantization frame work 调研，包括 Torch FX Quantization, Intel Neural Compressor, Tensor RT Quantization等,具体见PDF总结</p>
</li>
<li><p>调研并尝试基于 onnx 做 QAT(背景: torch 模型转为 onnx 时会出现模型结构名变化的问题，导致 QAT 得到的模型 权重以及量化参数 i.e. scale factor, zero point 无法与导出的 onnx 模型匹配)</p>
</li>
<li><p>详细了解 LLM metric Perpelexity (PPL), 以及关联的 torch cross entropy 的计算细节</p>
</li>
<li><p>opencloud 集群的使用, 了解 slurm 命令及参数等</p>
</li>
</ul>
</li>
</ol>
<h2 id="后续计划-23-11-11-23-12-24"><a href="#后续计划-23-11-11-23-12-24" class="headerlink" title="后续计划 (23.11.11-23.12.24)"></a>后续计划 (23.11.11-23.12.24)</h2><ol>
<li>测试以下 LLM PTQ 方法：AWQ, SmoothQuant, Outlier Suppression+, LLM.int8()</li>
<li>推进 onnx QAT</li>
<li>video super-resolution：基于 torch fx 对 basicvsr++ 进行 post training dynamic quantization (activation 的 scale factor 在推理时确定而非预先根据统计量计算出来) -&gt; 搞明白 Torch FX OPs 插入 &#x2F; 融合 &#x2F; 操作 的位置与做法</li>
<li>论文写作 课程作业</li>
</ol>
<h2 id="进度汇报-23-12-25-24-01-07"><a href="#进度汇报-23-12-25-24-01-07" class="headerlink" title="进度汇报 (23.12.25-24.01.07)"></a>进度汇报 (23.12.25-24.01.07)</h2><ol>
<li><p>School Task</p>
<ul>
<li>论文写作 课程作业</li>
<li>参加了上海人工智能实验室组织的书生·浦语大模型实战营，目前已完成前两节,部署了一下类似chatgpt的问答demo,简单笔记如下~<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/675925793">InterLM L-1: 书生·浦语大模型全链路开源开放体系</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/676498904">InterLM L-2: 轻松玩转书生·浦语大模型趣味 Demo</a></li>
</ul>
</li>
<li>阅读多模态论文：CogVLM(<a target="_blank" rel="noopener" href="https://github.com/THUDM/CogVLM">https://github.com/THUDM/CogVLM</a>)<ul>
<li>附图：<img src="Week_Report/cogvlm.png" alt="cogvlm" width="800" height="400"></li>
<li>整体思路：先将 image 输入通过 MLP 映射到与 text embedding 相同的空间中(上图左侧)，然后在预训练好的LLM上嫁接用与深度对齐两个模态的 Attention 和 MLP 部分，只训练这部分即可（上图右侧）</li>
</ul>
</li>
</ul>
</li>
<li><p>Sensetime Internship</p>
<ul>
<li>onnx QAT 项目停止：详细测试之后发现，torch.onnx.export() 模型导出后 onnx 模型结构名称与原 torch 模型的名称不匹配问题已在新版本解决。原版本为 torch1.8.0 现版本 torch1.13 及 torch2.x 都已不存在该问题</li>
<li>搞清了 Torch FX OPs 插入 &#x2F; 融合 &#x2F; 更改 的位置与做法, 整个基于 torch fx graph 去做量化的流程已走通</li>
<li>尝试做集成主流 LLM 量化算法(包括 AWQ, SmoothQuant, GPTQ等)的库，并可扩展新的量化算法，对比学习了 Intel Neural-Compressor &#x2F; LMDeploy &#x2F; OpenPPL PPQ 等，目前还需要与 mentor 进一步探讨确定如何推进</li>
<li>其它学习的部分：量化 &#x2F; MLLM 多模态大模型 具体见 PPT</li>
</ul>
</li>
</ol>
<h2 id="后续计划-23-12-25-24-01-07"><a href="#后续计划-23-12-25-24-01-07" class="headerlink" title="后续计划 (23.12.25-24.01.07)"></a>后续计划 (23.12.25-24.01.07)</h2><ol>
<li>跟进 video super-resolution 量化推进</li>
<li>LLM &#x2F; MLLM 量化算法部署测试与集成推进</li>
<li>完成 书生·浦语大模型实战营内容</li>
</ol>
<h2 id="进度汇报-24-01-07-24-01-21"><a href="#进度汇报-24-01-07-24-01-21" class="headerlink" title="进度汇报 (24.01.07-24.01.21)"></a>进度汇报 (24.01.07-24.01.21)</h2><ol>
<li>书生·浦语大模型实战营内容完成(包括基于 InternLM 和 LangChain 搭建知识库, XTuner 大模型单卡低成本微调, LMDeploy 大模型量化部署等)</li>
<li>VSR模型量化，先基于 mmdetection 的检测模型进行流程的验证，卡在捕获计算图这一步，解决掉这一步才可以游刃有余的进行量化过程中插入 op 的操作。遇到以下两类细节上的问题，正在解决<ul>
<li>forward 过程存在输入动态控制流，torch.fx 不支持 trace 捕获此类计算图</li>
<li>forward 存在对 inputs 调用len() method 而捕获过程会把 inputs 转换为抽象的 proxy 作为输入，proxy object 不支持调用 len() method</li>
</ul>
</li>
<li>LLM &#x2F; MLLM 量化，已经走通了常见模型 llama2-7b 的量化算法 AWQ SmoothQuant</li>
</ol>
<h2 id="后续计划-24-01-07-24-01-21"><a href="#后续计划-24-01-07-24-01-21" class="headerlink" title="后续计划 (24.01.07-24.01.21)"></a>后续计划 (24.01.07-24.01.21)</h2><ol>
<li>走通捕获计算图这一步</li>
<li>阅读 VSR 近期论文，了解最新进展</li>
<li>其它交代的事项</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://kyrie2to11.github.io/2022/08/30/Daily-Thoughts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jarvis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jarvis's Blog">
      <meta itemprop="description" content="记录学习和生活，留下时光的痕迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Jarvis's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/08/30/Daily-Thoughts/" class="post-title-link" itemprop="url">Daily Thoughts</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-08-30 08:44:00" itemprop="dateCreated datePublished" datetime="2022-08-30T08:44:00+08:00">2022-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-02 12:35:09" itemprop="dateModified" datetime="2024-12-02T12:35:09+08:00">2024-12-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="2022-08-16-判断（你自己）研究工作的价值"><a href="#2022-08-16-判断（你自己）研究工作的价值" class="headerlink" title="2022-08-16 判断（你自己）研究工作的价值"></a>2022-08-16 判断（你自己）研究工作的价值</h2><ul>
<li>用有新意的方法有效地解决一个研究问题</li>
<li>价值量化 i.e. 新意度×有效性×问题大小&#x3D;价值</li>
</ul>
<hr>
<h2 id="2022-08-16-如何读论文"><a href="#2022-08-16-如何读论文" class="headerlink" title="2022-08-16 如何读论文"></a>2022-08-16 如何读论文</h2><ul>
<li><p>文章一般结构</p>
<ol>
<li>title(标题)</li>
<li>abstract(摘要)</li>
<li>introduction（导言）</li>
<li>method(算法)</li>
<li>experiment(实验)</li>
<li>conclusion(结论)</li>
</ol>
</li>
<li><p>三遍法读论文</p>
<ol>
<li><p>pass 1 i.e. 1 2 6这三部分速览十几分钟，捎带看一下method和exp中的图表看上去怎么样，是否适合自己，不适合就放掉</p>
</li>
<li><p>pass 2 i.e. 不用太注意细节，比如公式、证明。主要是搞清楚重要的图和表，知道图表的每一个字是在干什么。比如:你的方法里整个流程图长什么样子、算法的图长什么样子、exp实验里每张图的x轴在干什么，y轴在干什么，每个点在干什么、作者提出的方法和别人是怎么对比的，之间差距有多大。这时可能还是没有特别搞懂他在干什么，但对文章有了一个大概的了解，在中间可以把相关文献标出来，重要文献。如果太难读不懂可以先从前人相关文章读起，或者说当前目标不需要完全搞懂他在干嘛，只需了解大概即可，决定是否需要再读下去。</p>
</li>
<li><p>pass 3 i.e. 这一遍当然需要每句话每段话在干什么。在读这篇文章的时候，在脑子里不断重复如何实现这篇文章，或者说一句话说我提出了什么问题用什么方法来解决这个问题，如果是我来做这个事的话怎么办，我应该可以用什么什么来实现这个东西，然后作者在做实验的时候怎么做的，换我会怎么做，能不能比他做得更好；然后作者说我这个文章在有些地方没有往下走，留在之后的结果中，如果我来做能不能往下进行下去。看了这一遍，之后再提到他的时候可以完完全全地复述出来，达到这个效果</p>
</li>
</ol>
</li>
</ul>
<hr>
<h2 id="2022-08-17-压力山大的难受记忆"><a href="#2022-08-17-压力山大的难受记忆" class="headerlink" title="2022-08-17 压力山大的难受记忆"></a>2022-08-17 压力山大的难受记忆</h2><ul>
<li>压力山大，喘不过气<ul>
<li>2022年8月17日，压力在研0组会前夕的一天集中爆发，十分难受，这个时刻特别像初高中放假完不成作业的巨大压力感，那种时刻可以称之为在人生的局部最不想经历的某件事之一。难受到一旦过去，除非是又遇到相似情况下，否则再也不愿意想起，简直是一种折磨。</li>
</ul>
</li>
<li>毫无疑问，这种压力是对身体十分有害的<ul>
<li>想来初中开始为什么开始有白头发，可能原因就是那些没写作业的压力山大导致的肝火旺盛，压抑、惶恐、而又暴躁。可能自己的性格一直以来都不太自洽，内心独白一套，外部表现又不太合内心独白，中间别扭的力量全部积压再内心。我感觉这是一种病，要改，否则早晚有一天会后悔。从身心健康的角度，我想以后做一个乐于交流，表里如一，内外自洽的人。</li>
</ul>
</li>
<li>为什么缺乏和别人的交流<ul>
<li>想来这不是一朝一夕养成的问题，小学3年级以前印象中自己是个调皮捣蛋又活泼的人，后来不知怎的越发沉默寡言，在初中程度不断加深，高中达到顶峰。一大原因可能是从小学到初中的成绩始终名列班级前茅的原因，不知道怎么回事，也并没有一套现在看来多么厉害的方法指导学习，特别是并不注重总结复习，但确实又一直很好。可能是总结复习的量没有达到自己的心理预期，但是达到了科目考试要求的程度。这种单一的沉浸在自己小世界里的学习经历让自己越发缺少与他人的交流。这是一种解释。缺乏交流，衍生来的比较孤僻，在独处的时候往往导致不自律，也就导致不会按计划的去完成各种作业，导致最后的压力山大、无处排解、压抑惶恐。这需要改，为了好好的活下去，一定要改。</li>
</ul>
</li>
<li>展望未来<ol>
<li>研究生期间能再有一些志同道合、志趣相投的好朋友，最好能找一个一起走下去的对象（虽然此时此刻期望值很低，可能跟最近一段时间太闲，太懒散导致的做什么事都没有兴趣和动力，只想躺尸有关）</li>
<li>未来还是要多一些需求，多些新鲜，更像个年轻人的样子。自我感觉无欲无求，可能的原因是自己想要的太多 太大，得到又很难很难，超出了自己的能力范畴（简称野心与能力不匹配），最终导致摆烂，干脆连想要的欲望也没有了- -。</li>
<li>俗话说的好，好死不如赖活着，自己都感觉无欲无求了，为什么还会躺尸式的活着，本质上是因为本能的自己从未想过靠死亡逃避来结束自己的一生，本能地要活出点意义来，虽然当前局部时间确实闲而又压抑的玩给自己带来了很不好的影响。</li>
<li>能找到自已想一生为之奋斗的目标真的是世间一大幸事，我的明天 明年 下个十年会在哪里，身边会有谁，一切都未知。自己从小都是个恋家的人，虽然不得已长期在外，离开的太久压抑了那种情感，但是想想每次要脱离自己熟悉的家乡，总是会生出一丝悲凉。最近又更跟赵茜的不想要小孩的想法深度绑定，没有了之前那种，自然地小孩的意愿，或者说几乎不会想到不要小孩。但是又觉得让自己的小孩重走一遍河北艰难的高考之路，实在是于心不忍。种种的种种都让我最近十分消极，躺尸，仿佛从某个瞬间开始，自己的三观崩塌了，我以后应该会再站起来的，但是重构的三观肯定不是原来的样子了，那么以后的那个自己会是自己四个月前所期望的那样吗？我们慢慢看，加油好好地活下去。</li>
</ol>
</li>
<li>2022-8-17结束思考吧，表里如一，能讲多少讲多少，想想自己本身就很菜，事实上也是如此距离组里的学术优秀的前辈差距甚远，但是不妨碍我就是我，慢慢走，完成明天的汇报ppt吧。</li>
</ul>
<hr>
<h2 id="2022-08-18-自我肯定"><a href="#2022-08-18-自我肯定" class="headerlink" title="2022-08-18 自我肯定"></a>2022-08-18 自我肯定</h2><ul>
<li>我遇到了什么？ 能不能解决？<ol>
<li>有办法解决，去解决</li>
<li>没有办法解决，承认事情失败，但不是人失败</li>
</ol>
</li>
<li>自我批评之后紧跟着自我肯定，这里不好那里不好，不能代表一个人完全没有价值，也不能代表这个世界上就没有人欣赏你或者爱你。—&gt;心理学认知行为疗法</li>
<li>“如何面对自己的普通”<ol>
<li>这个世界上绝大部分人确实都只是普通人，会犯普通人犯的错，有着普通人的缺点，可能一辈子没什么大的成就，也实现不了几个梦想，但这些不能代表一个人就是糟糕的，也不能代表他就是失败的，哪里有点不好，这就是人生的常态而已。说的矫情一点，真正的自信与自爱，就是去学着爱这样的人生，爱这样的自己。</li>
<li>就像那句金句“看透了生活的本质依然热爱生活”</li>
</ol>
</li>
</ul>
<hr>
<h2 id="2022-8-30-困顿迷茫下的小方向"><a href="#2022-8-30-困顿迷茫下的小方向" class="headerlink" title="2022-8-30 困顿迷茫下的小方向"></a>2022-8-30 困顿迷茫下的小方向</h2><ul>
<li>男孩不要太多虑，对脾不好，把身体搞好自然各方面都会变好</li>
<li>人无远虑，必有近忧</li>
<li>减少耍手机的时间，花时间看点书，静下来关注自身</li>
<li>2020年7月份想的是能站在信息科技时代潮头做弄潮儿，2022年却没有了心劲儿</li>
<li>客观上自己生活在中国底层，能养活自己就足够了，如果再能做出点小成就，那真的太好了，从这个角度看自己毫无心理压力</li>
<li>健康问题在身体还好的时候不会想到，但它又是这一切的基石</li>
<li>自己得学会点最基础的赖以生存的技能</li>
<li>朋友尽量不要合伙做生意</li>
<li>暂时想到这么多</li>
</ul>
<hr>
<h2 id="2022-9-27-近一两个月的迷茫"><a href="#2022-9-27-近一两个月的迷茫" class="headerlink" title="2022-9-27 近一两个月的迷茫"></a>2022-9-27 近一两个月的迷茫</h2><ul>
<li>失去了对自己身体和头脑的掌控，客观上有两方面的原因<ul>
<li>面临的问题是什么没有定义清楚</li>
<li>问题的解决不够彻底，导致时间花在刀把上</li>
<li>缺少与师兄的交流</li>
</ul>
</li>
<li>一旦自己克服了以上，就很自信！ Find Questions, Fix Them.</li>
</ul>
<hr>
<h2 id="2023-09-14-生活需要靶子"><a href="#2023-09-14-生活需要靶子" class="headerlink" title="2023-09-14 生活需要靶子"></a>2023-09-14 生活需要靶子</h2><ul>
<li>一直以来都需要一个靶子，在打靶的过程中会收获能力、见识增长的喜悦，只有这时自己才会感觉身心舒适</li>
<li>长久地留在一个舒适区，同一个地方、同一个任务、同一个工作水平等会令自己感到不安(情感需求除外，土象魔羯的我很渴望稳定的情感联结和依赖)</li>
<li>在辛苦工作一段时间后，能作为真正放松自己的靶子我甚至还没有找到,刷刷奶头乐视频,打打游戏并不能真正释放出工作所积累的负能量,仅仅起到一个延迟释放的作用。往往靠的都是在重新开始工作之前遗忘掉了之前的不堪…</li>
<li>天赋一直不用是会丢失的(比如静静地看论文复现实验), 接下来两个月的靶子已经有好几个矗立在前方了</li>
<li>如果不主动找靶子的话，一段时间后无所事事烦躁的心情就会袭上心头，迫使自己不得不去找靶子，综上需要主动的找到靶子，无论是感兴趣的，还是必须干的，得为每段时光找一个奔头</li>
</ul>
<hr>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jarvis</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
